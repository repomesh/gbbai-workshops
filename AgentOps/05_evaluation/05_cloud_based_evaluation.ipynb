{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20625fb7",
   "metadata": {},
   "source": [
    "# Cloud-Based Evaluation with Azure AI Projects\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to perform **cloud-based evaluations** using Azure AI Project's evaluation service. Unlike local evaluations that run on your machine, cloud-based evaluations execute in Azure infrastructure, providing scalability, centralized storage, and team collaboration capabilities.\n",
    "\n",
    "### What is Cloud-Based Evaluation?\n",
    "\n",
    "Cloud-based evaluation allows you to:\n",
    "- **Upload datasets** to Azure AI Project for centralized storage\n",
    "- **Run evaluations asynchronously** in Azure's scalable infrastructure\n",
    "- **Track evaluation jobs** and monitor progress programmatically\n",
    "- **View detailed results** in Azure AI Foundry Studio with rich visualizations\n",
    "- **Share results** with team members for collaborative analysis\n",
    "- **Maintain audit trails** with governance tags and version control\n",
    "\n",
    "### When to Use Cloud-Based Evaluation\n",
    "\n",
    "**Use Cloud-Based Evaluation When:**\n",
    "- Working with large datasets (100+ samples) that need distributed processing\n",
    "- Collaborating with a team that needs shared access to evaluation results\n",
    "- Building production pipelines with automated evaluation workflows\n",
    "- Requiring centralized governance and compliance tracking\n",
    "- Needing historical comparison of evaluation runs over time\n",
    "\n",
    "**Use Local Evaluation When:**\n",
    "- Rapid prototyping and iterative development\n",
    "- Small datasets (< 50 samples) with quick feedback loops\n",
    "- Debugging specific evaluator configurations\n",
    "- Working offline or with sensitive data that cannot leave local environment\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Dataset Management**: Datasets are versioned and stored in Azure AI Project, enabling reproducible evaluations\n",
    "2. **Evaluator Configuration**: Define which metrics to compute using `EvaluatorConfiguration` with proper data mapping\n",
    "3. **Asynchronous Execution**: Jobs run in the background; poll for status until completion\n",
    "4. **Results Storage**: All metrics stored centrally with access through Studio UI or SDK\n",
    "5. **Governance**: Use tags to classify evaluations by environment, data sensitivity, and purpose\n",
    "\n",
    "### Evaluation Workflow\n",
    "\n",
    "```\n",
    "1. Prepare Dataset (JSONL format)\n",
    "   ↓\n",
    "2. Upload to Azure AI Project\n",
    "   ↓\n",
    "3. Configure Evaluators (Quality, Safety, Agent)\n",
    "   ↓\n",
    "4. Create Evaluation Job\n",
    "   ↓\n",
    "5. Monitor Job Status (Polling)\n",
    "   ↓\n",
    "6. View Results in Studio / Download\n",
    "   ↓\n",
    "7. Analyze Metrics & Iterate\n",
    "```\n",
    "\n",
    "### Available Evaluators\n",
    "\n",
    "**Quality Metrics:**\n",
    "- **Coherence**: How well the response flows logically (1-5 scale)\n",
    "- **Relevance**: Whether response addresses the query appropriately (1-5 scale)\n",
    "- **Fluency**: Language quality and readability (1-5 scale)\n",
    "- **Groundedness**: Response fidelity to provided context (1-5 scale)\n",
    "\n",
    "**Agent Metrics:**\n",
    "- **Tool Call Accuracy**: Correctness of tool selection and arguments (0-1 binary or percentage)\n",
    "\n",
    "**Safety Metrics** (configured separately):\n",
    "- Content Safety categories (violence, sexual, self_harm, hate_unfairness)\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Azure AI Project with evaluation quota enabled\n",
    "- Azure OpenAI deployment (GPT-4 or GPT-4o recommended for LLM-judged metrics)\n",
    "- Dataset in JSONL format with required fields for your chosen evaluators\n",
    "- Sufficient API quota for evaluation workload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f31980",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Part 1: Environment Setup](#part-1-environment-setup)\n",
    "2. [Part 2: Dataset Preparation](#part-2-dataset-preparation)\n",
    "3. [Part 3: Upload Dataset to Azure](#part-3-upload-dataset-to-azure)\n",
    "4. [Part 4: Configure Evaluators](#part-4-configure-evaluators)\n",
    "5. [Part 5: Create and Run Evaluation Job](#part-5-create-and-run-evaluation-job)\n",
    "6. [Part 6: Monitor Job Status](#part-6-monitor-job-status)\n",
    "7. [Part 7: View and Analyze Results](#part-7-view-and-analyze-results)\n",
    "8. [Summary and Best Practices](#summary-and-best-practices)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a8bf6a",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "Configure the Azure AI Project client and verify connectivity.\n",
    "\n",
    "**Required Environment Variables:**\n",
    "- `AZURE_AI_PROJECT_ENDPOINT`: Your project endpoint URL\n",
    "- `AZURE_OPENAI_ENDPOINT_GPT_4o`: Model endpoint for LLM-judged metrics\n",
    "- `AZURE_OPENAI_API_KEY_GPT_4o`: API key for model access\n",
    "- `AZURE_OPENAI_MODEl_GPT_4o`: Deployment name (e.g., gpt-4o-mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "new_path_entry = \"/opt/homebrew/bin\"  # Replace with the directory you want to add\n",
    "current_path = os.environ.get('PATH', '')\n",
    "\n",
    "if new_path_entry not in current_path.split(os.pathsep):\n",
    "    os.environ['PATH'] = new_path_entry + os.pathsep + current_path\n",
    "    print(f\"Updated PATH for this session: {os.environ['PATH']}\")\n",
    "else:\n",
    "    print(f\"PATH already contains {new_path_entry}: {current_path}\")\n",
    "\n",
    "# You can then verify with shutil.which again\n",
    "print(f\"Location of 'az' found by kernel now: {shutil.which('az')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32abc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add parent directory to path for agent_utils import\n",
    "parent_dir = Path(__file__).parent.parent if hasattr(__builtins__, '__file__') else Path.cwd().parent\n",
    "sys.path.insert(0, str(parent_dir / \"01_agent\"))\n",
    "\n",
    "# Load environment variables from parent directory\n",
    "agent_ops_dir = Path.cwd().parent if Path.cwd().name == \"05_evaluation\" else Path.cwd()\n",
    "env_path = agent_ops_dir / \".env\"\n",
    "load_dotenv(env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd96940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "\n",
    "endpoint = os.environ[\"AZURE_AI_PROJECT_ENDPOINT\"]\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "project_client = AIProjectClient(endpoint=endpoint, credential=credential)\n",
    "\n",
    "print(f\"Connected to Azure AI Project: {endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da34869",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Dataset Preparation\n",
    "\n",
    "Create a dataset in JSONL format for cloud-based evaluation.\n",
    "\n",
    "**Dataset Requirements:**\n",
    "- **Format**: JSONL (JSON Lines) - one JSON object per line\n",
    "- **Fields**: Include all fields required by your chosen evaluators\n",
    "- **Consistency**: Use the same field names across all samples\n",
    "- **Validation**: Ensure valid JSON on each line\n",
    "\n",
    "**Common Fields:**\n",
    "- `query`: User's question or input\n",
    "- `context`: Retrieved/provided context for the response\n",
    "- `response`: Model's generated response\n",
    "- `expected_tool_calls`: Ground truth for tool usage (agent scenarios)\n",
    "- `actual_tool_calls`: Model's actual tool calls (agent scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2711840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create sample evaluation data\n",
    "evaluation_samples = [\n",
    "    {\n",
    "        \"query\": \"What are the opening hours of the Space Needle in Seattle?\",\n",
    "        \"context\": \"The Space Needle is open from 9:00 AM to 11:00 PM daily.\",\n",
    "        \"response\": \"The Space Needle is open from 9:00 AM to 11:00 PM every day.\",\n",
    "        \"expected_tool_calls\": [],\n",
    "        \"actual_tool_calls\": []\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the weather like in Seattle today?\",\n",
    "        \"context\": \"Seattle typically has rainy weather in winter and mild summers.\",\n",
    "        \"response\": \"The current weather in Seattle is rainy with a temperature of 14°C.\",\n",
    "        \"expected_tool_calls\": [\n",
    "            {\n",
    "                \"name\": \"fetch_weather\",\n",
    "                \"arguments\": {\"location\": \"Seattle\"}\n",
    "            }\n",
    "        ],\n",
    "        \"actual_tool_calls\": [\n",
    "            {\n",
    "                \"name\": \"fetch_weather\",\n",
    "                \"arguments\": {\"location\": \"Seattle\"}\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Can you recommend a good coffee shop in Seattle?\",\n",
    "        \"context\": \"Seattle is famous for its coffee culture with many excellent cafes.\",\n",
    "        \"response\": \"I recommend Pike Place Market Starbucks, the original Starbucks location, or local favorites like Espresso Vivace.\",\n",
    "        \"expected_tool_calls\": [],\n",
    "        \"actual_tool_calls\": []\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How do I get to Pike Place Market from downtown Seattle?\",\n",
    "        \"context\": \"Pike Place Market is located in downtown Seattle at 85 Pike St.\",\n",
    "        \"response\": \"Pike Place Market is in downtown Seattle. You can walk there from most downtown locations, or take a bus to Pike Street.\",\n",
    "        \"expected_tool_calls\": [],\n",
    "        \"actual_tool_calls\": []\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What temperature should I expect in Seattle in summer?\",\n",
    "        \"context\": \"Seattle summers are mild with temperatures typically between 20-25°C.\",\n",
    "        \"response\": \"In summer, Seattle typically has temperatures between 20-25°C (68-77°F), making it quite pleasant.\",\n",
    "        \"expected_tool_calls\": [],\n",
    "        \"actual_tool_calls\": []\n",
    "    }\n",
    "]\n",
    "\n",
    "# Write to JSONL file\n",
    "dataset_file = \"data/evaluate_test_data.jsonl\"\n",
    "with open(dataset_file, 'w') as f:\n",
    "    for sample in evaluation_samples:\n",
    "        f.write(json.dumps(sample) + '\\n')\n",
    "\n",
    "print(f\"Created dataset file: {dataset_file}\")\n",
    "print(f\"Number of samples: {len(evaluation_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ee374",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Upload Dataset to Azure\n",
    "\n",
    "Upload the prepared dataset to Azure AI Project for centralized storage and evaluation.\n",
    "\n",
    "**Dataset Versioning:**\n",
    "- Use semantic versioning (e.g., \"1.0\", \"2.0\") for tracking changes\n",
    "- Store multiple versions for comparing evaluation results over time\n",
    "- Each version is immutable once uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68577630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataset to Azure AI Project\n",
    "# Each line in the JSONL file represents one evaluation sample\n",
    "\n",
    "dataset_name = os.environ.get(\"DATASET_NAME\", \"seattle-assistant-eval-dataset\")\n",
    "dataset_version = os.environ.get(\"DATASET_VERSION\", \"3.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a7ed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = project_client.datasets.upload_file(\n",
    "    name=dataset_name,\n",
    "    version=dataset_version,\n",
    "    file_path=dataset_file,\n",
    ")\n",
    "\n",
    "print(f\"Dataset uploaded successfully!\")\n",
    "print(f\"Dataset Name: {dataset.name}\")\n",
    "print(f\"Dataset Version: {dataset.version}\")\n",
    "print(f\"Dataset ID: {dataset.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8a0049",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Configure Evaluators\n",
    "\n",
    "Define which evaluators to run and how to map dataset fields to evaluator inputs.\n",
    "\n",
    "### Available Evaluator Types\n",
    "\n",
    "**Quality Evaluators (LLM-Judged):**\n",
    "- **Coherence**: Logical flow and consistency (requires: query, response)\n",
    "- **Relevance**: Response appropriateness (requires: query, context, response)\n",
    "- **Fluency**: Language quality and readability (requires: query, response)\n",
    "- **Groundedness**: Fidelity to context (requires: context, response)\n",
    "\n",
    "**Agent Evaluators:**\n",
    "- **Tool Call Accuracy**: Correctness of tool usage (requires: query, expected_tool_calls, actual_tool_calls)\n",
    "\n",
    "**Safety Evaluators** (configured separately via Azure AI Content Safety)\n",
    "\n",
    "### Data Mapping Syntax\n",
    "\n",
    "Use `${data.<field_name>}` to map dataset fields to evaluator inputs:\n",
    "```python\n",
    "data_mapping={\n",
    "    \"query\": \"${data.query}\",\n",
    "    \"response\": \"${data.response}\",\n",
    "    \"context\": \"${data.context}\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Model Configuration\n",
    "\n",
    "LLM-judged metrics require a capable model (GPT-4/GPT-4o) to act as the judge:\n",
    "```python\n",
    "model_config = {\n",
    "    \"azure_endpoint\": \"https://...\",\n",
    "    \"azure_deployment\": \"gpt-4o-mini\",\n",
    "    \"api_version\": \"2024-08-01-preview\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57148ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Evaluator configurations will be created using proper SDK models\n",
    "# in the next step using EvaluatorConfiguration and EvaluatorIds\n",
    "\n",
    "# Available evaluator IDs (from EvaluatorIds enum):\n",
    "# - COHERENCE: Measures how well the response flows logically\n",
    "# - RELEVANCE: Measures if the response is relevant to the query\n",
    "# - FLUENCY: Measures language quality\n",
    "# - GROUNDEDNESS: Measures if response is based on provided context\n",
    "# - F1_SCORE: Measures overlap between response and ground truth\n",
    "# - SIMILARITY: Measures semantic similarity\n",
    "# - TOOL_CALL_ACCURACY: Evaluates tool call correctness (for agent scenarios)\n",
    "\n",
    "print(\"Available Evaluators for Cloud-Based Evaluation:\")\n",
    "print(\"  Quality Metrics:\")\n",
    "print(\"    - Coherence (query + response)\")\n",
    "print(\"    - Relevance (query + context + response)\")\n",
    "print(\"    - Fluency (query + response)\")\n",
    "print(\"    - Groundedness (context + response)\")\n",
    "print(\"  Agent Metrics:\")\n",
    "print(\"    - Tool Call Accuracy (for agent tool usage)\")\n",
    "print(\"\\nNote: Safety evaluators are configured separately through Azure AI Content Safety\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da201f9a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Create and Run Evaluation Job\n",
    "\n",
    "Create an evaluation job that runs asynchronously in Azure's cloud infrastructure.\n",
    "\n",
    "**Evaluation Configuration:**\n",
    "- **Display Name**: Human-readable identifier for the evaluation\n",
    "- **Description**: Purpose and scope of the evaluation\n",
    "- **Data**: Reference to uploaded dataset by ID\n",
    "- **Evaluators**: Dictionary of configured evaluators\n",
    "- **Tags**: Metadata for governance and filtering\n",
    "\n",
    "**Job Lifecycle:**\n",
    "1. **Created**: Job submitted to Azure\n",
    "2. **Running**: Evaluation in progress (distributed execution)\n",
    "3. **Completed**: All metrics computed successfully\n",
    "4. **Failed**: Error occurred during evaluation (check logs)\n",
    "5. **Canceled**: Manually stopped by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee8c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import (\n",
    "    Evaluation,\n",
    "    EvaluatorConfiguration,\n",
    "    EvaluatorIds,\n",
    "    InputDataset\n",
    ")\n",
    "\n",
    "# Configure the model to use for LLM-judged metrics\n",
    "# Note: Pass model_config as a plain dictionary with snake_case keys\n",
    "# The evaluation service expects this format for proper model config validation\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ[\"AZURE_OPENAI_ENDPOINT_GPT_4o\"],\n",
    "    \"azure_deployment\": os.environ[\"AZURE_OPENAI_MODEl_GPT_4o\"],\n",
    "    \"api_version\": \"2024-08-01-preview\",\n",
    "}\n",
    "\n",
    "# Create evaluator configurations using data_mapping (not column_mapping)\n",
    "evaluators = {\n",
    "    \"coherence\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.COHERENCE,\n",
    "        data_mapping={\n",
    "            \"response\": \"${data.response}\",\n",
    "            \"query\": \"${data.query}\"\n",
    "        },\n",
    "        init_params={\"model_config\": model_config}\n",
    "    ),\n",
    "    \"relevance\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.RELEVANCE,\n",
    "        data_mapping={\n",
    "            \"response\": \"${data.response}\",\n",
    "            \"context\": \"${data.context}\",\n",
    "            \"query\": \"${data.query}\"\n",
    "        },\n",
    "        init_params={\"model_config\": model_config}\n",
    "    ),\n",
    "    \"fluency\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.FLUENCY,\n",
    "        data_mapping={\n",
    "            \"response\": \"${data.response}\",\n",
    "            \"query\": \"${data.query}\"\n",
    "        },\n",
    "        init_params={\"model_config\": model_config}\n",
    "    ),\n",
    "    \"groundedness\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.GROUNDEDNESS,\n",
    "        data_mapping={\n",
    "            \"response\": \"${data.response}\",\n",
    "            \"context\": \"${data.context}\"\n",
    "        },\n",
    "        init_params={\"model_config\": model_config}\n",
    "    ),\n",
    "    \"tool_call_accuracy\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.TOOL_CALL_ACCURACY,\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"expected_tool_calls\": \"${data.expected_tool_calls}\",\n",
    "            \"actual_tool_calls\": \"${data.actual_tool_calls}\"\n",
    "        },\n",
    "        init_params={\"model_config\": model_config}\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Create evaluation object with proper configuration\n",
    "# The 'data' parameter requires an InputDataset object with the dataset ID\n",
    "evaluation = Evaluation(\n",
    "    display_name=\"seattle-assistant-coherence-relevance-safety-eval\",\n",
    "    description=\"Cloud-based evaluation of Seattle assistant responses for quality metrics\",\n",
    "    data=InputDataset(id=dataset.id),\n",
    "    evaluators=evaluators,\n",
    "    # Optional tags for governance and tracking\n",
    "    tags={\n",
    "        \"environment\": \"development\",\n",
    "        \"evaluator_type\": \"quality_safety_agent\",\n",
    "        \"assistant\": \"seattle_tourist\",\n",
    "        \"data_classification\": \"test\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create evaluation job\n",
    "# Include model endpoint and API key in headers for LLM-judged metrics\n",
    "evaluation_response = project_client.evaluations.create(\n",
    "    evaluation,\n",
    "    headers={\n",
    "        \"model-endpoint\": os.environ[\"AZURE_OPENAI_ENDPOINT_GPT_4o\"],\n",
    "        \"api-key\": os.environ[\"AZURE_OPENAI_API_KEY_GPT_4o\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Evaluation Job Created Successfully!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Evaluation Name: {evaluation_response.name}\")\n",
    "print(f\"Status: {evaluation_response.status}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f2dbe4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Monitor Job Status\n",
    "\n",
    "Poll the evaluation job status until completion.\n",
    "\n",
    "**Monitoring Considerations:**\n",
    "- **Duration**: Depends on dataset size and number of evaluators\n",
    "- **Polling Interval**: 5-10 seconds is reasonable for most jobs\n",
    "- **Timeout**: Set a maximum wait time based on expected duration\n",
    "- **Error Handling**: Check for failure status and retrieve error details\n",
    "\n",
    "**Typical Duration Estimates:**\n",
    "- 10 samples, 3 evaluators: ~30-60 seconds\n",
    "- 50 samples, 5 evaluators: ~2-3 minutes\n",
    "- 100+ samples, 5+ evaluators: ~5-10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9879229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Poll for job completion\n",
    "max_attempts = 60  # Maximum 5 minutes (60 * 5 seconds)\n",
    "attempt = 0\n",
    "\n",
    "print(\"Polling evaluation job status...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "while attempt < max_attempts:\n",
    "    evaluation_response = project_client.evaluations.get(evaluation_response.name)\n",
    "    status = evaluation_response.status\n",
    "    \n",
    "    print(f\"[Attempt {attempt + 1}/{max_attempts}] Status: {status}\")\n",
    "    \n",
    "    if status in [\"Completed\", \"Failed\", \"Canceled\"]:\n",
    "        break\n",
    "    \n",
    "    time.sleep(5)  # Wait 5 seconds before next check\n",
    "    attempt += 1\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Final Status: {evaluation_response.status}\")\n",
    "\n",
    "if hasattr(evaluation_response, 'error') and evaluation_response.error:\n",
    "    print(f\"Error: {evaluation_response.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fb6cd4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: View and Analyze Results\n",
    "\n",
    "Retrieve evaluation results and access detailed metrics in Azure AI Foundry Studio.\n",
    "\n",
    "**Results Structure:**\n",
    "- **Summary Metrics**: Aggregate scores across all samples\n",
    "- **Instance Results**: Per-sample detailed scores (downloadable as JSONL)\n",
    "- **Studio URL**: Direct link to rich visualization dashboard\n",
    "\n",
    "**Accessing Detailed Results:**\n",
    "1. Use the Studio URL from evaluation response\n",
    "2. Navigate to \"Evaluations\" tab in Azure AI Foundry\n",
    "3. Download `instance_results.jsonl` for programmatic analysis\n",
    "4. View per-sample scores, reasoning, and distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c551c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Get evaluation details\n",
    "evaluation_response = project_client.evaluations.get(evaluation_response.name)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Evaluation Name: {evaluation_response.name}\")\n",
    "print(f\"Status: {evaluation_response.status}\")\n",
    "print(f\"Display Name: {evaluation_response.display_name}\")\n",
    "\n",
    "# Extract Studio URL from properties\n",
    "studio_url = None\n",
    "if hasattr(evaluation_response, 'properties') and evaluation_response.properties:\n",
    "    studio_url = evaluation_response.properties.get('AiStudioEvaluationUri')\n",
    "\n",
    "if studio_url:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"View detailed results in Azure AI Foundry Studio:\")\n",
    "    print(studio_url)\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"\\nStudio URL not available.\")\n",
    "\n",
    "# Try to get metrics from properties or results\n",
    "if hasattr(evaluation_response, 'properties') and evaluation_response.properties:\n",
    "    # Check if there are any metric-related properties\n",
    "    metric_keys = [k for k in evaluation_response.properties.keys() if 'metric' in k.lower() or 'score' in k.lower()]\n",
    "    if metric_keys:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"METRICS FROM PROPERTIES\")\n",
    "        print(\"=\" * 60)\n",
    "        for key in metric_keys:\n",
    "            print(f\"{key}: {evaluation_response.properties[key]}\")\n",
    "\n",
    "# If results are available in the evaluation object\n",
    "if hasattr(evaluation_response, 'results') and evaluation_response.results:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUMMARY METRICS\")\n",
    "    print(\"=\" * 60)\n",
    "    pprint(evaluation_response.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda53c07",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Cloud-Based Advantages**: Scalability, team collaboration, centralized governance\n",
    "2. **Dataset Versioning**: Track evaluation history with versioned datasets\n",
    "3. **Multiple Evaluators**: Combine quality, safety, and agent metrics for comprehensive assessment\n",
    "4. **Asynchronous Execution**: Jobs run in background; poll for completion\n",
    "5. **Rich Visualization**: Azure AI Foundry Studio provides detailed analysis tools\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "#### 1. Dataset Preparation\n",
    "- ✅ Use consistent field names across all samples\n",
    "- ✅ Validate JSONL format (one JSON object per line)\n",
    "- ✅ Include all required fields for your chosen evaluators\n",
    "- ✅ Start with small datasets (10-20 samples) for initial testing\n",
    "- ✅ Add diverse scenarios to capture edge cases\n",
    "\n",
    "#### 2. Evaluator Configuration\n",
    "- ✅ Map dataset fields correctly using `${data.<field_name>}` syntax\n",
    "- ✅ Choose evaluators appropriate for your use case (quality vs. safety vs. agent)\n",
    "- ✅ Use GPT-4 or GPT-4o for LLM-judged metrics (better reasoning)\n",
    "- ✅ Ensure sufficient API quota for evaluation workload\n",
    "- ✅ Test evaluator configs with small datasets first\n",
    "\n",
    "#### 3. Model Configuration\n",
    "- ✅ Use `azure_endpoint`, `azure_deployment`, `api_version` format\n",
    "- ✅ Pass model_config as dictionary with snake_case keys\n",
    "- ✅ Verify model deployment is active and has quota\n",
    "- ✅ Use same model version across evaluation runs for consistency\n",
    "\n",
    "#### 4. Governance and Compliance\n",
    "- ✅ Use tags to classify evaluations by:\n",
    "  - Environment (development, staging, production)\n",
    "  - Data classification (test, internal, customer)\n",
    "  - Purpose (debugging, validation, benchmarking)\n",
    "  - Team or project identifier\n",
    "- ✅ Document evaluation purposes and expected outcomes\n",
    "- ✅ Track data sensitivity levels for compliance\n",
    "- ✅ Archive evaluation results for audit trails\n",
    "\n",
    "#### 5. Results Analysis\n",
    "- ✅ Review aggregate metrics in evaluation response\n",
    "- ✅ Use Studio URL for detailed per-sample analysis\n",
    "- ✅ Download `instance_results.jsonl` for programmatic analysis\n",
    "- ✅ Track metrics over time to measure improvements\n",
    "- ✅ Identify low-scoring samples for targeted debugging\n",
    "- ✅ Use results to guide prompt engineering or model fine-tuning\n",
    "\n",
    "### Common Evaluator Data Mappings Reference\n",
    "\n",
    "| Evaluator | Required Fields | Mapping |\n",
    "|-----------|----------------|---------|\n",
    "| **Coherence** | query, response | `{\"query\": \"${data.query}\", \"response\": \"${data.response}\"}` |\n",
    "| **Fluency** | query, response | `{\"query\": \"${data.query}\", \"response\": \"${data.response}\"}` |\n",
    "| **Relevance** | query, context, response | `{\"query\": \"${data.query}\", \"context\": \"${data.context}\", \"response\": \"${data.response}\"}` |\n",
    "| **Groundedness** | context, response | `{\"context\": \"${data.context}\", \"response\": \"${data.response}\"}` |\n",
    "| **Tool Call Accuracy** | query, expected_tool_calls, actual_tool_calls | `{\"query\": \"${data.query}\", \"expected_tool_calls\": \"${data.expected_tool_calls}\", \"actual_tool_calls\": \"${data.actual_tool_calls}\"}` |\n",
    "\n",
    "### Quality Thresholds (Suggested)\n",
    "\n",
    "| Metric | Excellent | Good | Acceptable | Needs Improvement |\n",
    "|--------|-----------|------|------------|-------------------|\n",
    "| Coherence | 4.5-5.0 | 4.0-4.4 | 3.5-3.9 | < 3.5 |\n",
    "| Relevance | 4.5-5.0 | 4.0-4.4 | 3.5-3.9 | < 3.5 |\n",
    "| Fluency | 4.5-5.0 | 4.0-4.4 | 3.5-3.9 | < 3.5 |\n",
    "| Groundedness | 4.5-5.0 | 4.0-4.4 | 3.5-3.9 | < 3.5 |\n",
    "| Tool Call Accuracy | > 0.95 | 0.90-0.95 | 0.80-0.89 | < 0.80 |\n",
    "\n",
    "### Troubleshooting Common Issues\n",
    "\n",
    "| Issue | Possible Cause | Solution |\n",
    "|-------|---------------|----------|\n",
    "| Job fails immediately | Invalid data mapping | Verify field names match dataset |\n",
    "| Low groundedness scores | Response hallucinates | Improve retrieval or add context constraints |\n",
    "| Evaluation timeout | Large dataset | Split into smaller batches |\n",
    "| Missing results | Job still running | Poll longer or check Studio UI |\n",
    "| Tool call accuracy = 0 | Empty tool call fields | Ensure expected/actual_tool_calls are populated |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Iterate on Prompts**: Use low-scoring samples to refine system prompts\n",
    "2. **Expand Dataset**: Add more diverse scenarios based on production use cases\n",
    "3. **Automate Evaluations**: Integrate into CI/CD pipelines for continuous quality monitoring\n",
    "4. **Compare Versions**: Run evaluations on different model versions to measure improvements\n",
    "5. **Custom Evaluators**: Build domain-specific evaluators for specialized metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ff20df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Understanding Evaluation Results Structure\n",
    "\n",
    "### Instance Results Format\n",
    "\n",
    "The `instance_results.jsonl` file contains per-sample detailed metrics. Each line is a JSON object with:\n",
    "\n",
    "**Input Fields** (from your dataset):\n",
    "```json\n",
    "{\n",
    "  \"query\": \"What's the weather like in Seattle?\",\n",
    "  \"context\": \"Seattle typically has rainy weather...\",\n",
    "  \"response\": \"The current weather in Seattle is...\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Evaluator Scores**:\n",
    "```json\n",
    "{\n",
    "  \"coherence.score\": 4.5,\n",
    "  \"coherence.reason\": \"Response flows logically...\",\n",
    "  \"relevance.score\": 5.0,\n",
    "  \"relevance.reason\": \"Directly addresses the query...\",\n",
    "  \"fluency.score\": 4.0,\n",
    "  \"groundedness.score\": 4.5,\n",
    "  \"tool_call_accuracy.score\": 1.0\n",
    "}\n",
    "```\n",
    "\n",
    "### Score Interpretation\n",
    "\n",
    "| Metric | Scale | Interpretation |\n",
    "|--------|-------|----------------|\n",
    "| Coherence, Relevance, Fluency, Groundedness | 1-5 | 1=poor, 3=acceptable, 5=excellent |\n",
    "| Tool Call Accuracy | 0-1 | 0=incorrect, 1=correct (binary) |\n",
    "\n",
    "### Programmatic Analysis Example\n",
    "\n",
    "After downloading `instance_results.jsonl`:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load results\n",
    "results = []\n",
    "with open('instance_results.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        results.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Identify low-performing samples\n",
    "low_relevance = df[df['relevance.score'] < 3.0]\n",
    "print(f\"Samples with low relevance: {len(low_relevance)}\")\n",
    "\n",
    "# Calculate aggregate metrics\n",
    "print(f\"Average coherence: {df['coherence.score'].mean():.2f}\")\n",
    "print(f\"Average relevance: {df['relevance.score'].mean():.2f}\")\n",
    "\n",
    "# Check metric correlation\n",
    "correlation = df[['coherence.score', 'relevance.score']].corr()\n",
    "print(correlation)\n",
    "\n",
    "# Find samples needing improvement\n",
    "needs_improvement = df[\n",
    "    (df['coherence.score'] < 3.5) | \n",
    "    (df['relevance.score'] < 3.5)\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c42682",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "### Official Documentation\n",
    "- [Azure AI Evaluation SDK Documentation](https://learn.microsoft.com/azure/ai-studio/how-to/evaluate-sdk)\n",
    "- [Evaluator Types and Metrics Reference](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics)\n",
    "- [Azure AI Foundry Studio](https://ai.azure.com)\n",
    "- [Evaluation Results Analysis Guide](https://learn.microsoft.com/azure/ai-studio/how-to/evaluate-results)\n",
    "- [Cloud-Based Evaluation Tutorial](https://learn.microsoft.com/azure/ai-studio/how-to/evaluate-cloud)\n",
    "\n",
    "### Related Notebooks\n",
    "- `03_rag_evaluation.ipynb`: Local RAG-specific evaluators\n",
    "- `02_simulator_eval.ipynb`: Agent conversation testing\n",
    "- `04_agent_evaluation.ipynb`: Agent-specific metrics\n",
    "\n",
    "### Azure AI Project Setup\n",
    "- [Create an Azure AI Project](https://learn.microsoft.com/azure/ai-studio/how-to/create-projects)\n",
    "- [Configure Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource)\n",
    "- [Manage Quotas and Limits](https://learn.microsoft.com/azure/ai-studio/how-to/quota)\n",
    "\n",
    "### Code Samples\n",
    "- [Azure AI Evaluation Samples on GitHub](https://github.com/Azure-Samples/azureai-samples)\n",
    "- [Evaluation SDK Examples](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
