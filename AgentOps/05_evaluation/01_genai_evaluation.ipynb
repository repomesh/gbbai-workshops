{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895e6de8",
   "metadata": {},
   "source": [
    "# Local Agent Evaluation with Azure AI Evaluation SDK\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to perform **local evaluation** of Azure AI agents using the Azure AI Evaluation SDK. Local evaluation runs entirely on your development machine, providing fast feedback loops for iterative agent development.\n",
    "\n",
    "### What is Local Agent Evaluation?\n",
    "\n",
    "Local evaluation allows you to:\n",
    "- **Test agent responses** against quality metrics (Relevance, Coherence, Fluency)\n",
    "- **Run evaluations locally** without uploading data to the cloud\n",
    "- **Iterate quickly** with immediate feedback on prompt/configuration changes\n",
    "- **Analyze results** programmatically with full control over the evaluation pipeline\n",
    "- **Save evaluation history** for tracking improvements over time\n",
    "\n",
    "### Evaluation Workflow\n",
    "\n",
    "```\n",
    "1. Define Test Prompts\n",
    "   â†“\n",
    "2. Collect Agent Responses (via Azure AI Agent API)\n",
    "   â†“\n",
    "3. Initialize Evaluators (Relevance, Coherence, Fluency)\n",
    "   â†“\n",
    "4. Run Evaluations Locally\n",
    "   â†“\n",
    "5. Analyze Results & Calculate Metrics\n",
    "   â†“\n",
    "6. Save Results for Historical Tracking\n",
    "   â†“\n",
    "7. Iterate on Agent Configuration\n",
    "```\n",
    "\n",
    "### Quality Metrics Explained\n",
    "\n",
    "**Relevance Evaluator:**\n",
    "- **Purpose**: Measures whether the response appropriately addresses the query\n",
    "- **Input**: Query + Response\n",
    "- **Output**: Score 1-5 (1=irrelevant, 5=highly relevant)\n",
    "- **Use Case**: Ensure agent stays on topic and answers the question\n",
    "\n",
    "**Coherence Evaluator:**\n",
    "- **Purpose**: Measures logical flow and consistency of the response\n",
    "- **Input**: Query + Response\n",
    "- **Output**: Score 1-5 (1=incoherent, 5=highly coherent)\n",
    "- **Use Case**: Detect rambling, contradictions, or disjointed reasoning\n",
    "\n",
    "**Fluency Evaluator:**\n",
    "- **Purpose**: Measures language quality, grammar, and readability\n",
    "- **Input**: Query + Response\n",
    "- **Output**: Score 1-5 (1=poor language, 5=excellent fluency)\n",
    "- **Use Case**: Ensure professional, well-written responses\n",
    "\n",
    "### When to Use Local vs. Cloud Evaluation\n",
    "\n",
    "**Use Local Evaluation (This Notebook) When:**\n",
    "- Rapid prototyping and iterative development\n",
    "- Small test sets (< 50 queries)\n",
    "- Quick feedback loops during development\n",
    "- Debugging specific agent behaviors\n",
    "- Working offline or with sensitive data\n",
    "\n",
    "**Use Cloud Evaluation (`05_cloud_based_evaluation.ipynb`) When:**\n",
    "- Large datasets (100+ samples)\n",
    "- Team collaboration and shared results\n",
    "- Production quality gates in CI/CD pipelines\n",
    "- Centralized governance and audit trails\n",
    "- Historical comparison across multiple runs\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Azure AI Project with deployed agent\n",
    "- Azure OpenAI deployment (GPT-4/GPT-4o for evaluation)\n",
    "- Azure credentials configured (`az login`)\n",
    "- Environment variables set in `.env` file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2130d7d",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Part 1: Environment Setup](#part-1-environment-setup)\n",
    "   - 1.1: Install Dependencies\n",
    "   - 1.2: Configure Environment\n",
    "2. [Part 2: Define Test Prompts](#part-2-define-test-prompts)\n",
    "3. [Part 3: Collect Agent Responses](#part-3-collect-agent-responses)\n",
    "4. [Part 4: Initialize Evaluators](#part-4-initialize-evaluators)\n",
    "5. [Part 5: Run Evaluations](#part-5-run-evaluations)\n",
    "6. [Part 6: Analyze Results](#part-6-analyze-results)\n",
    "7. [Part 7: Save Evaluation History](#part-7-save-evaluation-history)\n",
    "8. [Summary and Best Practices](#summary-and-best-practices)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4db833",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "### 1.1: Install Dependencies\n",
    "\n",
    "Install the Azure AI Evaluation SDK for local evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e005ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-evaluation==1.13.5 -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c841b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "new_path_entry = \"/opt/homebrew/bin\"  # Replace with the directory you want to add\n",
    "current_path = os.environ.get('PATH', '')\n",
    "\n",
    "if new_path_entry not in current_path.split(os.pathsep):\n",
    "    os.environ['PATH'] = new_path_entry + os.pathsep + current_path\n",
    "    print(f\"Updated PATH for this session: {os.environ['PATH']}\")\n",
    "else:\n",
    "    print(f\"PATH already contains {new_path_entry}: {current_path}\")\n",
    "\n",
    "# You can then verify with shutil.which again\n",
    "print(f\"Location of 'az' found by kernel now: {shutil.which('az')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a31018e",
   "metadata": {},
   "source": [
    "### 1.2: Configure Environment\n",
    "\n",
    "Set up Azure AI Project client and load configuration from environment variables.\n",
    "\n",
    "**Required Environment Variables:**\n",
    "- `AZURE_AI_PROJECT_ENDPOINT`: Your Azure AI Foundry project endpoint\n",
    "- `AZURE_OPENAI_API_KEY_GPT_4o`: API key for the evaluation model\n",
    "- `AZURE_OPENAI_ENDPOINT_GPT_4o`: Azure OpenAI endpoint\n",
    "- `AZURE_OPENAI_MODEl_GPT_4o`: Deployment name (e.g., gpt-4o-mini)\n",
    "- `TARGET_AGENT_ID`: The agent ID to evaluate (optional, can be set in code)\n",
    "\n",
    "**Authentication:**\n",
    "- Use `az login` for DefaultAzureCredential\n",
    "- Ensure you have access to the Azure AI Project and OpenAI resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.evaluation import RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator\n",
    "\n",
    "# Add parent directory to path for agent_utils import\n",
    "parent_dir = Path(__file__).parent.parent if hasattr(__builtins__, '__file__') else Path.cwd().parent\n",
    "sys.path.insert(0, str(parent_dir / \"utils\"))\n",
    "\n",
    "from agent_utils import AgentManager\n",
    "\n",
    "# Load environment variables from parent directory\n",
    "agent_ops_dir = Path.cwd().parent if Path.cwd().name == \"05_evaluation\" else Path.cwd()\n",
    "env_path = agent_ops_dir / \".env\"\n",
    "load_dotenv(env_path)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(name)s: %(message)s\")\n",
    "logger = logging.getLogger(\"agent_eval\")\n",
    "\n",
    "# Suppress verbose Azure SDK and HTTP logging\n",
    "logging.getLogger(\"azure.core.pipeline.policies.http_logging_policy\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"azure.identity\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"azure.cosmos._cosmos_http_logging_policy\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING) \n",
    "logging.getLogger(\"openai\").setLevel(logging.WARNING)  \n",
    "\n",
    "# Initialize Azure AI Project Client with endpoint\n",
    "endpoint = os.getenv(\"AZURE_AI_PROJECT_ENDPOINT\")\n",
    "if not endpoint:\n",
    "    raise ValueError(\"Set AZURE_AI_PROJECT_ENDPOINT in .env file\")\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "project_client = AIProjectClient(endpoint=endpoint, credential=credential)\n",
    "agent_manager = AgentManager(project_client)\n",
    "logger.info(\"âœ… Connected to Azure AI project\")\n",
    "\n",
    "# Get Azure OpenAI configuration from .env\n",
    "model_api_key = os.getenv(\"AZURE_OPENAI_API_KEY_GPT_4o\")\n",
    "if not model_api_key:\n",
    "    raise ValueError(\"Set AZURE_OPENAI_API_KEY_GPT_4o in .env file\")\n",
    "\n",
    "model_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT_GPT_4o\")\n",
    "if not model_endpoint:\n",
    "    raise ValueError(\"Set AZURE_OPENAI_ENDPOINT_GPT_4o in .env file\")\n",
    "\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_MODEl_GPT_4o\", \"gpt-4o\")\n",
    "\n",
    "logger.info(f\"âœ… Evaluation will use deployment '{deployment_name}' at endpoint '{model_endpoint}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b92662",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Define Test Prompts\n",
    "\n",
    "Create a set of test prompts that represent typical user queries for your agent.\n",
    "\n",
    "**Best Practices:**\n",
    "- Include diverse question types (factual, explanatory, comparative)\n",
    "- Cover core agent capabilities\n",
    "- Add edge cases and challenging queries\n",
    "- Start with 3-10 prompts for quick iteration\n",
    "- Expand to 20-50 prompts for comprehensive evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd04229",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Collect Agent Responses\n",
    "\n",
    "Run the target agent on each test prompt and collect responses for evaluation.\n",
    "\n",
    "**Process:**\n",
    "1. Create a new thread for each query (isolated context)\n",
    "2. Send prompt to the agent\n",
    "3. Capture the response text\n",
    "4. Store query-response pairs for evaluation\n",
    "5. Clean up threads after completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_AGENT_ID = 'asst_3pPWPYFexU3fEwbYB3VDWO1N'\n",
    "\n",
    "PROMPTS: List[str] = [\n",
    "    \"Summarize the Azure AI Foundry service in two sentences.\",\n",
    "    \"List three responsible AI considerations when deploying an agent to production.\",\n",
    "    \"Explain how prompt caching can improve latency for frequently repeated questions.\",\n",
    "]\n",
    "\n",
    "evaluation_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "for prompt in PROMPTS:\n",
    "    thread = agent_manager.create_thread()\n",
    "    try:\n",
    "        response_text = agent_manager.run_agent_simple(\n",
    "            thread_id=thread.id,\n",
    "            agent_id=TARGET_AGENT_ID,\n",
    "            user_message=prompt,\n",
    "            verbose=False,\n",
    "        )\n",
    "        if not response_text:\n",
    "            logger.warning(\"Agent returned an empty response for prompt: %s\", prompt)\n",
    "            continue\n",
    "        evaluation_rows.append({\"query\": prompt, \"response\": response_text})\n",
    "        logger.info(\"Captured response for prompt: %s\", prompt)\n",
    "    except Exception as exc:\n",
    "        logger.exception(\"Unable to capture response for prompt '%s': %s\", prompt, exc)\n",
    "    finally:\n",
    "        agent_manager.delete_thread(thread.id, silent=True)\n",
    "\n",
    "if not evaluation_rows:\n",
    "    raise RuntimeError(\"No agent responses captured; aborting evaluation.\")\n",
    "\n",
    "logger.info(\"Collected %d agent responses for evaluation.\", len(evaluation_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5da741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Display as Markdown table with full text (no truncation)\n",
    "markdown_output = \"# Collected Agent Responses\\n\\n\"\n",
    "markdown_output += \"| # | Query | Response |\\n\"\n",
    "markdown_output += \"|---|-------|----------|\\n\"\n",
    "\n",
    "for i, row in enumerate(evaluation_rows, 1):\n",
    "    query = row['query'].replace('|', '\\\\|').replace('\\n', '<br>')\n",
    "    response = row['response'].replace('|', '\\\\|').replace('\\n', '<br>')\n",
    "    markdown_output += f\"| {i} | {query} | {response} |\\n\"\n",
    "\n",
    "display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f60a17",
   "metadata": {},
   "source": [
    "### 3.1: Save Dataset to JSONL\n",
    "\n",
    "Persist the collected responses to JSONL format for reproducibility and historical tracking.\n",
    "\n",
    "**JSONL Format:**\n",
    "- One JSON object per line\n",
    "- Each object contains `query` and `response` fields\n",
    "- Timestamped filename for versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%d%H%M%S\")\n",
    "# Use absolute path relative to notebook location\n",
    "notebook_dir = Path.cwd() if Path.cwd().name == \"05_evaluation\" else Path.cwd() / \"05_evaluation\"\n",
    "dataset_dir = notebook_dir / \"data\"\n",
    "dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "dataset_path = dataset_dir / f\"eval_{timestamp}.jsonl\"\n",
    "\n",
    "with dataset_path.open(\"w\", encoding=\"utf-8\") as handle:\n",
    "    for row in evaluation_rows:\n",
    "        handle.write(json.dumps(row, ensure_ascii=True) + \"\\n\")\n",
    "\n",
    "logger.info(f\"âœ… Wrote evaluation dataset to {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd911ff2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Initialize Evaluators\n",
    "\n",
    "Configure the quality evaluators with the Azure OpenAI model for LLM-judged metrics.\n",
    "\n",
    "**Evaluator Configuration:**\n",
    "- All three evaluators use the same model configuration\n",
    "- GPT-4 or GPT-4o recommended for best evaluation quality\n",
    "- API key and endpoint must match your Azure OpenAI deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c60f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "# Configure model for evaluators\n",
    "model_config = {\n",
    "    \"azure_endpoint\": model_endpoint,\n",
    "    \"api_key\": model_api_key,\n",
    "    \"azure_deployment\": deployment_name,\n",
    "    \"api_version\": \"2024-08-01-preview\"\n",
    "}\n",
    "\n",
    "# Initialize evaluators\n",
    "relevance_eval = RelevanceEvaluator(model_config)\n",
    "coherence_eval = CoherenceEvaluator(model_config)\n",
    "fluency_eval = FluencyEvaluator(model_config)\n",
    "\n",
    "logger.info(\"âœ… Evaluators initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470b339e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Run Evaluations\n",
    "\n",
    "Execute all three evaluators on each collected response.\n",
    "\n",
    "**Evaluation Process:**\n",
    "- Each evaluator receives the query and response\n",
    "- LLM judges the quality based on specific criteria\n",
    "- Returns a score (typically 1-5 scale)\n",
    "- Results include both score and reasoning (if available)\n",
    "\n",
    "**Error Handling:**\n",
    "- Wrap evaluator calls in try-except for robustness\n",
    "- Log evaluation progress for debugging\n",
    "- Continue evaluation even if individual samples fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79c9012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on each response\n",
    "results = []\n",
    "\n",
    "for row in evaluation_rows:\n",
    "    query = row[\"query\"]\n",
    "    response = row[\"response\"]\n",
    "    \n",
    "    logger.info(f\"Evaluating: {query[:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Run evaluators\n",
    "        relevance_score = relevance_eval(query=query, response=response)\n",
    "        coherence_score = coherence_eval(query=query, response=response)\n",
    "        fluency_score = fluency_eval(query=query, response=response)\n",
    "        \n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"relevance\": relevance_score.get(\"relevance\", relevance_score),\n",
    "            \"coherence\": coherence_score.get(\"coherence\", coherence_score),\n",
    "            \"fluency\": fluency_score.get(\"fluency\", fluency_score)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error evaluating query '{query[:50]}...': {str(e)}\")\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"relevance\": None,\n",
    "            \"coherence\": None,\n",
    "            \"fluency\": None,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "    \n",
    "logger.info(f\"âœ… Evaluation completed for {len(results)} responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70eca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "df = pd.DataFrame(results)\n",
    "display(df)\n",
    "\n",
    "# Calculate averages (exclude None values from errors)\n",
    "avg_relevance = df[\"relevance\"].dropna().mean()\n",
    "avg_coherence = df[\"coherence\"].dropna().mean()\n",
    "avg_fluency = df[\"fluency\"].dropna().mean()\n",
    "\n",
    "logger.info(f\"\\nðŸ“Š Average Scores:\")\n",
    "logger.info(f\"  Relevance: {avg_relevance:.2f}\")\n",
    "logger.info(f\"  Coherence: {avg_coherence:.2f}\")\n",
    "logger.info(f\"  Fluency: {avg_fluency:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46412748",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Analyze Results\n",
    "\n",
    "Review evaluation metrics and identify areas for improvement.\n",
    "\n",
    "**Analysis Checklist:**\n",
    "- âœ… Review aggregate metrics (mean, min, max)\n",
    "- âœ… Identify low-scoring responses (< 3.0)\n",
    "- âœ… Check for consistency across metrics\n",
    "- âœ… Spot patterns in failing queries\n",
    "- âœ… Compare with quality thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a89aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Statistics by metric\n",
    "for metric in [\"relevance\", \"coherence\", \"fluency\"]:\n",
    "    scores = df[metric].dropna()\n",
    "    print(f\"\\n{metric.upper()} Statistics:\")\n",
    "    print(f\"  Mean: {scores.mean():.2f}\")\n",
    "    print(f\"  Median: {scores.median():.2f}\")\n",
    "    print(f\"  Min: {scores.min():.2f}\")\n",
    "    print(f\"  Max: {scores.max():.2f}\")\n",
    "    print(f\"  Std Dev: {scores.std():.2f}\")\n",
    "\n",
    "# Identify low-scoring responses\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOW-SCORING RESPONSES (< 3.0)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "low_threshold = 3.0\n",
    "low_scores = df[(df[\"relevance\"] < low_threshold) | \n",
    "                 (df[\"coherence\"] < low_threshold) | \n",
    "                 (df[\"fluency\"] < low_threshold)]\n",
    "\n",
    "if len(low_scores) > 0:\n",
    "    for idx, row in low_scores.iterrows():\n",
    "        print(f\"\\nQuery: {row['query']}\")\n",
    "        print(f\"  Relevance: {row['relevance']:.2f}\")\n",
    "        print(f\"  Coherence: {row['coherence']:.2f}\")\n",
    "        print(f\"  Fluency: {row['fluency']:.2f}\")\n",
    "else:\n",
    "    print(\"âœ… All responses meet the quality threshold!\")\n",
    "\n",
    "# Quality assessment\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUALITY ASSESSMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def assess_quality(score):\n",
    "    if score >= 4.5:\n",
    "        return \"Excellent\"\n",
    "    elif score >= 4.0:\n",
    "        return \"Good\"\n",
    "    elif score >= 3.5:\n",
    "        return \"Acceptable\"\n",
    "    else:\n",
    "        return \"Needs Improvement\"\n",
    "\n",
    "print(f\"Overall Relevance: {assess_quality(avg_relevance)} ({avg_relevance:.2f})\")\n",
    "print(f\"Overall Coherence: {assess_quality(avg_coherence)} ({avg_coherence:.2f})\")\n",
    "print(f\"Overall Fluency: {assess_quality(avg_fluency)} ({avg_fluency:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211134dd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Save Evaluation History\n",
    "\n",
    "Persist evaluation results for historical tracking and comparison.\n",
    "\n",
    "**Saved Data:**\n",
    "- Timestamp and agent metadata\n",
    "- Individual query-response-scores\n",
    "- Aggregate metrics\n",
    "- Model configuration used for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "results_path = dataset_dir / f\"eval_results_{timestamp}.json\"\n",
    "with results_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"timestamp\": timestamp,\n",
    "        \"agent_id\": TARGET_AGENT_ID,\n",
    "        \"model\": deployment_name,\n",
    "        \"results\": results,\n",
    "        \"averages\": {\n",
    "            \"relevance\": float(avg_relevance),\n",
    "            \"coherence\": float(avg_coherence),\n",
    "            \"fluency\": float(avg_fluency)\n",
    "        }\n",
    "    }, f, indent=2)\n",
    "\n",
    "logger.info(f\"âœ… Saved results to {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16acf1b9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Local Evaluation Benefits**: Fast feedback loops, no cloud upload, full control\n",
    "2. **Quality Metrics**: Relevance, Coherence, Fluency provide comprehensive assessment\n",
    "3. **Iterative Development**: Run evaluations frequently during development\n",
    "4. **Historical Tracking**: Save results with timestamps for trend analysis\n",
    "5. **Quick Debugging**: Identify problematic queries and iterate on prompts\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "#### 1. Test Prompt Design\n",
    "- âœ… Start with 3-10 representative queries\n",
    "- âœ… Include diverse question types (factual, explanatory, comparative)\n",
    "- âœ… Add edge cases and challenging scenarios\n",
    "- âœ… Cover all core agent capabilities\n",
    "- âœ… Expand to 20-50 prompts for comprehensive coverage\n",
    "\n",
    "#### 2. Evaluation Frequency\n",
    "- âœ… Run after every significant prompt change\n",
    "- âœ… Evaluate before deploying to staging/production\n",
    "- âœ… Create baseline evaluations for comparison\n",
    "- âœ… Track metrics over time to detect regressions\n",
    "- âœ… Automate with CI/CD for continuous monitoring\n",
    "\n",
    "#### 3. Model Configuration for Evaluation\n",
    "- âœ… Use GPT-4 or GPT-4o for best evaluation quality\n",
    "- âœ… Ensure sufficient API quota for evaluation workload\n",
    "- âœ… Use consistent model version across runs\n",
    "- âœ… Test evaluators on known good/bad responses first\n",
    "\n",
    "#### 4. Results Analysis\n",
    "- âœ… Review individual scores, not just averages\n",
    "- âœ… Investigate low-scoring responses (< 3.0)\n",
    "- âœ… Look for patterns in failing queries\n",
    "- âœ… Compare metrics across different agent versions\n",
    "- âœ… Set quality thresholds based on use case requirements\n",
    "\n",
    "#### 5. Error Handling\n",
    "- âœ… Wrap evaluator calls in try-except blocks\n",
    "- âœ… Log evaluation progress for debugging\n",
    "- âœ… Continue evaluation even if individual samples fail\n",
    "- âœ… Store error information for troubleshooting\n",
    "- âœ… Monitor API rate limits and quota usage\n",
    "\n",
    "### Quality Thresholds (Recommended)\n",
    "\n",
    "| Metric | Excellent | Good | Acceptable | Needs Improvement |\n",
    "|--------|-----------|------|------------|-------------------|\n",
    "| Relevance | 4.5-5.0 | 4.0-4.4 | 3.5-3.9 | < 3.5 |\n",
    "| Coherence | 4.5-5.0 | 4.0-4.4 | 3.5-3.9 | < 3.5 |\n",
    "| Fluency | 4.5-5.0 | 4.0-4.4 | 3.5-3.9 | < 3.5 |\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "| Issue | Possible Cause | Solution |\n",
    "|-------|---------------|----------|\n",
    "| Low Relevance | Agent off-topic or misunderstands query | Improve system prompt clarity |\n",
    "| Low Coherence | Rambling or contradictory responses | Add output structure guidelines |\n",
    "| Low Fluency | Grammar or formatting issues | Review prompt examples, adjust temperature |\n",
    "| All scores low | Evaluation model misconfigured | Verify model_config parameters |\n",
    "| Evaluation errors | API rate limits or quota | Add retry logic, check quota |\n",
    "\n",
    "### Expanding Your Evaluation\n",
    "\n",
    "**Add More Evaluators:**\n",
    "```python\n",
    "from azure.ai.evaluation import (\n",
    "    GroundednessEvaluator,  # For RAG scenarios\n",
    "    F1ScoreEvaluator,        # For exact match comparison\n",
    "    SimilarityEvaluator      # For semantic similarity\n",
    ")\n",
    "```\n",
    "\n",
    "**Add Context for RAG Evaluation:**\n",
    "```python\n",
    "# Include retrieved context in your evaluation rows\n",
    "evaluation_rows.append({\n",
    "    \"query\": prompt,\n",
    "    \"context\": retrieved_context,  # Add retrieved documents\n",
    "    \"response\": response_text\n",
    "})\n",
    "\n",
    "# Use GroundednessEvaluator\n",
    "groundedness_eval = GroundednessEvaluator(model_config)\n",
    "score = groundedness_eval(context=context, response=response)\n",
    "```\n",
    "\n",
    "**Compare Agent Versions:**\n",
    "```python\n",
    "# Evaluate multiple agent versions with same prompts\n",
    "agents_to_compare = [\n",
    "    {\"id\": \"agent_v1\", \"name\": \"Baseline\"},\n",
    "    {\"id\": \"agent_v2\", \"name\": \"Improved Prompt\"},\n",
    "    {\"id\": \"agent_v3\", \"name\": \"With Tools\"}\n",
    "]\n",
    "\n",
    "for agent in agents_to_compare:\n",
    "    # Run evaluation for each agent\n",
    "    # Compare metrics side-by-side\n",
    "```\n",
    "\n",
    "### Integration with CI/CD\n",
    "\n",
    "**Example GitHub Actions Workflow:**\n",
    "```yaml\n",
    "name: Agent Quality Gate\n",
    "on: [pull_request]\n",
    "jobs:\n",
    "  evaluate:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v2\n",
    "      - name: Run Agent Evaluation\n",
    "        run: python evaluate_agent.py\n",
    "      - name: Check Quality Thresholds\n",
    "        run: |\n",
    "          if [ $(jq '.averages.relevance' results.json) -lt 4.0 ]; then\n",
    "            echo \"Quality gate failed: Relevance below threshold\"\n",
    "            exit 1\n",
    "          fi\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Expand Test Coverage**: Add more diverse queries covering edge cases\n",
    "2. **Add Groundedness**: For RAG scenarios, evaluate context fidelity\n",
    "3. **Track Over Time**: Create dashboard comparing evaluations across versions\n",
    "4. **Automate**: Integrate into CI/CD for continuous quality monitoring\n",
    "5. **Custom Evaluators**: Build domain-specific metrics for specialized use cases\n",
    "6. **Cloud Evaluation**: Use `05_cloud_based_evaluation.ipynb` for large-scale testing\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Azure AI Evaluation SDK Documentation](https://learn.microsoft.com/azure/ai-studio/how-to/develop/evaluate-sdk)\n",
    "- [Built-in Evaluators Reference](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics)\n",
    "- [Custom Evaluators Guide](https://learn.microsoft.com/azure/ai-studio/how-to/develop/evaluate-sdk#custom-evaluators)\n",
    "- [Agent Evaluation Best Practices](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-approach-gen-ai)\n",
    "\n",
    "### Related Notebooks\n",
    "\n",
    "- `02_simulator_eval.ipynb`: Agent conversation testing with multiple scenarios\n",
    "- `03_rag_evaluation.ipynb`: RAG-specific evaluators (Retrieval, Groundedness, etc.)\n",
    "- `05_cloud_based_evaluation.ipynb`: Cloud-based evaluation for large datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
