{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4414434e",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) Evaluations\n",
    "\n",
    "This notebook demonstrates how to evaluate Retrieval-Augmented Generation (RAG) systems using Azure AI evaluation tools.\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "A RAG system generates the most relevant answer consistent with grounding documents in response to a user's query. At a high level:\n",
    "1. User's query triggers a search retrieval in the corpus of grounding documents\n",
    "2. Retrieved documents provide grounding context for the AI model\n",
    "3. AI model generates a response based on the context\n",
    "\n",
    "## RAG Evaluation Aspects\n",
    "\n",
    "RAG evaluations address three critical aspects:\n",
    "\n",
    "1. **Relevance of Retrieval Results to Query**\n",
    "   - **Document Retrieval**: Use when you have ground truth labels (qrels) for accurate measurements\n",
    "   - **Retrieval**: Use when you only have retrieved context without labels\n",
    "\n",
    "2. **Consistency of Generated Response with Grounding Documents**\n",
    "   - **Groundedness**: Customizable LLM-judge prompt for groundedness definition\n",
    "   - **Groundedness Pro**: Straightforward definition powered by Azure AI Content Safety\n",
    "\n",
    "3. **Relevance of Final Response to Query**\n",
    "   - **Relevance**: Use when you don't have ground truth\n",
    "   - **Response Completeness**: Use when you have ground truth and want to ensure no critical information is missed\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Groundedness** = Precision aspect (shouldn't contain content outside grounding context)\n",
    "- **Response Completeness** = Recall aspect (shouldn't miss critical information compared to ground truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955830df",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Environment Setup](#environment-setup)\n",
    "2. [Model Configuration](#model-configuration)\n",
    "3. [Retrieval Evaluators](#retrieval-evaluators)\n",
    "   - 3.1: Retrieval Evaluator\n",
    "   - 3.2: Document Retrieval Evaluator\n",
    "4. [Groundedness Evaluators](#groundedness-evaluators)\n",
    "   - 4.1: Groundedness Evaluator\n",
    "   - 4.2: Groundedness Pro Evaluator\n",
    "5. [Response Quality Evaluators](#response-quality-evaluators)\n",
    "   - 5.1: Relevance Evaluator\n",
    "   - 5.2: Response Completeness Evaluator\n",
    "6. [Complete RAG Evaluation Example](#complete-rag-evaluation-example)\n",
    "7. [Summary and Best Practices](#summary-and-best-practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d7bf8f",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Load environment variables and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb75440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "new_path_entry = \"/opt/homebrew/bin\"  # Replace with the directory you want to add\n",
    "current_path = os.environ.get('PATH', '')\n",
    "\n",
    "if new_path_entry not in current_path.split(os.pathsep):\n",
    "    os.environ['PATH'] = new_path_entry + os.pathsep + current_path\n",
    "    print(f\"Updated PATH for this session: {os.environ['PATH']}\")\n",
    "else:\n",
    "    print(f\"PATH already contains {new_path_entry}: {current_path}\")\n",
    "\n",
    "# You can then verify with shutil.which again\n",
    "print(f\"Location of 'az' found by kernel now: {shutil.which('az')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add parent directory to path\n",
    "parent_dir = Path(__file__).parent.parent if hasattr(__builtins__, '__file__') else Path.cwd().parent\n",
    "sys.path.insert(0, str(parent_dir / \"utils\"))\n",
    "\n",
    "# Load environment variables\n",
    "agent_ops_dir = Path.cwd().parent if Path.cwd().name == \"05_evaluation\" else Path.cwd()\n",
    "env_path = agent_ops_dir / \".env\"\n",
    "load_dotenv(env_path)\n",
    "\n",
    "print(\"✅ Environment loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f8ee22",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "Configure the LLM-judge model for AI-assisted evaluators. All RAG evaluators except Groundedness Pro use this configuration.\n",
    "\n",
    "### Supported Models\n",
    "\n",
    "| Evaluators | Reasoning Models (o-series) | Non-reasoning Models (GPT-4/4o) | Enable Reasoning |\n",
    "|------------|----------------------------|--------------------------------|------------------|\n",
    "| Retrieval, Groundedness, Relevance, Response Completeness | ✅ Supported | ✅ Supported | `is_reasoning_model=True` |\n",
    "| Groundedness Pro | ❌ Not Supported | ✅ Supported | N/A (uses Azure AI Content Safety) |\n",
    "\n",
    "**Recommendation**: For complex evaluations, use reasoning models like `gpt-4.1-mini` for balanced performance and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0e0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration\n",
    "from pprint import pprint\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT_GPT_4o\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY_GPT_4o\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION_GPT_4o\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_MODEl_GPT_4o\"],\n",
    ")\n",
    "\n",
    "print(\"✅ Model configuration created successfully\")\n",
    "print(f\"   Endpoint: {model_config['azure_endpoint']}\")\n",
    "print(f\"   Deployment: {model_config['azure_deployment']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1842088a",
   "metadata": {},
   "source": [
    "## Part 3: Retrieval Evaluators\n",
    "\n",
    "Retrieval quality is upstream in RAG and critical to final response quality. Poor retrieval results in poor final responses.\n",
    "\n",
    "### When to Use Which Evaluator?\n",
    "\n",
    "- **Retrieval**: Textual quality measurement without ground truth (LLM-based)\n",
    "- **Document Retrieval**: Classical IR metrics (NDCG, XDCG, Fidelity) with ground truth labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af1a767",
   "metadata": {},
   "source": [
    "### 3.1: Retrieval Evaluator\n",
    "\n",
    "**Purpose**: Measures textual quality of retrieval results using LLM without requiring ground truth.\n",
    "\n",
    "**Key Features**:\n",
    "- No ground truth required (unlike Document Retrieval)\n",
    "- Evaluates relevance of context chunks to query\n",
    "- Assesses if most relevant chunks are at the top\n",
    "- Context chunks encoded as strings\n",
    "\n",
    "**Output**: Likert scale score (1-5, higher is better)\n",
    "- Score >= threshold → pass\n",
    "- Score < threshold → fail\n",
    "\n",
    "**Use Case**: Quick quality check of retrieval without needing labeled ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f594c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import RetrievalEvaluator\n",
    "import json\n",
    "\n",
    "retrieval = RetrievalEvaluator(model_config=model_config, threshold=3)\n",
    "\n",
    "result = retrieval(\n",
    "    query=\"Where was Marie Curie born?\",\n",
    "    context=\"Background: 1. Marie Curie was born in Warsaw. 2. Marie Curie was born on November 7, 1867. 3. Marie Curie is a French scientist. \",\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RETRIEVAL EVALUATION RESULT\")\n",
    "print(\"=\" * 80)\n",
    "print(json.dumps(result, indent=2))\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Score: {result.get('retrieval', 'N/A')}/5\")\n",
    "print(f\"Result: {result.get('retrieval_result', 'N/A')}\")\n",
    "print(f\"\\nReason: {result.get('retrieval_reason', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe62921",
   "metadata": {},
   "source": [
    "### 3.2: Document Retrieval Evaluator\n",
    "\n",
    "**Purpose**: Measures retrieval quality using classical IR metrics with ground truth labels (qrels).\n",
    "\n",
    "**Computed Metrics**:\n",
    "\n",
    "| Metric | Category | Description |\n",
    "|--------|----------|-------------|\n",
    "| **Fidelity** | Search Fidelity | Good documents returned / Total known good documents |\n",
    "| **NDCG** | Search NDCG | Quality of rankings vs ideal order |\n",
    "| **XDCG** | Search XDCG | Quality in top-k documents regardless of other scores |\n",
    "| **Max Relevance N** | Search Max Relevance | Maximum relevance in top-k chunks |\n",
    "| **Holes** | Search Label Sanity | Missing query relevance judgments |\n",
    "\n",
    "**Use Case**: Parameter sweep optimization - test various search parameters (algorithms, top_k, chunk sizes) to find optimal RAG configuration.\n",
    "\n",
    "**Requirements**:\n",
    "- Ground truth labels (query relevance judgments)\n",
    "- Label score min/max range\n",
    "- Retrieved documents with relevance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946e04ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import DocumentRetrievalEvaluator\n",
    "\n",
    "# Ground truth: Query relevance labels from human or LLM judges\n",
    "retrieval_ground_truth = [\n",
    "    {\"document_id\": \"1\", \"query_relevance_label\": 4},\n",
    "    {\"document_id\": \"2\", \"query_relevance_label\": 2},\n",
    "    {\"document_id\": \"3\", \"query_relevance_label\": 3},\n",
    "    {\"document_id\": \"4\", \"query_relevance_label\": 1},\n",
    "    {\"document_id\": \"5\", \"query_relevance_label\": 0},\n",
    "]\n",
    "\n",
    "# Label score range\n",
    "ground_truth_label_min = 0\n",
    "ground_truth_label_max = 4\n",
    "\n",
    "# Retrieved documents from search system\n",
    "retrieved_documents = [\n",
    "    {\"document_id\": \"2\", \"relevance_score\": 45.1},\n",
    "    {\"document_id\": \"6\", \"relevance_score\": 35.8},\n",
    "    {\"document_id\": \"3\", \"relevance_score\": 29.2},\n",
    "    {\"document_id\": \"5\", \"relevance_score\": 25.4},\n",
    "    {\"document_id\": \"7\", \"relevance_score\": 18.8},\n",
    "]\n",
    "\n",
    "document_retrieval_evaluator = DocumentRetrievalEvaluator(\n",
    "    ground_truth_label_min=ground_truth_label_min,\n",
    "    ground_truth_label_max=ground_truth_label_max,\n",
    "    # Optional: Override thresholds for pass/fail\n",
    "    ndcg_threshold=0.5,\n",
    "    xdcg_threshold=50.0,\n",
    "    fidelity_threshold=0.5,\n",
    "    top1_relevance_threshold=50.0,\n",
    "    top3_max_relevance_threshold=50.0,\n",
    ")\n",
    "\n",
    "result = document_retrieval_evaluator(\n",
    "    retrieval_ground_truth=retrieval_ground_truth,\n",
    "    retrieved_documents=retrieved_documents\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DOCUMENT RETRIEVAL EVALUATION RESULT\")\n",
    "print(\"=\" * 80)\n",
    "print(json.dumps(result, indent=2, default=str))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY METRICS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    f\"NDCG@3: {result.get('ndcg@3', 'N/A'):.4f} ({result.get('ndcg@3_result', 'N/A')})\")\n",
    "print(\n",
    "    f\"XDCG@3: {result.get('xdcg@3', 'N/A'):.4f} ({result.get('xdcg@3_result', 'N/A')})\")\n",
    "print(\n",
    "    f\"Fidelity: {result.get('fidelity', 'N/A'):.4f} ({result.get('fidelity_result', 'N/A')})\")\n",
    "print(\n",
    "    f\"Top-1 Relevance: {result.get('top1_relevance', 'N/A')} ({result.get('top1_relevance_result', 'N/A')})\")\n",
    "print(\n",
    "    f\"Top-3 Max Relevance: {result.get('top3_max_relevance', 'N/A')} ({result.get('top3_max_relevance_result', 'N/A')})\")\n",
    "print(f\"Holes: {result.get('holes', 'N/A')} (lower is better)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1f74bb",
   "metadata": {},
   "source": [
    "## Part 4: Groundedness Evaluators\n",
    "\n",
    "Groundedness evaluates how well the generated response aligns with grounding context, ensuring the model doesn't fabricate content.\n",
    "\n",
    "**Two Options**:\n",
    "- **Groundedness**: Customizable LLM-judge with open-source prompt\n",
    "- **Groundedness Pro**: Azure AI Content Safety-powered, straightforward definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cb5c2d",
   "metadata": {},
   "source": [
    "### 4.1: Groundedness Evaluator\n",
    "\n",
    "**Purpose**: Measures how well the generated response aligns with given context (grounding source).\n",
    "\n",
    "**Key Features**:\n",
    "- Customizable LLM-judge prompt\n",
    "- Captures **precision** aspect (doesn't fabricate beyond context)\n",
    "- Complementary to Response Completeness (recall aspect)\n",
    "\n",
    "**Output**: Likert scale score (1-5, higher is better)\n",
    "- Lower score = irrelevant to query or fabricated content\n",
    "- Higher score = well-grounded in context\n",
    "\n",
    "**Use Case**: Ensure AI doesn't hallucinate or add information not present in grounding documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cdcd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import GroundednessEvaluator\n",
    "\n",
    "groundedness = GroundednessEvaluator(model_config=model_config, threshold=3)\n",
    "\n",
    "result = groundedness(\n",
    "    query=\"Is Marie Curie born in Paris?\",\n",
    "    context=\"Background: 1. Marie Curie is born on November 7, 1867. 2. Marie Curie is born in Warsaw.\",\n",
    "    response=\"No, Marie Curie is born in Warsaw.\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GROUNDEDNESS EVALUATION RESULT\")\n",
    "print(\"=\" * 80)\n",
    "print(json.dumps(result, indent=2))\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Score: {result.get('groundedness', 'N/A')}/5\")\n",
    "print(f\"Result: {result.get('groundedness_result', 'N/A')}\")\n",
    "print(f\"\\nReason: {result.get('groundedness_reason', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4644ef",
   "metadata": {},
   "source": [
    "### 4.2: Groundedness Pro Evaluator\n",
    "\n",
    "**Purpose**: Detects if generated text is consistent with given context, powered by Azure AI Content Safety.\n",
    "\n",
    "**Key Features**:\n",
    "- Binary label output (True/False)\n",
    "- Straightforward definition\n",
    "- Avoids speculation or fabrication\n",
    "- Enterprise-grade safety checks\n",
    "\n",
    "**Output**: Boolean score\n",
    "- `True` = All content grounded in context\n",
    "- `False` = Contains ungrounded content\n",
    "\n",
    "**Requirements**: Azure AI Project credentials (not just OpenAI model config)\n",
    "\n",
    "**Use Case**: Production RAG systems requiring strict groundedness validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fc7321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import GroundednessProEvaluator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Use Azure AI Project endpoint\n",
    "azure_ai_project = os.environ[\"AZURE_AI_PROJECT_ENDPOINT\"]\n",
    "\n",
    "groundedness_pro = GroundednessProEvaluator(\n",
    "    azure_ai_project=azure_ai_project, \n",
    "    credential=DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "result = groundedness_pro(\n",
    "    query=\"Is Marie Curie born in Paris?\", \n",
    "    context=\"Background: 1. Marie Curie is born on November 7, 1867. 2. Marie Curie is born in Warsaw.\",\n",
    "    response=\"No, Marie Curie is born in Warsaw.\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GROUNDEDNESS PRO EVALUATION RESULT\")\n",
    "print(\"=\" * 80)\n",
    "print(json.dumps(result, indent=2))\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Label: {result.get('groundedness_pro_label', 'N/A')}\")\n",
    "print(f\"Reason: {result.get('groundedness_pro_reason', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1b2f86",
   "metadata": {},
   "source": [
    "## Part 5: Response Quality Evaluators\n",
    "\n",
    "Evaluate the final response quality in relation to the query and expected output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93740d05",
   "metadata": {},
   "source": [
    "### 5.1: Relevance Evaluator\n",
    "\n",
    "**Purpose**: Measures how effectively a response addresses a query (without ground truth).\n",
    "\n",
    "**Key Features**:\n",
    "- Assesses accuracy, completeness, and direct relevance\n",
    "- No ground truth required\n",
    "- Evaluates final response quality\n",
    "\n",
    "**Output**: Likert scale score (1-5, higher is better)\n",
    "- Higher score = better relevance to query\n",
    "\n",
    "**Use Case**: Ensure AI generates relevant responses even when you don't have expected answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188e956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import RelevanceEvaluator\n",
    "\n",
    "relevance = RelevanceEvaluator(model_config=model_config, threshold=3)\n",
    "\n",
    "result = relevance(\n",
    "    query=\"Is Marie Curie born in Paris?\",\n",
    "    response=\"No, Marie Curie is born in Warsaw.\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RELEVANCE EVALUATION RESULT\")\n",
    "print(\"=\" * 80)\n",
    "print(json.dumps(result, indent=2))\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Score: {result.get('relevance', 'N/A')}/5\")\n",
    "print(f\"Result: {result.get('relevance_result', 'N/A')}\")\n",
    "print(f\"\\nReason: {result.get('relevance_reason', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c538833a",
   "metadata": {},
   "source": [
    "### 5.2: Response Completeness Evaluator\n",
    "\n",
    "**Purpose**: Measures if response captures all critical information from ground truth (recall aspect).\n",
    "\n",
    "**Key Features**:\n",
    "- Requires ground truth expected response\n",
    "- Captures **recall** aspect (completeness)\n",
    "- Complementary to Groundedness (precision aspect)\n",
    "- Detects missing critical information\n",
    "\n",
    "**Output**: Likert scale score (1-5, higher is better)\n",
    "- Lower score = missing critical information\n",
    "- Higher score = complete coverage of expected content\n",
    "\n",
    "**Use Case**: Ensure AI responses don't miss important information when you have ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66263baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import ResponseCompletenessEvaluator\n",
    "\n",
    "response_completeness = ResponseCompletenessEvaluator(\n",
    "    model_config=model_config, threshold=3)\n",
    "\n",
    "result = response_completeness(\n",
    "    response=\"Based on the retrieved documents, the shareholder meeting discussed the operational efficiency of the company and financing options.\",\n",
    "    ground_truth=\"The shareholder meeting discussed the compensation package of the company CEO.\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RESPONSE COMPLETENESS EVALUATION RESULT\")\n",
    "print(\"=\" * 80)\n",
    "print(json.dumps(result, indent=2))\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Score: {result.get('response_completeness', 'N/A')}/5\")\n",
    "print(f\"Result: {result.get('response_completeness_result', 'N/A')}\")\n",
    "print(f\"\\nReason: {result.get('response_completeness_reason', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58339108",
   "metadata": {},
   "source": [
    "## Part 6: Complete RAG Evaluation Example\n",
    "\n",
    "Demonstrate a comprehensive RAG evaluation combining multiple evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4731b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample RAG scenario\n",
    "query = \"What are the main causes of climate change?\"\n",
    "\n",
    "context = \"\"\"Climate change is primarily caused by human activities that increase greenhouse gas emissions. \n",
    "The main causes include: 1) Burning fossil fuels for energy and transportation, which releases carbon dioxide. \n",
    "2) Deforestation, which reduces carbon absorption by trees. 3) Industrial processes and agriculture, \n",
    "which emit methane and other greenhouse gases. 4) Changes in land use that affect carbon storage.\"\"\"\n",
    "\n",
    "response = \"\"\"The main causes of climate change are burning fossil fuels, deforestation, and industrial activities. \n",
    "These human activities increase greenhouse gas emissions like carbon dioxide and methane in the atmosphere.\"\"\"\n",
    "\n",
    "ground_truth = \"\"\"Climate change is mainly caused by burning fossil fuels, deforestation, industrial processes, \n",
    "and agriculture, which all increase greenhouse gas emissions.\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE RAG EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(f\"\\nContext (first 100 chars): {context[:100]}...\")\n",
    "print(f\"\\nResponse: {response}\")\n",
    "print(f\"\\nGround Truth: {ground_truth}\")\n",
    "\n",
    "# Evaluate retrieval quality\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"1. RETRIEVAL QUALITY\")\n",
    "print(\"=\" * 80)\n",
    "retrieval_result = retrieval(query=query, context=context)\n",
    "print(f\"Retrieval Score: {retrieval_result.get('retrieval', 'N/A')}/5 ({retrieval_result.get('retrieval_result', 'N/A')})\")\n",
    "print(f\"Reason: {retrieval_result.get('retrieval_reason', 'N/A')[:150]}...\")\n",
    "\n",
    "# Evaluate groundedness\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2. GROUNDEDNESS (Precision)\")\n",
    "print(\"=\" * 80)\n",
    "groundedness_result = groundedness(query=query, context=context, response=response)\n",
    "print(f\"Groundedness Score: {groundedness_result.get('groundedness', 'N/A')}/5 ({groundedness_result.get('groundedness_result', 'N/A')})\")\n",
    "print(f\"Reason: {groundedness_result.get('groundedness_reason', 'N/A')[:150]}...\")\n",
    "\n",
    "# Evaluate relevance\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3. RELEVANCE\")\n",
    "print(\"=\" * 80)\n",
    "relevance_result = relevance(query=query, response=response)\n",
    "print(f\"Relevance Score: {relevance_result.get('relevance', 'N/A')}/5 ({relevance_result.get('relevance_result', 'N/A')})\")\n",
    "print(f\"Reason: {relevance_result.get('relevance_reason', 'N/A')[:150]}...\")\n",
    "\n",
    "# Evaluate response completeness\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"4. RESPONSE COMPLETENESS (Recall)\")\n",
    "print(\"=\" * 80)\n",
    "completeness_result = response_completeness(response=response, ground_truth=ground_truth)\n",
    "print(f\"Completeness Score: {completeness_result.get('response_completeness', 'N/A')}/5 ({completeness_result.get('response_completeness_result', 'N/A')})\")\n",
    "print(f\"Reason: {completeness_result.get('response_completeness_reason', 'N/A')[:150]}...\")\n",
    "\n",
    "# Overall assessment\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OVERALL RAG QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "all_passed = all([\n",
    "    retrieval_result.get('retrieval_result') == 'pass',\n",
    "    groundedness_result.get('groundedness_result') == 'pass',\n",
    "    relevance_result.get('relevance_result') == 'pass',\n",
    "    completeness_result.get('response_completeness_result') == 'pass'\n",
    "])\n",
    "\n",
    "if all_passed:\n",
    "    print(\"✅ All evaluations PASSED - RAG system performing well!\")\n",
    "else:\n",
    "    print(\"⚠️  Some evaluations FAILED - Review individual metrics for improvements\")\n",
    "    \n",
    "print(\"\\nMetric Summary:\")\n",
    "print(f\"  Retrieval: {retrieval_result.get('retrieval_result', 'N/A')}\")\n",
    "print(f\"  Groundedness: {groundedness_result.get('groundedness_result', 'N/A')}\")\n",
    "print(f\"  Relevance: {relevance_result.get('relevance_result', 'N/A')}\")\n",
    "print(f\"  Completeness: {completeness_result.get('response_completeness_result', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c706f97",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "### Evaluation Strategy Decision Tree\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────┐\n",
    "│ Evaluating RAG System                   │\n",
    "└────────────┬────────────────────────────┘\n",
    "             │\n",
    "             ├─► Retrieval Quality?\n",
    "             │   ├─► Have ground truth labels? → Document Retrieval\n",
    "             │   └─► No labels? → Retrieval\n",
    "             │\n",
    "             ├─► Response Consistency?\n",
    "             │   ├─► Need customization? → Groundedness\n",
    "             │   └─► Need enterprise safety? → Groundedness Pro\n",
    "             │\n",
    "             └─► Response Quality?\n",
    "                 ├─► Have ground truth? → Response Completeness\n",
    "                 └─► No ground truth? → Relevance\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Comprehensive Evaluation**: Use multiple evaluators together for complete RAG assessment\n",
    "   - Retrieval quality (upstream)\n",
    "   - Groundedness (precision)\n",
    "   - Response completeness (recall)\n",
    "   - Relevance (final quality)\n",
    "\n",
    "2. **Ground Truth Trade-offs**:\n",
    "   - **With Ground Truth**: Document Retrieval, Response Completeness (more accurate)\n",
    "   - **Without Ground Truth**: Retrieval, Relevance (more flexible)\n",
    "\n",
    "3. **Precision vs Recall**:\n",
    "   - **Groundedness** = Precision (doesn't add false info)\n",
    "   - **Response Completeness** = Recall (doesn't miss critical info)\n",
    "\n",
    "4. **Model Selection**:\n",
    "   - Use reasoning models (o-series, GPT-4.1-mini) for complex evaluations\n",
    "   - Set `is_reasoning_model=True` when using reasoning models\n",
    "   - Balance performance and cost\n",
    "\n",
    "5. **Threshold Tuning**:\n",
    "   - Default threshold = 3 (fair)\n",
    "   - Adjust based on quality requirements\n",
    "   - Higher threshold = stricter quality standards\n",
    "\n",
    "6. **Parameter Sweep for Optimization**:\n",
    "   - Use Document Retrieval metrics (NDCG, XDCG, Fidelity)\n",
    "   - Test different: search algorithms, top_k values, chunk sizes\n",
    "   - Find optimal configuration for your use case\n",
    "\n",
    "### Common Patterns\n",
    "\n",
    "| Scenario | Recommended Evaluators |\n",
    "|----------|------------------------|\n",
    "| Production RAG without ground truth | Retrieval + Groundedness Pro + Relevance |\n",
    "| RAG development with ground truth | Document Retrieval + Groundedness + Response Completeness |\n",
    "| Parameter sweep optimization | Document Retrieval (multiple configurations) |\n",
    "| Quick quality check | Retrieval + Groundedness + Relevance |\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Azure AI Evaluation Documentation](https://learn.microsoft.com/azure/ai-studio/how-to/evaluate-sdk)\n",
    "- [RAG Evaluation Best Practices](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-rag)\n",
    "- [Azure AI Foundry Studio](https://ai.azure.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
