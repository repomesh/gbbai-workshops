{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eebafa17",
   "metadata": {},
   "source": [
    "# Agent Conversation Testing and Evaluation\n",
    "\n",
    "This notebook demonstrates how to test Azure AI agents with synthetic conversations and evaluate their responses using quality metrics.\n",
    "\n",
    "## What is Agent Conversation Testing?\n",
    "\n",
    "Agent conversation testing validates that your AI agent produces high-quality responses across different user scenarios. The process involves:\n",
    "1. **Test Scenario Definition**: Create realistic user queries representing different personas and use cases\n",
    "2. **Agent Interaction**: Execute conversations with the agent to gather responses\n",
    "3. **Quality Evaluation**: Assess response quality using AI-assisted evaluators\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "This notebook evaluates agent responses using three key quality metrics:\n",
    "\n",
    "1. **Relevance**: How well the response addresses the user's query\n",
    "   - Measures accuracy, completeness, and directness\n",
    "   - Likert scale 1-5 (higher is better)\n",
    "\n",
    "2. **Coherence**: Logical flow and consistency of the response\n",
    "   - Assesses if ideas connect naturally\n",
    "   - Likert scale 1-5 (higher is better)\n",
    "\n",
    "3. **Fluency**: Language quality and readability\n",
    "   - Evaluates grammar, naturalness, and clarity\n",
    "   - Likert scale 1-5 (higher is better)\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "```\n",
    "Test Scenarios ‚Üí Agent Conversations ‚Üí Extract Q&A Pairs ‚Üí Evaluate Quality ‚Üí Analyze Results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9f8430",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Part 1: Environment Setup](#part-1-environment-setup)\n",
    "2. [Part 2: Model Configuration](#part-2-model-configuration)\n",
    "3. [Part 3: Define Test Scenarios](#part-3-define-test-scenarios)\n",
    "4. [Part 4: Define Agent Callback](#part-4-define-agent-callback)\n",
    "5. [Part 5: Run Agent Conversations](#part-5-run-agent-conversations)\n",
    "   - 5.1: View Conversation Results\n",
    "6. [Part 6: Initialize Evaluators](#part-6-initialize-evaluators)\n",
    "7. [Part 7: Evaluate Conversations](#part-7-evaluate-conversations)\n",
    "8. [Part 8: Analyze Results](#part-8-analyze-results)\n",
    "   - 8.1: Save Evaluation Results\n",
    "9. [Summary and Best Practices](#summary-and-best-practices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81298d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-evaluation wikipedia -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1d31da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "new_path_entry = \"/opt/homebrew/bin\"  # Replace with the directory you want to add\n",
    "current_path = os.environ.get('PATH', '')\n",
    "\n",
    "if new_path_entry not in current_path.split(os.pathsep):\n",
    "    os.environ['PATH'] = new_path_entry + os.pathsep + current_path\n",
    "    print(f\"Updated PATH for this session: {os.environ['PATH']}\")\n",
    "else:\n",
    "    print(f\"PATH already contains {new_path_entry}: {current_path}\")\n",
    "\n",
    "# You can then verify with shutil.which again\n",
    "print(f\"Location of 'az' found by kernel now: {shutil.which('az')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b523ec9",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "Load required libraries and configure Azure AI project connection.\n",
    "\n",
    "**Prerequisites**:\n",
    "- `.env` file in `17_agent_ops/` directory with:\n",
    "  - `AZURE_AI_PROJECT_ENDPOINT` - Azure AI Foundry project endpoint\n",
    "  - `AZURE_OPENAI_API_KEY_GPT_4o` - Azure OpenAI API key\n",
    "  - `AZURE_OPENAI_ENDPOINT_GPT_4o` - Azure OpenAI endpoint\n",
    "  - `AZURE_OPENAI_MODEl_GPT_4o` - Deployment name (defaults to gpt-4o)\n",
    "  - `TARGET_AGENT_ID` - The agent ID to evaluate\n",
    "- Authenticate via `az login` for DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb7a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.evaluation.simulator import Simulator\n",
    "from azure.ai.evaluation import RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator\n",
    "from azure.ai.projects import AIProjectClient\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Add parent directory to path for agent_utils import\n",
    "parent_dir = Path(__file__).parent.parent if hasattr(__builtins__, '__file__') else Path.cwd().parent\n",
    "sys.path.insert(0, str(parent_dir / \"utils\"))\n",
    "\n",
    "from agent_utils import AgentManager\n",
    "\n",
    "# Load environment variables from parent directory\n",
    "agent_ops_dir = Path.cwd().parent if Path.cwd().name == \"05_evaluation\" else Path.cwd()\n",
    "env_path = agent_ops_dir / \".env\"\n",
    "load_dotenv(env_path)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(name)s: %(message)s\")\n",
    "logger = logging.getLogger(\"simulator_eval\")\n",
    "\n",
    "# Suppress verbose Azure SDK HTTP logging\n",
    "logging.getLogger(\"azure.core.pipeline.policies.http_logging_policy\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"azure.identity\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"azure.cosmos._cosmos_http_logging_policy\").setLevel(logging.WARNING)\n",
    "\n",
    "# Initialize Azure AI Project Client with endpoint\n",
    "endpoint = os.getenv(\"AZURE_AI_PROJECT_ENDPOINT\")\n",
    "if not endpoint:\n",
    "    raise ValueError(\"Set AZURE_AI_PROJECT_ENDPOINT in .env file\")\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "project_client = AIProjectClient(endpoint=endpoint, credential=credential)\n",
    "agent_manager = AgentManager(project_client)\n",
    "logger.info(\"‚úÖ Connected to Azure AI project\")\n",
    "\n",
    "# Get Azure OpenAI configuration from .env\n",
    "model_api_key = os.getenv(\"AZURE_OPENAI_API_KEY_GPT_4o\")\n",
    "if not model_api_key:\n",
    "    raise ValueError(\"Set AZURE_OPENAI_API_KEY_GPT_4o in .env file\")\n",
    "\n",
    "model_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT_GPT_4o\")\n",
    "if not model_endpoint:\n",
    "    raise ValueError(\"Set AZURE_OPENAI_ENDPOINT_GPT_4o in .env file\")\n",
    "\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_MODEl_GPT_4o\", \"gpt-4o\")\n",
    "\n",
    "# Agent to evaluate\n",
    "TARGET_AGENT_ID = os.getenv(\"TARGET_AGENT_ID\", \"asst_3pPWPYFexU3fEwbYB3VDWO1N\")\n",
    "\n",
    "# Setup data directory\n",
    "timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%d%H%M%S\")\n",
    "notebook_dir = Path.cwd() if Path.cwd().name == \"05_evaluation\" else Path.cwd() / \"05_evaluation\"\n",
    "dataset_dir = notebook_dir / \"data\"\n",
    "dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger.info(f\"‚úÖ Evaluation will use deployment '{deployment_name}' at endpoint '{model_endpoint}'\")\n",
    "logger.info(f\"üéØ Target agent: {TARGET_AGENT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbf027d",
   "metadata": {},
   "source": [
    "## Part 2: Model Configuration\n",
    "\n",
    "Configure the LLM model for evaluators and set up test context.\n",
    "\n",
    "**Model Configuration**:\n",
    "- Uses Azure OpenAI for AI-assisted quality evaluators\n",
    "- Supports both API key and Azure AD authentication\n",
    "- Configurable deployment and API version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5704a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model for evaluators\n",
    "evaluator_model_config = {\n",
    "    \"azure_endpoint\": model_endpoint,\n",
    "    \"api_key\": model_api_key,\n",
    "    \"azure_deployment\": deployment_name,\n",
    "    \"api_version\": \"2024-08-01-preview\"\n",
    "}\n",
    "\n",
    "# Use Wikipedia to get contextual information for test scenarios\n",
    "wiki_search_term = \"Azure AI\"\n",
    "wiki_title = wikipedia.search(wiki_search_term)[0]\n",
    "wiki_page = wikipedia.page(wiki_title)\n",
    "text = wiki_page.summary[:1000]\n",
    "\n",
    "logger.info(f\"‚úÖ Model configuration ready\")\n",
    "logger.info(f\"üìö Test context from Wikipedia: {wiki_title}\")\n",
    "logger.info(f\"üìù Context preview (first 200 chars): {text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f428d63f",
   "metadata": {},
   "source": [
    "## Part 3: Define Test Scenarios\n",
    "\n",
    "Create realistic test scenarios representing different user personas and use cases.\n",
    "\n",
    "**Test Scenario Design**:\n",
    "- Each scenario represents a distinct user persona\n",
    "- Queries should reflect real-world use cases\n",
    "- Include variety: beginners, experts, educators, practitioners\n",
    "- Consider different information needs and goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d808eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test scenarios with different user personas\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"scenario\": \"Student learning basics\",\n",
    "        \"persona\": \"Beginner student\",\n",
    "        \"query\": f\"Can you explain what {wiki_search_term} is in simple terms?\"\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"Teacher preparing lesson\",\n",
    "        \"persona\": \"Educator\",\n",
    "        \"query\": f\"What are the key concepts of {wiki_search_term} I should teach to my students?\"\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"Technical overview\",\n",
    "        \"persona\": \"Technical professional\",\n",
    "        \"query\": f\"What are the main technical components of {wiki_search_term}?\"\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"Practical application\",\n",
    "        \"persona\": \"Developer/Practitioner\",\n",
    "        \"query\": f\"How can I use {wiki_search_term} in a real-world project?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "logger.info(f\"‚úÖ Defined {len(test_scenarios)} test scenarios\")\n",
    "for i, scenario in enumerate(test_scenarios, 1):\n",
    "    logger.info(f\"  {i}. {scenario['scenario']} ({scenario['persona']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c8242a",
   "metadata": {},
   "source": [
    "## Part 4: Define Agent Callback\n",
    "\n",
    "Create a callback function to interact with the Azure AI agent.\n",
    "\n",
    "**Callback Function Purpose**:\n",
    "- Abstracts agent interaction logic\n",
    "- Handles thread creation and cleanup\n",
    "- Manages errors gracefully\n",
    "- Returns responses in expected format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callback function to interact with the agent\n",
    "async def agent_callback(\n",
    "    messages: Dict[str, List[Dict]],\n",
    "    stream: bool = False,\n",
    "    session_state: Any = None,\n",
    "    context: Optional[Dict[str, Any]] = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Callback function that the simulator will use to interact with your agent.\n",
    "    This simulates how a user would interact with the agent.\n",
    "    \"\"\"\n",
    "    messages_list = messages[\"messages\"]\n",
    "    # Get the last message from the user\n",
    "    latest_message = messages_list[-1]\n",
    "    query = latest_message[\"content\"]\n",
    "\n",
    "    # Create a new thread for this conversation\n",
    "    thread = agent_manager.create_thread()\n",
    "\n",
    "    try:\n",
    "        # Call the agent\n",
    "        response_text = agent_manager.run_agent_simple(\n",
    "            thread_id=thread.id,\n",
    "            agent_id=TARGET_AGENT_ID,\n",
    "            user_message=query,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # Format the response to follow the OpenAI chat protocol format\n",
    "        formatted_response = {\n",
    "            \"content\": response_text,\n",
    "            \"role\": \"assistant\",\n",
    "            \"context\": context or {},\n",
    "        }\n",
    "        messages[\"messages\"].append(formatted_response)\n",
    "\n",
    "        return {\n",
    "            \"messages\": messages[\"messages\"],\n",
    "            \"stream\": stream,\n",
    "            \"session_state\": session_state,\n",
    "            \"context\": context\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in agent callback: {e}\")\n",
    "        # Return error response\n",
    "        error_response = {\n",
    "            \"content\": f\"Error: {str(e)}\",\n",
    "            \"role\": \"assistant\",\n",
    "            \"context\": context or {},\n",
    "        }\n",
    "        messages[\"messages\"].append(error_response)\n",
    "        return {\n",
    "            \"messages\": messages[\"messages\"],\n",
    "            \"stream\": stream,\n",
    "            \"session_state\": session_state,\n",
    "            \"context\": context\n",
    "        }\n",
    "    finally:\n",
    "        agent_manager.delete_thread(thread.id, silent=True)\n",
    "\n",
    "logger.info(\"‚úÖ Agent callback function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fde140",
   "metadata": {},
   "source": [
    "## Part 5: Run Agent Conversations\n",
    "\n",
    "Execute conversations with the agent for each test scenario.\n",
    "\n",
    "**Process**:\n",
    "1. Iterate through test scenarios\n",
    "2. Create message format for each query\n",
    "3. Call agent callback to get response\n",
    "4. Store conversation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc3a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run agent conversations for all test scenarios\n",
    "async def run_agent_conversations():\n",
    "    \"\"\"\n",
    "    Execute conversations with the agent using predefined test scenarios.\n",
    "    Returns conversation results with scenario metadata.\n",
    "    \"\"\"\n",
    "    conversation_results = []\n",
    "\n",
    "    for test in test_scenarios:\n",
    "        logger.info(f\"üìù Testing: {test['scenario']}\")\n",
    "\n",
    "        # Create conversation format expected by callback\n",
    "        messages = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": test[\"query\"]}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Call the agent\n",
    "        response = await agent_callback(messages=messages)\n",
    "\n",
    "        # Store the result with metadata\n",
    "        conversation_results.append({\n",
    "            \"scenario\": test[\"scenario\"],\n",
    "            \"persona\": test[\"persona\"],\n",
    "            \"messages\": response[\"messages\"]\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"  ‚úÖ Completed: {test['scenario']}\")\n",
    "\n",
    "    return conversation_results\n",
    "\n",
    "# Execute conversations\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"üöÄ Starting agent conversation testing...\")\n",
    "logger.info(\"=\" * 80)\n",
    "conversation_outputs = await run_agent_conversations()\n",
    "logger.info(f\"\\n‚úÖ Completed {len(conversation_outputs)} conversations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce55369",
   "metadata": {},
   "source": [
    "### 5.1: View Conversation Results\n",
    "\n",
    "Display and save the generated conversations for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d17049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display conversation outputs\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONVERSATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for conv in conversation_outputs:\n",
    "    print(f\"\\nüìå Scenario: {conv['scenario']} ({conv['persona']})\")\n",
    "    print(\"-\" * 80)\n",
    "    for msg in conv[\"messages\"]:\n",
    "        role_icon = \"üë§\" if msg[\"role\"] == \"user\" else \"ü§ñ\"\n",
    "        print(f\"{role_icon} {msg['role'].upper()}: {msg['content'][:200]}...\")\n",
    "    print()\n",
    "\n",
    "# Save conversation outputs\n",
    "conversation_output_path = dataset_dir / f\"conversations_{timestamp}.json\"\n",
    "with conversation_output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(conversation_outputs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "logger.info(f\"‚úÖ Saved conversations to {conversation_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a220a4",
   "metadata": {},
   "source": [
    "## Part 6: Initialize Evaluators\n",
    "\n",
    "Set up AI-assisted evaluators to assess response quality.\n",
    "\n",
    "**Evaluator Types**:\n",
    "- **RelevanceEvaluator**: Measures how well the response addresses the query\n",
    "- **CoherenceEvaluator**: Assesses logical flow and consistency\n",
    "- **FluencyEvaluator**: Evaluates language quality and readability\n",
    "\n",
    "All evaluators use an LLM-judge approach with Likert scale scoring (1-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486d9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize quality evaluators\n",
    "relevance_eval = RelevanceEvaluator(evaluator_model_config)\n",
    "coherence_eval = CoherenceEvaluator(evaluator_model_config)\n",
    "fluency_eval = FluencyEvaluator(evaluator_model_config)\n",
    "\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"‚úÖ Evaluators initialized successfully\")\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"  ‚Ä¢ Relevance Evaluator - Measures query-response alignment\")\n",
    "logger.info(\"  ‚Ä¢ Coherence Evaluator - Assesses logical consistency\")\n",
    "logger.info(\"  ‚Ä¢ Fluency Evaluator - Evaluates language quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3085bfb",
   "metadata": {},
   "source": [
    "## Run Simulator\n",
    "\n",
    "Generate synthetic conversations by directly calling the agent with test queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee71b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run direct agent conversations (alternative to Simulator for simpler testing)\n",
    "async def run_direct_conversations():\n",
    "    \"\"\"\n",
    "    Directly call the agent with predefined queries instead of using the Simulator.\n",
    "    This is simpler and more reliable for basic testing scenarios.\n",
    "    \"\"\"\n",
    "    # Define test scenarios\n",
    "    test_queries = [\n",
    "        {\n",
    "            \"scenario\": \"Student learning basics\",\n",
    "            \"query\": f\"Can you explain what {wiki_search_term} is in simple terms?\"\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"Teacher preparing lesson\",\n",
    "            \"query\": f\"What are the key concepts of {wiki_search_term} I should teach to my students?\"\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"Technical overview\",\n",
    "            \"query\": f\"What are the main technical components of {wiki_search_term}?\"\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"Practical application\",\n",
    "            \"query\": f\"How can I use {wiki_search_term} in a real-world project?\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for test in test_queries:\n",
    "        logger.info(f\"Testing scenario: {test['scenario']}\")\n",
    "\n",
    "        # Create conversation format expected by callback\n",
    "        messages = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": test[\"query\"]}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Call the agent\n",
    "        response = await agent_callback(messages=messages)\n",
    "\n",
    "        # Store the result\n",
    "        results.append({\n",
    "            \"scenario\": test[\"scenario\"],\n",
    "            \"messages\": response[\"messages\"]\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the conversations\n",
    "logger.info(\"üöÄ Starting agent conversations...\")\n",
    "simulator_outputs = await run_direct_conversations()\n",
    "logger.info(f\"‚úÖ Completed {len(simulator_outputs)} conversations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d3cfea",
   "metadata": {},
   "source": [
    "## Display Simulator Outputs\n",
    "\n",
    "View and save the generated conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af47f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display simulator outputs\n",
    "print(\"=\" * 80)\n",
    "print(\"SIMULATOR OUTPUTS\")\n",
    "print(\"=\" * 80)\n",
    "print(json.dumps(simulator_outputs, indent=2))\n",
    "\n",
    "# Save simulator outputs\n",
    "simulator_output_path = dataset_dir / f\"simulator_outputs_{timestamp}.json\"\n",
    "with simulator_output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(simulator_outputs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "logger.info(f\"‚úÖ Saved simulator outputs to {simulator_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859aef55",
   "metadata": {},
   "source": [
    "## Initialize Evaluators\n",
    "\n",
    "Set up evaluators to assess the quality of simulated conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1f1f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model for evaluators\n",
    "evaluator_model_config = {\n",
    "    \"azure_endpoint\": model_endpoint,\n",
    "    \"api_key\": model_api_key,\n",
    "    \"azure_deployment\": deployment_name,\n",
    "    \"api_version\": \"2024-08-01-preview\"\n",
    "}\n",
    "\n",
    "# Initialize evaluators\n",
    "relevance_eval = RelevanceEvaluator(evaluator_model_config)\n",
    "coherence_eval = CoherenceEvaluator(evaluator_model_config)\n",
    "fluency_eval = FluencyEvaluator(evaluator_model_config)\n",
    "\n",
    "logger.info(\"‚úÖ Evaluators initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e53946",
   "metadata": {},
   "source": [
    "## Part 7: Evaluate Conversations\n",
    "\n",
    "Extract query-response pairs and evaluate them using quality metrics.\n",
    "\n",
    "**Evaluation Process**:\n",
    "1. Extract Q&A pairs from conversations\n",
    "2. Run each evaluator (Relevance, Coherence, Fluency)\n",
    "3. Handle errors gracefully\n",
    "4. Store results with scenario metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract query-response pairs from conversations\n",
    "evaluation_data = []\n",
    "\n",
    "for conversation in conversation_outputs:\n",
    "    if \"messages\" in conversation:\n",
    "        messages = conversation[\"messages\"]\n",
    "        # Extract user queries and assistant responses\n",
    "        for i in range(len(messages) - 1):\n",
    "            if messages[i][\"role\"] == \"user\" and messages[i + 1][\"role\"] == \"assistant\":\n",
    "                evaluation_data.append({\n",
    "                    \"scenario\": conversation.get(\"scenario\", f\"Conversation {len(evaluation_data) + 1}\"),\n",
    "                    \"persona\": conversation.get(\"persona\", \"Unknown\"),\n",
    "                    \"query\": messages[i][\"content\"],\n",
    "                    \"response\": messages[i + 1][\"content\"]\n",
    "                })\n",
    "\n",
    "logger.info(f\"\\nüìä Extracted {len(evaluation_data)} query-response pairs for evaluation\")\n",
    "\n",
    "# Run evaluation on all conversations\n",
    "logger.info(\"\\n\" + \"=\" * 80)\n",
    "logger.info(\"üîç Starting quality evaluation...\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for row in evaluation_data:\n",
    "    scenario = row[\"scenario\"]\n",
    "    persona = row[\"persona\"]\n",
    "    query = row[\"query\"]\n",
    "    response = row[\"response\"]\n",
    "\n",
    "    logger.info(f\"\\nüìù Evaluating: {scenario}\")\n",
    "    logger.info(f\"   Query: {query[:80]}...\")\n",
    "\n",
    "    # Run evaluators with error handling\n",
    "    try:\n",
    "        relevance_score = relevance_eval(query=query, response=response)\n",
    "        coherence_score = coherence_eval(query=query, response=response)\n",
    "        fluency_score = fluency_eval(query=query, response=response)\n",
    "\n",
    "        result = {\n",
    "            \"scenario\": scenario,\n",
    "            \"persona\": persona,\n",
    "            \"query\": query,\n",
    "            \"response\": response[:200] + \"...\" if len(response) > 200 else response,\n",
    "            \"relevance\": relevance_score.get(\"gpt_relevance\", relevance_score.get(\"relevance\", 0)),\n",
    "            \"coherence\": coherence_score.get(\"gpt_coherence\", coherence_score.get(\"coherence\", 0)),\n",
    "            \"fluency\": fluency_score.get(\"gpt_fluency\", fluency_score.get(\"fluency\", 0))\n",
    "        }\n",
    "        \n",
    "        evaluation_results.append(result)\n",
    "        logger.info(f\"   ‚úÖ Scores - Relevance: {result['relevance']:.1f}, Coherence: {result['coherence']:.1f}, Fluency: {result['fluency']:.1f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"   ‚ùå Error: {str(e)}\")\n",
    "        evaluation_results.append({\n",
    "            \"scenario\": scenario,\n",
    "            \"persona\": persona,\n",
    "            \"query\": query,\n",
    "            \"response\": response[:200] + \"...\" if len(response) > 200 else response,\n",
    "            \"relevance\": 0,\n",
    "            \"coherence\": 0,\n",
    "            \"fluency\": 0,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "logger.info(f\"\\n‚úÖ Evaluation completed for {len(evaluation_results)} conversations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a89edb",
   "metadata": {},
   "source": [
    "## Part 8: Analyze Results\n",
    "\n",
    "Visualize and analyze evaluation results to identify patterns and insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365bcb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for analysis\n",
    "results_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "# Display results table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "display(results_df[[\"scenario\", \"persona\", \"relevance\", \"coherence\", \"fluency\"]])\n",
    "\n",
    "# Calculate statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "avg_relevance = results_df[\"relevance\"].mean()\n",
    "avg_coherence = results_df[\"coherence\"].mean()\n",
    "avg_fluency = results_df[\"fluency\"].mean()\n",
    "\n",
    "print(f\"\\nüìä Average Scores (out of 5.0):\")\n",
    "print(f\"  Relevance:  {avg_relevance:.2f}\")\n",
    "print(f\"  Coherence:  {avg_coherence:.2f}\")\n",
    "print(f\"  Fluency:    {avg_fluency:.2f}\")\n",
    "print(f\"  Overall:    {(avg_relevance + avg_coherence + avg_fluency) / 3:.2f}\")\n",
    "\n",
    "# Per-scenario analysis\n",
    "print(f\"\\nüìã Per-Scenario Performance:\")\n",
    "scenario_stats = results_df.groupby(\"scenario\")[[\"relevance\", \"coherence\", \"fluency\"]].mean()\n",
    "for scenario, scores in scenario_stats.iterrows():\n",
    "    avg_score = scores.mean()\n",
    "    status = \"‚úÖ\" if avg_score >= 4.0 else \"‚ö†Ô∏è\" if avg_score >= 3.0 else \"‚ùå\"\n",
    "    print(f\"  {status} {scenario}: {avg_score:.2f}\")\n",
    "\n",
    "# Quality assessment\n",
    "print(f\"\\nüéØ Quality Assessment:\")\n",
    "overall_avg = (avg_relevance + avg_coherence + avg_fluency) / 3\n",
    "if overall_avg >= 4.0:\n",
    "    print(\"  ‚úÖ EXCELLENT - Agent responses are high quality across all metrics\")\n",
    "elif overall_avg >= 3.5:\n",
    "    print(\"  ‚úÖ GOOD - Agent responses meet quality standards\")\n",
    "elif overall_avg >= 3.0:\n",
    "    print(\"  ‚ö†Ô∏è  FAIR - Agent responses need improvement in some areas\")\n",
    "else:\n",
    "    print(\"  ‚ùå POOR - Agent responses require significant improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c484c1d",
   "metadata": {},
   "source": [
    "### 8.1: Save Evaluation Results\n",
    "\n",
    "Persist evaluation results for future reference and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a1c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive evaluation results\n",
    "results_path = dataset_dir / f\"evaluation_results_{timestamp}.json\"\n",
    "with results_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"metadata\": {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"agent_id\": TARGET_AGENT_ID,\n",
    "            \"model\": deployment_name,\n",
    "            \"test_context\": wiki_search_term,\n",
    "            \"num_scenarios\": len(test_scenarios),\n",
    "            \"num_evaluations\": len(evaluation_results)\n",
    "        },\n",
    "        \"test_scenarios\": test_scenarios,\n",
    "        \"evaluation_results\": evaluation_results,\n",
    "        \"summary_statistics\": {\n",
    "            \"averages\": {\n",
    "                \"relevance\": float(avg_relevance),\n",
    "                \"coherence\": float(avg_coherence),\n",
    "                \"fluency\": float(avg_fluency),\n",
    "                \"overall\": float((avg_relevance + avg_coherence + avg_fluency) / 3)\n",
    "            },\n",
    "            \"per_scenario\": scenario_stats.to_dict()\n",
    "        }\n",
    "    }, f, indent=2)\n",
    "\n",
    "logger.info(f\"\\n‚úÖ Saved evaluation results to {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38139ab1",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Test Scenario Design**\n",
    "   - Create diverse scenarios representing real user needs\n",
    "   - Include different personas: beginners, experts, educators, practitioners\n",
    "   - Cover various information types: definitions, concepts, technical details, applications\n",
    "\n",
    "2. **Quality Metrics Understanding**\n",
    "   - **Relevance (4.0+)**: Response directly addresses the query\n",
    "   - **Coherence (4.0+)**: Ideas flow logically and connect naturally\n",
    "   - **Fluency (4.0+)**: Language is clear, grammatical, and natural\n",
    "\n",
    "3. **Evaluation Strategy**\n",
    "   - Run evaluations consistently across all scenarios\n",
    "   - Handle errors gracefully to avoid incomplete results\n",
    "   - Analyze per-scenario performance to identify weak areas\n",
    "   - Track results over time to measure improvements\n",
    "\n",
    "4. **Agent Quality Standards**\n",
    "   - **Excellent (4.0+)**: Production-ready, high-quality responses\n",
    "   - **Good (3.5-4.0)**: Acceptable for most use cases\n",
    "   - **Fair (3.0-3.5)**: Needs improvement before deployment\n",
    "   - **Poor (<3.0)**: Requires significant agent refinement\n",
    "\n",
    "### Common Patterns\n",
    "\n",
    "| Scenario Type | Expected Relevance | Expected Coherence | Expected Fluency |\n",
    "|--------------|-------------------|-------------------|-----------------|\n",
    "| Simple definitions | 4.5+ | 4.0+ | 4.5+ |\n",
    "| Technical explanations | 4.0+ | 4.0+ | 4.0+ |\n",
    "| Educational content | 4.5+ | 4.5+ | 4.5+ |\n",
    "| Practical guidance | 4.0+ | 4.0+ | 4.0+ |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Expand Test Coverage**: Add more scenarios and edge cases\n",
    "- **Customize Evaluators**: Create domain-specific evaluation metrics\n",
    "- **A/B Testing**: Compare different agent configurations\n",
    "- **Continuous Monitoring**: Track quality metrics over time\n",
    "- **User Feedback Integration**: Combine automated metrics with real user feedback\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Azure AI Evaluation Documentation](https://learn.microsoft.com/azure/ai-studio/how-to/evaluate-sdk)\n",
    "- [Agent Quality Best Practices](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-agent)\n",
    "- [Azure AI Foundry Studio](https://ai.azure.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
