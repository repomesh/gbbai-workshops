{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf320319",
   "metadata": {},
   "source": [
    "# Agent Evaluators (Preview)\n",
    "\n",
    "This notebook demonstrates how to evaluate Azure AI agents using agent-specific evaluators. Azure AI Foundry supports three types of agent evaluators for agentic workflows:\n",
    "\n",
    "## Agent-Specific Evaluators\n",
    "\n",
    "1. **Intent Resolution**: Measures how well the system identifies and understands user intent\n",
    "2. **Tool Call Accuracy**: Evaluates the accuracy and efficiency of tool calls made by an agent\n",
    "3. **Task Adherence**: Assesses whether the agent stays on track to complete tasks\n",
    "\n",
    "## Additional Evaluators Available\n",
    "\n",
    "Besides agent-specific evaluators, you can also assess other quality and safety aspects:\n",
    "\n",
    "- **Quality**: Relevance, Coherence, Fluency\n",
    "- **Safety**: CodeVulnerabilities, Violence, Self-harm, Sexual, HateUnfairness, IndirectAttack, ProtectedMaterials\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. Setup environment and Azure AI Project client\n",
    "2. Create and configure an agent with tools\n",
    "3. Run agent to generate test data\n",
    "4. Convert agent messages for evaluation\n",
    "5. Configure and run evaluators\n",
    "6. Analyze evaluation results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623a841",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Part 1: Environment Setup](#part-1-environment-setup)\n",
    "2. [Part 2: Create Azure AI Agent](#part-2-create-azure-ai-agent)\n",
    "3. [Part 3: Run Agent to Generate Test Data](#part-3-run-agent-to-generate-test-data)\n",
    "   - 3.1: Create Thread\n",
    "   - 3.2: Send User Message\n",
    "   - 3.3: Execute Agent Run\n",
    "   - 3.4: View Conversation Messages\n",
    "4. [Part 4: Convert Agent Messages for Evaluation](#part-4-convert-agent-messages-for-evaluation)\n",
    "   - 4.1: Inspect Converted Data\n",
    "5. [Part 5: Model Configuration for AI-Assisted Evaluators](#part-5-model-configuration-for-ai-assisted-evaluators)\n",
    "6. [Part 6: Run Batch Evaluation](#part-6-run-batch-evaluation)\n",
    "7. [Part 7: Individual Evaluator Examples](#part-7-individual-evaluator-examples)\n",
    "   - 7.1: Intent Resolution Evaluator\n",
    "   - 7.2: Tool Call Accuracy Evaluator\n",
    "   - 7.3: Task Adherence Evaluator\n",
    "8. [Summary](#summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "new_path_entry = \"/opt/homebrew/bin\"  # Replace with the directory you want to add\n",
    "current_path = os.environ.get('PATH', '')\n",
    "\n",
    "if new_path_entry not in current_path.split(os.pathsep):\n",
    "    os.environ['PATH'] = new_path_entry + os.pathsep + current_path\n",
    "    print(f\"Updated PATH for this session: {os.environ['PATH']}\")\n",
    "else:\n",
    "    print(f\"PATH already contains {new_path_entry}: {current_path}\")\n",
    "\n",
    "# You can then verify with shutil.which again\n",
    "print(f\"Location of 'az' found by kernel now: {shutil.which('az')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd47653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add parent directory to path for agent_utils import\n",
    "parent_dir = Path(__file__).parent.parent if hasattr(\n",
    "    __builtins__, '__file__') else Path.cwd().parent\n",
    "sys.path.insert(0, str(parent_dir / \"utils\"))\n",
    "\n",
    "# Load environment variables from parent directory\n",
    "agent_ops_dir = Path.cwd().parent if Path.cwd(\n",
    ").name == \"05_evaluation\" else Path.cwd()\n",
    "env_path = agent_ops_dir / \".env\"\n",
    "load_dotenv(env_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943fca15",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "Configure the notebook environment and load necessary dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f11f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.agents.models import FunctionTool, ToolSet\n",
    "\n",
    "# Import your custom functions to be used as Tools for the Agent\n",
    "from user_functions import user_functions\n",
    "\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=os.environ[\"AZURE_AI_PROJECT_ENDPOINT\"],\n",
    "    credential=DefaultAzureCredential(),\n",
    ")\n",
    "\n",
    "AGENT_NAME = \"Seattle Tourist Assistant\"\n",
    "\n",
    "# Add Tools to be used by Agent\n",
    "functions = FunctionTool(user_functions)\n",
    "\n",
    "toolset = ToolSet()\n",
    "toolset.add(functions)\n",
    "\n",
    "# To enable tool calls executed automatically\n",
    "project_client.agents.enable_auto_function_calls(toolset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3120f2da",
   "metadata": {},
   "source": [
    "## Part 2: Create Azure AI Agent\n",
    "\n",
    "Initialize the Azure AI Project client and create an agent with function tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = project_client.agents.create_agent(\n",
    "    model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "    name=AGENT_NAME,\n",
    "    instructions=\"You are a helpful assistant\",\n",
    "    toolset=toolset,\n",
    ")\n",
    "\n",
    "print(f\"Created agent, ID: {agent.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9fe37c",
   "metadata": {},
   "source": [
    "## Part 3: Run Agent to Generate Test Data\n",
    "\n",
    "Execute the agent with a sample query to generate conversation data for evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e5839",
   "metadata": {},
   "source": [
    "### 3.1: Create Thread\n",
    "\n",
    "Create a conversation thread for the agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafb78a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = project_client.agents.threads.create()\n",
    "print(f\"Created thread, ID: {thread.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcc6210",
   "metadata": {},
   "source": [
    "### 3.2: Send User Message\n",
    "\n",
    "Add a user query to the thread.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d527da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create message to thread\n",
    "MESSAGE = \"Can you email me weather info for Seattle ?\"\n",
    "\n",
    "message = project_client.agents.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=MESSAGE,\n",
    ")\n",
    "print(f\"Created message, ID: {message.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81762a06",
   "metadata": {},
   "source": [
    "### 3.3: Execute Agent Run\n",
    "\n",
    "Process the user message and generate agent response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe34b30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = project_client.agents.runs.create_and_process(\n",
    "    thread_id=thread.id, agent_id=agent.id)\n",
    "\n",
    "print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "if run.status == \"failed\":\n",
    "    print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "print(f\"Run ID: {run.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093d254c",
   "metadata": {},
   "source": [
    "### 3.4: View Conversation Messages\n",
    "\n",
    "Display the complete conversation between user and agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad53f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in project_client.agents.messages.list(thread.id, order=\"asc\"):\n",
    "    print(f\"Role: {message.role}\")\n",
    "    print(f\"Content: {message.content[0].text.value}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5632bbf",
   "metadata": {},
   "source": [
    "## Part 4: Convert Agent Messages for Evaluation\n",
    "\n",
    "Azure AI Foundry provides native integration for evaluating agent messages. The AIAgentConverter transforms agent runs into the format required by evaluators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe1774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import AIAgentConverter\n",
    "\n",
    "# Initialize the converter that will be backed by the project.\n",
    "converter = AIAgentConverter(project_client)\n",
    "\n",
    "thread_id = thread.id\n",
    "run_id = run.id\n",
    "file_name = \"./data/evaluation_agent_data.jsonl\"\n",
    "\n",
    "# Get a single agent run data\n",
    "evaluation_data_single_run = converter.convert(\n",
    "    thread_id=thread_id, run_id=run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c0582",
   "metadata": {},
   "source": [
    "### 4.1: Inspect Converted Data\n",
    "\n",
    "View the evaluation data structure that will be used by evaluators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32c6389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONVERTED EVALUATION DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Print formatted JSON for better readability\n",
    "print(json.dumps(evaluation_data_single_run, indent=2, default=str))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA STRUCTURE OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Keys in evaluation data: {list(evaluation_data_single_run.keys())}\")\n",
    "\n",
    "if 'query' in evaluation_data_single_run:\n",
    "    print(f\"\\nQuery: {evaluation_data_single_run['query']}\")\n",
    "\n",
    "if 'response' in evaluation_data_single_run:\n",
    "    print(\n",
    "        f\"\\nResponse preview: {str(evaluation_data_single_run['response'])[:200]}...\")\n",
    "\n",
    "if 'tool_calls' in evaluation_data_single_run:\n",
    "    tool_calls = evaluation_data_single_run['tool_calls']\n",
    "    print(\n",
    "        f\"\\nNumber of tool calls: {len(tool_calls) if isinstance(tool_calls, list) else 'N/A'}\")\n",
    "\n",
    "if 'conversation' in evaluation_data_single_run:\n",
    "    conv = evaluation_data_single_run['conversation']\n",
    "    if isinstance(conv, dict) and 'messages' in conv:\n",
    "        print(f\"\\nNumber of messages in conversation: {len(conv['messages'])}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7354caf8",
   "metadata": {},
   "source": [
    "## Part 5: Model Configuration for AI-Assisted Evaluators\n",
    "\n",
    "Configure the model that will act as the LLM-judge for evaluation. Azure AI supports both reasoning models (o-series) and non-reasoning models (GPT-4/GPT-4o) as judges.\n",
    "\n",
    "### Supported Models\n",
    "\n",
    "- **Reasoning Models** (e.g., o1, o3-mini): Set `is_reasoning_model=True` when initializing evaluators\n",
    "- **Non-Reasoning Models** (e.g., GPT-4.1, GPT-4o): Default configuration\n",
    "\n",
    "For complex evaluation requiring refined reasoning, we recommend using strong reasoning models like o3-mini or GPT-4.1-mini for a balance of performance and cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d11614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import (\n",
    "    ToolCallAccuracyEvaluator,\n",
    "    AzureOpenAIModelConfiguration,\n",
    "    IntentResolutionEvaluator,\n",
    "    TaskAdherenceEvaluator,\n",
    ")\n",
    "from pprint import pprint\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT_GPT_4o\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY_GPT_4o\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION_GPT_4o\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_MODEl_GPT_4o\"],\n",
    ")\n",
    "# Needed to use content safety evaluators\n",
    "azure_ai_project = os.environ[\"AZURE_AI_PROJECT_ENDPOINT\"]\n",
    "\n",
    "intent_resolution = IntentResolutionEvaluator(model_config=model_config)\n",
    "\n",
    "tool_call_accuracy = ToolCallAccuracyEvaluator(model_config=model_config)\n",
    "\n",
    "task_adherence = TaskAdherenceEvaluator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d808a6",
   "metadata": {},
   "source": [
    "## Part 6: Run Batch Evaluation\n",
    "\n",
    "Run all configured evaluators on the evaluation dataset and upload results to Azure AI Foundry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0771c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "response = evaluate(\n",
    "    data=file_name,\n",
    "    evaluators={\n",
    "        \"tool_call_accuracy\": tool_call_accuracy,\n",
    "        \"intent_resolution\": intent_resolution,\n",
    "        \"task_adherence\": task_adherence,\n",
    "    },\n",
    "    azure_ai_project=azure_ai_project,\n",
    ")\n",
    "pprint(f'AI Foundary URL: {response.get(\"studio_url\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acee2f3",
   "metadata": {},
   "source": [
    "## Part 7: Individual Evaluator Examples\n",
    "\n",
    "Test each evaluator individually with specific examples to understand their behavior and output format.\n",
    "\n",
    "### 7.1: Intent Resolution Evaluator\n",
    "\n",
    "**Purpose**: Measures how well the system identifies and understands user intent, including:\n",
    "\n",
    "- How well it scopes the user's intent\n",
    "- Whether it asks clarifying questions\n",
    "- If it reminds users of capability scope\n",
    "\n",
    "**Output**: Likert scale score (1-5, higher is better)\n",
    "\n",
    "- Score >= threshold → pass\n",
    "- Score < threshold → fail\n",
    "\n",
    "**Use Case**: Evaluate if your agent correctly identifies what users want to accomplish.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0413090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import IntentResolutionEvaluator\n",
    "\n",
    "intent_resolution = IntentResolutionEvaluator(\n",
    "    model_config=model_config, threshold=3)\n",
    "intent_resolution(\n",
    "    query=\"What are the opening hours of the Eiffel Tower?\",\n",
    "    response=\"Opening hours of the Eiffel Tower are 9:00 AM to 11:00 PM.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a6ee0d",
   "metadata": {},
   "source": [
    "### 7.2: Tool Call Accuracy Evaluator\n",
    "\n",
    "**Purpose**: Measures the accuracy and efficiency of tool calls made by an agent, including:\n",
    "\n",
    "- Relevance and helpfulness of tools invoked\n",
    "- Correctness of parameters used\n",
    "- Counts of missing or excessive calls\n",
    "\n",
    "**Supported Tools**:\n",
    "\n",
    "- File Search\n",
    "- Azure AI Search\n",
    "- Bing Grounding, Bing Custom Search\n",
    "- SharePoint Grounding\n",
    "- Code Interpreter\n",
    "- Fabric Data Agent\n",
    "- OpenAPI\n",
    "- Function Tool (user-defined tools)\n",
    "\n",
    "**Output**: Likert scale score (1-5, higher is better) plus detailed breakdown:\n",
    "\n",
    "- `tool_calls_made_by_agent`: Total calls made\n",
    "- `correct_tool_calls_made_by_agent`: Correct calls\n",
    "- `per_tool_call_details`: Per-tool analysis\n",
    "- `excess_tool_calls`: Unnecessary calls\n",
    "- `missing_tool_calls`: Required calls not made\n",
    "\n",
    "**Use Case**: Evaluate if your agent selects the right tools and uses them correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1cdfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import ToolCallAccuracyEvaluator\n",
    "\n",
    "tool_call_accuracy = ToolCallAccuracyEvaluator(\n",
    "    model_config=model_config, threshold=3)\n",
    "\n",
    "# provide the agent response with tool calls\n",
    "tool_call_accuracy(\n",
    "    query=\"What timezone corresponds to 41.8781,-87.6298?\",\n",
    "    response=[\n",
    "        {\n",
    "            \"createdAt\": \"2025-04-25T23:55:52Z\",\n",
    "            \"run_id\": \"run_DmnhUGqYd1vCBolcjjODVitB\",\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"tool_call\",\n",
    "                    \"tool_call_id\": \"call_qi2ug31JqzDuLy7zF5uiMbGU\",\n",
    "                    \"name\": \"azure_maps_timezone\",\n",
    "                    \"arguments\": {\n",
    "                        \"lat\": 41.878100000000003,\n",
    "                        \"lon\": -87.629800000000003\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"createdAt\": \"2025-04-25T23:55:54Z\",\n",
    "            \"run_id\": \"run_DmnhUGqYd1vCBolcjjODVitB\",\n",
    "            \"tool_call_id\": \"call_qi2ug31JqzDuLy7zF5uiMbGU\",\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"tool_result\",\n",
    "                    \"tool_result\": {\n",
    "                        \"ianaId\": \"America/Chicago\",\n",
    "                        \"utcOffset\": None,\n",
    "                        \"abbreviation\": None,\n",
    "                        \"isDaylightSavingTime\": None\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"createdAt\": \"2025-04-25T23:55:55Z\",\n",
    "            \"run_id\": \"run_DmnhUGqYd1vCBolcjjODVitB\",\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"The timezone for the coordinates 41.8781, -87.6298 is America/Chicago.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    tool_definitions=[\n",
    "        {\n",
    "            \"name\": \"azure_maps_timezone\",\n",
    "                    \"description\": \"local time zone information for a given latitude and longitude.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"lat\": {\n",
    "                                \"type\": \"float\",\n",
    "                                \"description\": \"The latitude of the location.\"\n",
    "                            },\n",
    "                            \"lon\": {\n",
    "                                \"type\": \"float\",\n",
    "                                \"description\": \"The longitude of the location.\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# alternatively, provide the tool calls directly without the full agent response\n",
    "tool_call_accuracy(\n",
    "    query=\"How is the weather in Seattle?\",\n",
    "    tool_calls=[{\n",
    "        \"type\": \"tool_call\",\n",
    "        \"tool_call_id\": \"call_CUdbkBfvVBla2YP3p24uhElJ\",\n",
    "        \"name\": \"fetch_weather\",\n",
    "        \"arguments\": {\n",
    "                        \"location\": \"Seattle\"\n",
    "        }\n",
    "    }],\n",
    "    tool_definitions=[{\n",
    "        \"id\": \"fetch_weather\",\n",
    "        \"name\": \"fetch_weather\",\n",
    "        \"description\": \"Fetches the weather information for the specified location.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The location to fetch weather for.\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0af9497",
   "metadata": {},
   "source": [
    "### 7.3: Task Adherence Evaluator\n",
    "\n",
    "**Purpose**: Assesses whether the agent stays on track to complete tasks instead of making inefficient or out-of-scope steps. Measures how well an agent's response adheres to:\n",
    "\n",
    "- Their assigned tasks\n",
    "- Task instructions (extracted from system message and user query)\n",
    "- Available tools\n",
    "\n",
    "**Output**: Likert scale score (1-5, higher is better)\n",
    "\n",
    "- Score >= threshold → pass (good adherence)\n",
    "- Score < threshold → fail (agent went off-track)\n",
    "\n",
    "**Use Case**: Evaluate if your agent completes tasks efficiently without unnecessary or irrelevant actions.\n",
    "\n",
    "**Input Format**: Accepts either:\n",
    "\n",
    "- `conversation`: Dict with messages array (shown below)\n",
    "- `query` and `response`: Individual strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e2f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import TaskAdherenceEvaluator\n",
    "from IPython.display import display, HTML\n",
    "import json as json_module\n",
    "\n",
    "task_adherence = TaskAdherenceEvaluator(model_config=model_config, threshold=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdcdf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure example, there's only a vague adherence to the task\n",
    "result = task_adherence(\n",
    "    query=\"What are the best practices for maintaining a healthy rose garden during the summer?\",\n",
    "    response=\"Make sure to water your roses regularly and trim them occasionally.\",\n",
    ")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcea7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Success example, full adherence to the task\n",
    "result = task_adherence(\n",
    "    query=\"What are the best practices for maintaining a healthy rose garden during the summer?\",\n",
    "    response=\"For optimal summer care of your rose garden, start by watering deeply early in the morning to ensure the roots are well-hydrated without encouraging fungal growth. Apply a 2-3 inch layer of organic mulch around the base of the plants to conserve moisture and regulate soil temperature. Fertilize with a balanced rose fertilizer every 4 to 6 weeks to support healthy growth. Prune away any dead or diseased wood to promote good air circulation, and inspect regularly for pests such as aphids or spider mites, treating them promptly with an appropriate organic insecticidal soap. Finally, ensure that your roses receive at least 6 hours of direct sunlight daily for robust flowering.\",\n",
    ")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34905e6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete workflow for evaluating Azure AI agents:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Agent Creation**: Built an agent with function tools for real-world scenarios\n",
    "2. **Data Generation**: Executed agent runs to create evaluation data\n",
    "3. **Native Integration**: Used AIAgentConverter for seamless evaluation\n",
    "4. **Three Agent Evaluators**:\n",
    "   - **Intent Resolution**: Validates user intent understanding\n",
    "   - **Tool Call Accuracy**: Ensures correct tool selection and usage\n",
    "   - **Task Adherence**: Confirms agents stay on task\n",
    "5. **Flexible Evaluation**: Both batch evaluation and individual testing supported\n",
    "\n",
    "### Evaluation Results Interpretation\n",
    "\n",
    "All three evaluators use a **Likert scale (1-5)**:\n",
    "\n",
    "- **5**: Excellent - Agent performed optimally\n",
    "- **4**: Good - Minor issues, acceptable performance\n",
    "- **3**: Fair - Threshold for pass/fail (default)\n",
    "- **2**: Poor - Significant problems detected\n",
    "- **1**: Very Poor - Critical failures\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Set Appropriate Thresholds**: Default is 3, but adjust based on your quality requirements\n",
    "2. **Use Reasoning Models**: For complex scenarios, enable `is_reasoning_model=True` with o-series models\n",
    "3. **Analyze Details**: Review the `reason` and `additional_details` fields to understand scores\n",
    "4. **Track Over Time**: Run evaluations regularly to monitor agent improvements\n",
    "5. **Combine Evaluators**: Use all three together for comprehensive assessment\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Azure AI Evaluation Documentation](https://learn.microsoft.com/azure/ai-studio/how-to/evaluate-sdk)\n",
    "- [Agent Evaluators Reference](https://learn.microsoft.com/azure/ai-studio/how-to/evaluate-agent)\n",
    "- [Azure AI Foundry Studio](https://ai.azure.com)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
