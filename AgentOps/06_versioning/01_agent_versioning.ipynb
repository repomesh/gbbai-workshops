{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a02c76",
   "metadata": {},
   "source": [
    "# Agent CI/CD with Versioning and Automated Evaluation\n",
    "\n",
    "This notebook demonstrates a complete CI/CD workflow for Azure AI agents with:\n",
    "\n",
    "## Key Features\n",
    "- üîÑ **Automatic Versioning**: Every agent update creates a version snapshot\n",
    "- üìä **Automated Evaluation**: Trigger evaluations after configuration changes\n",
    "- üìú **Version History**: Track all changes with timestamps and descriptions\n",
    "- üîç **Version Comparison**: Compare different versions side-by-side with detailed diffs\n",
    "- üìà **Visual Score Comparison**: See performance differences with charts and metrics\n",
    "- üéØ **Change Tracking**: Identify exactly what changed between versions (instructions, descriptions, prompts)\n",
    "- ‚èÆÔ∏è **Rollback Support**: Restore previous versions when needed\n",
    "- üöÄ **CI/CD Pipeline**: Update ‚Üí Version ‚Üí Evaluate ‚Üí Deploy workflow\n",
    "\n",
    "## Workflow Overview\n",
    "1. **Update Agent Configuration**: Modify system prompt, parameters, or tools\n",
    "2. **Automatic Versioning**: Current state saved to `versions` array, `currentVersion` incremented\n",
    "3. **Trigger Evaluation**: Run agent evaluators (Intent Resolution, Tool Call Accuracy, Task Adherence)\n",
    "4. **Review Results**: Analyze evaluation metrics before promoting to production\n",
    "5. **Rollback if Needed**: Restore previous version if evaluation fails\n",
    "\n",
    "## Version Control Schema\n",
    "Each version snapshot contains:\n",
    "- `versionNumber`: Sequential version number\n",
    "- `timestamp`: When the version was created\n",
    "- `changeDescription`: What changed in this version\n",
    "- `changedBy`: Who made the change\n",
    "- `snapshot`: Complete configuration at that point in time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2312a2b",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Environment Setup](#environment-setup)\n",
    "2. [Initialize Clients and Managers](#initialize-clients-and-managers)\n",
    "3. [Get Existing Agent](#get-existing-agent)\n",
    "4. [Update Agent with Automatic Versioning](#update-agent-with-automatic-versioning)\n",
    "5. [View Version History](#view-version-history)\n",
    "6. [Automated Evaluation After Update](#automated-evaluation-after-update)\n",
    "7. [Compare Versions](#compare-versions)\n",
    "8. [Rollback to Previous Version](#rollback-to-previous-version)\n",
    "9. [Complete CI/CD Pipeline Example](#complete-cicd-pipeline-example)\n",
    "   - Test Good vs Bad Updates\n",
    "   - Compare Evaluation Results\n",
    "   - View Version History with Changes\n",
    "   - Visual Score Comparison\n",
    "9. [Complete CI/CD Pipeline Example](#complete-cicd-pipeline-example)\n",
    "10. [Best Practices](#best-practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4149ab10",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b896fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "new_path_entry = \"/opt/homebrew/bin\"  # Replace with the directory you want to add\n",
    "current_path = os.environ.get('PATH', '')\n",
    "\n",
    "if new_path_entry not in current_path.split(os.pathsep):\n",
    "    os.environ['PATH'] = new_path_entry + os.pathsep + current_path\n",
    "    print(f\"Updated PATH for this session: {os.environ['PATH']}\")\n",
    "else:\n",
    "    print(f\"PATH already contains {new_path_entry}: {current_path}\")\n",
    "\n",
    "# You can then verify with shutil.which again\n",
    "print(f\"Location of 'az' found by kernel now: {shutil.which('az')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6191eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Add parent directory to path for agent utilities\n",
    "parent_dir = Path.cwd().parent / \"utils\"\n",
    "sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "# Add current directory for version manager\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Load environment variables\n",
    "env_path = Path.cwd().parent / \".env\"\n",
    "load_dotenv(env_path)\n",
    "\n",
    "print(\"‚úÖ Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c59c2b",
   "metadata": {},
   "source": [
    "## Initialize Clients and Managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79416e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_db import AgentDB\n",
    "from agent_utils import AgentManager\n",
    "from agent_version_manager import AgentVersionManager\n",
    "\n",
    "# Get project endpoint\n",
    "endpoint = os.getenv(\"AZURE_AI_PROJECT_ENDPOINT\")\n",
    "if not endpoint:\n",
    "    raise ValueError(\"Please set AZURE_AI_PROJECT_ENDPOINT in environment\")\n",
    "\n",
    "# Verify Cosmos DB endpoint\n",
    "cosmos_endpoint = os.getenv(\"AZURE_COSMOS_ENDPOINT\")\n",
    "if not cosmos_endpoint:\n",
    "    raise ValueError(\"Please set AZURE_COSMOS_ENDPOINT in environment\")\n",
    "\n",
    "# Initialize Azure AI Project Client\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "# Initialize managers\n",
    "agent_manager = AgentManager(project_client=project_client)\n",
    "version_manager = AgentVersionManager(agent_manager=agent_manager)\n",
    "\n",
    "print(\"‚úÖ Clients initialized\")\n",
    "print(f\"üì¶ Project Endpoint: {endpoint}\")\n",
    "print(f\"üì¶ Cosmos DB Endpoint: {cosmos_endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebbf4cc",
   "metadata": {},
   "source": [
    "## Get Existing Agent\n",
    "\n",
    "Let's retrieve the Web Research Assistant that we want to manage with CI/CD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f7122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the agent from database using Azure Agent ID\n",
    "AZURE_AGENT_ID = \"asst_CncymMdTqov5hRCQQJrvLwhX\"\n",
    "\n",
    "agent_data = agent_manager.get_agent_metadata(azure_agent_id=AZURE_AGENT_ID)\n",
    "\n",
    "if not agent_data:\n",
    "    raise ValueError(f\"Agent not found: {AZURE_AGENT_ID}\")\n",
    "\n",
    "print(f\"‚úÖ Agent retrieved: {agent_data['name']}\")\n",
    "print(f\"üìã Current version: {agent_data.get('currentVersion', 'Not set')}\")\n",
    "print(f\"üìù Status: {agent_data['status']}\")\n",
    "print(f\"üìÇ Category: {agent_data['category']}\")\n",
    "print(f\"\\nüìÑ Current instruction (first 200 chars):\")\n",
    "print(agent_data['instruction'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa5c02b",
   "metadata": {},
   "source": [
    "## Update Agent with Automatic Versioning\n",
    "\n",
    "When we update the agent configuration, the system will:\n",
    "1. Save the current state to the `versions` array\n",
    "2. Apply the updates\n",
    "3. Increment the `currentVersion` field\n",
    "4. Update timestamps\n",
    "\n",
    "This creates a complete audit trail of all changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52f6c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the updates we want to make\n",
    "updates = {\n",
    "    \"instruction\": \"\"\"You are an Advanced Web Research Assistant with real-time web search capabilities.\n",
    "\n",
    "## Role\n",
    "You help users find current information from the web, including:\n",
    "- Latest news and current events with source verification\n",
    "- Recent industry updates and emerging trends\n",
    "- Real-time data, statistics, and market insights\n",
    "- Current product information, reviews, and comparisons\n",
    "- Academic research and technical documentation\n",
    "\n",
    "## Enhanced Capabilities\n",
    "1. **Multi-source Verification**: Cross-reference information from multiple sources\n",
    "2. **Trend Analysis**: Identify patterns and trends in search results\n",
    "3. **Source Quality Assessment**: Evaluate credibility of information sources\n",
    "4. **Contextual Summarization**: Provide concise summaries with key insights\n",
    "\n",
    "## Constraints\n",
    "1. When asked about current events or recent information, use the Bing search tool to find up-to-date information\n",
    "2. Always cite your sources with links to the websites you reference\n",
    "3. Present information clearly and concisely, using structured formats when appropriate\n",
    "4. If information is time-sensitive, mention when the data was retrieved\n",
    "5. For controversial topics, present multiple perspectives with proper attribution\n",
    "6. Reply in English unless specifically asked for another language\n",
    "7. If search results are insufficient, acknowledge limitations and suggest alternative approaches\n",
    "\n",
    "## Quality Standards\n",
    "- Verify facts across multiple sources when possible\n",
    "- Distinguish between facts, opinions, and speculation\n",
    "- Provide context for statistics and data points\n",
    "- Update users if information may have changed since retrieval\"\"\",\n",
    "    \"description\": \"Advanced Web Research Assistant with enhanced verification and analysis capabilities. Uses Bing Search for real-time information retrieval with multi-source validation.\",\n",
    "    \"sample_prompts\": [\n",
    "        \"What are the latest AI developments this week with source verification?\",\n",
    "        \"Compare the top 3 cloud computing platforms based on recent reviews\",\n",
    "        \"What are the emerging trends in sustainable technology?\",\n",
    "        \"Find and summarize recent research on quantum computing applications\",\n",
    "        \"What is the current market sentiment on electric vehicles?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Update with versioning\n",
    "success = version_manager.update_agent_with_versioning(\n",
    "    agent_id=agent_data[\"id\"],\n",
    "    updates=updates,\n",
    "    change_description=\"Enhanced research capabilities with multi-source verification and trend analysis\",\n",
    "    changed_by=\"xle@microsoft.com\"\n",
    ")\n",
    "\n",
    "if success:\n",
    "    print(\"‚úÖ Agent updated successfully with versioning\")\n",
    "    \n",
    "    # Get updated data\n",
    "    updated_agent = agent_manager.get_agent_metadata(azure_agent_id=AZURE_AGENT_ID)\n",
    "    print(f\"üìã New version: {updated_agent.get('currentVersion')}\")\n",
    "    print(f\"üìù Versions in history: {len(updated_agent.get('versions', []))}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to update agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b36cbb",
   "metadata": {},
   "source": [
    "## View Version History\n",
    "\n",
    "Let's examine the version history to see all changes made to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a940c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get version history\n",
    "version_history = version_manager.get_version_history(agent_id=agent_data[\"id\"])\n",
    "\n",
    "print(f\"üìú Version History ({len(version_history)} versions)\\n\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for version in version_history:\n",
    "    print(f\"\\nüîñ Version {version['versionNumber']}\")\n",
    "    print(f\"   ‚è∞ Timestamp: {version['timestamp']}\")\n",
    "    print(f\"   üë§ Changed by: {version['changedBy']}\")\n",
    "    print(f\"   üìù Description: {version['changeDescription']}\")\n",
    "    \n",
    "    snapshot = version['snapshot']\n",
    "    print(f\"   üìÑ Instruction preview: {snapshot.get('instruction', '')[:100]}...\")\n",
    "    print(f\"   üìÇ Category: {snapshot.get('category')}\")\n",
    "    print(f\"   üìä Status: {snapshot.get('status')}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a5c5a9",
   "metadata": {},
   "source": [
    "## Automated Evaluation After Update\n",
    "\n",
    "After updating the agent, we should evaluate its performance to ensure the changes improved quality.\n",
    "We'll run the agent through test scenarios and evaluate with Azure AI evaluators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb94403",
   "metadata": {},
   "source": [
    "### Step 1: Prepare Evaluation Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e6482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import (\n",
    "    IntentResolutionEvaluator,\n",
    "    ToolCallAccuracyEvaluator,\n",
    "    AzureOpenAIModelConfiguration,\n",
    "    AIAgentConverter\n",
    ")\n",
    "\n",
    "# Configure the model for evaluation (LLM judge)\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT_GPT_4o\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY_GPT_4o\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION_GPT_4o\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_MODEl_GPT_4o\"],\n",
    ")\n",
    "\n",
    "# Initialize evaluators\n",
    "intent_resolution = IntentResolutionEvaluator(model_config=model_config, threshold=3)\n",
    "tool_call_accuracy = ToolCallAccuracyEvaluator(model_config=model_config, threshold=3)\n",
    "\n",
    "print(\"‚úÖ Evaluators initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c528adf9",
   "metadata": {},
   "source": [
    "### Step 2: Create Test Thread and Run Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85401421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread for testing\n",
    "thread = agent_manager.create_thread()\n",
    "print(f\"‚úÖ Test thread created: {thread.id}\")\n",
    "\n",
    "# Test query\n",
    "test_query = \"What are the latest developments in generative AI this month?\"\n",
    "\n",
    "# Add message to thread\n",
    "message = project_client.agents.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=test_query\n",
    ")\n",
    "print(f\"‚úÖ Test message added: {test_query}\")\n",
    "\n",
    "# Run the agent\n",
    "run = project_client.agents.runs.create_and_process(\n",
    "    thread_id=thread.id,\n",
    "    agent_id=AZURE_AGENT_ID\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Agent run completed with status: {run.status}\")\n",
    "\n",
    "# Display conversation\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"AGENT CONVERSATION\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for msg in project_client.agents.messages.list(thread.id, order=\"asc\"):\n",
    "    print(f\"\\n{msg.role.upper()}:\")\n",
    "    print(msg.content[0].text.value)\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914991aa",
   "metadata": {},
   "source": [
    "### Step 3: Convert Agent Data for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6931c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize converter\n",
    "converter = AIAgentConverter(project_client)\n",
    "\n",
    "# Convert agent run to evaluation format\n",
    "evaluation_data = converter.convert(thread_id=thread.id, run_id=run.id)\n",
    "\n",
    "print(\"‚úÖ Evaluation data converted\")\n",
    "print(f\"\\nüìä Data structure:\")\n",
    "print(json.dumps(evaluation_data, indent=2, default=str)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9096d28f",
   "metadata": {},
   "source": [
    "### Step 4: Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47e9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluators\n",
    "print(\"üîç Running evaluations...\\n\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Intent Resolution\n",
    "print(\"\\nüìã Intent Resolution Evaluator\")\n",
    "intent_result = intent_resolution(\n",
    "    query=evaluation_data.get(\"query\"),\n",
    "    response=evaluation_data.get(\"response\")\n",
    ")\n",
    "print(f\"  Score: {intent_result.get('intent_resolution')}\")\n",
    "print(f\"  Pass: {intent_result.get('intent_resolution_pass')}\")\n",
    "print(f\"  Reason: {intent_result.get('intent_resolution_reason')}\")\n",
    "\n",
    "# Tool Call Accuracy\n",
    "print(\"\\nüîß Tool Call Accuracy Evaluator\")\n",
    "if evaluation_data.get(\"tool_calls\"):\n",
    "    tool_result = tool_call_accuracy(\n",
    "        query=evaluation_data.get(\"query\"),\n",
    "        response=evaluation_data.get(\"response\"),\n",
    "        tool_calls=evaluation_data.get(\"tool_calls\"),\n",
    "        tool_definitions=evaluation_data.get(\"tool_definitions\", [])\n",
    "    )\n",
    "    print(f\"  Score: {tool_result.get('tool_call_accuracy')}\")\n",
    "    print(f\"  Pass: {tool_result.get('tool_call_accuracy_pass')}\")\n",
    "    print(f\"  Correct calls: {tool_result.get('correct_tool_calls_made_by_agent')}\")\n",
    "    print(f\"  Total calls: {tool_result.get('tool_calls_made_by_agent')}\")\n",
    "else:\n",
    "    print(\"  No tool calls detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cdae0d",
   "metadata": {},
   "source": [
    "### Step 5: Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed5300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile evaluation results\n",
    "evaluation_results = {\n",
    "    \"version\": updated_agent.get(\"currentVersion\"),\n",
    "    \"timestamp\": updated_agent.get(\"dateModified\"),\n",
    "    \"test_query\": test_query,\n",
    "    \"intent_resolution\": {\n",
    "        \"score\": intent_result.get('intent_resolution'),\n",
    "        \"pass\": intent_result.get('intent_resolution_pass'),\n",
    "        \"reason\": intent_result.get('intent_resolution_reason')\n",
    "    },\n",
    "    \"tool_call_accuracy\": {\n",
    "        \"score\": tool_result.get('tool_call_accuracy') if evaluation_data.get(\"tool_calls\") else \"N/A\",\n",
    "        \"pass\": tool_result.get('tool_call_accuracy_pass') if evaluation_data.get(\"tool_calls\") else \"N/A\"\n",
    "    } if evaluation_data.get(\"tool_calls\") else {\"score\": \"N/A\", \"pass\": \"N/A\"}\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "eval_filename = f\"data/evaluation_v{updated_agent.get('currentVersion')}.json\"\n",
    "with open(eval_filename, 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"‚úÖ Evaluation results saved to: {eval_filename}\")\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"  Version: {evaluation_results['version']}\")\n",
    "print(f\"  Intent Resolution: {evaluation_results['intent_resolution']['score']} (Pass: {evaluation_results['intent_resolution']['pass']})\")\n",
    "print(f\"  Tool Call Accuracy: {evaluation_results['tool_call_accuracy']['score']} (Pass: {evaluation_results['tool_call_accuracy']['pass']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6890d06d",
   "metadata": {},
   "source": [
    "## Compare Versions\n",
    "\n",
    "Let's compare different versions to see what changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d8709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current version number and version history\n",
    "current_version = version_manager.get_current_version_number(agent_data[\"id\"])\n",
    "version_history = version_manager.get_version_history(\n",
    "    agent_id=agent_data[\"id\"])\n",
    "\n",
    "if len(version_history) >= 1:\n",
    "    # Get the most recent version snapshot from history\n",
    "    previous_version = version_history[0]  # Most recent in history\n",
    "\n",
    "    # Get current agent state\n",
    "    current_agent = agent_manager.get_agent_metadata(\n",
    "        azure_agent_id=AZURE_AGENT_ID)\n",
    "\n",
    "    print(\n",
    "        f\"üìä Comparing Version {previous_version['versionNumber']} (Previous) vs Current State (Version {current_version})\\n\")\n",
    "\n",
    "    # Compare key fields\n",
    "    differences = {}\n",
    "    prev_snapshot = previous_version['snapshot']\n",
    "\n",
    "    for key in [\"name\", \"description\", \"instruction\", \"category\", \"status\", \"samplePrompts\"]:\n",
    "        prev_val = prev_snapshot.get(key)\n",
    "        curr_val = current_agent.get(key)\n",
    "\n",
    "        if prev_val != curr_val:\n",
    "            differences[key] = {\n",
    "                \"previous\": prev_val,\n",
    "                \"current\": curr_val\n",
    "            }\n",
    "\n",
    "    if differences:\n",
    "        from IPython.display import display, Markdown\n",
    "\n",
    "        print(\"\\nüîç Differences Found:\\n\")\n",
    "\n",
    "        # Create markdown table\n",
    "        table_rows = [\"| Field | Version {} (Previous) | Version {} (Current) |\".format(\n",
    "            previous_version['versionNumber'], current_version\n",
    "        )]\n",
    "        table_rows.append(\n",
    "            \"|-------|----------------------|---------------------|\")\n",
    "\n",
    "        for field, changes in differences.items():\n",
    "            prev_val = changes.get(\"previous\", \"\")\n",
    "            curr_val = changes.get(\"current\", \"\")\n",
    "\n",
    "            # Format values for table display\n",
    "            if isinstance(prev_val, str):\n",
    "                # Truncate long strings and escape markdown\n",
    "                prev_display = prev_val[:150].replace(\n",
    "                    \"\\n\", \" \").replace(\"|\", \"\\\\|\")\n",
    "                if len(prev_val) > 150:\n",
    "                    prev_display += \"...\"\n",
    "            elif isinstance(prev_val, list):\n",
    "                prev_display = f\"{len(prev_val)} items\"\n",
    "            else:\n",
    "                prev_display = str(prev_val)\n",
    "\n",
    "            if isinstance(curr_val, str):\n",
    "                curr_display = curr_val[:150].replace(\n",
    "                    \"\\n\", \" \").replace(\"|\", \"\\\\|\")\n",
    "                if len(curr_val) > 150:\n",
    "                    curr_display += \"...\"\n",
    "            elif isinstance(curr_val, list):\n",
    "                curr_display = f\"{len(curr_val)} items\"\n",
    "            else:\n",
    "                curr_display = str(curr_val)\n",
    "\n",
    "            table_rows.append(\n",
    "                f\"| **{field}** | {prev_display} | {curr_display} |\")\n",
    "\n",
    "        markdown_table = \"\\n\".join(table_rows)\n",
    "        display(Markdown(markdown_table))\n",
    "\n",
    "        # Show detailed differences for arrays\n",
    "        for field, changes in differences.items():\n",
    "            if isinstance(changes.get(\"previous\"), list) or isinstance(changes.get(\"current\"), list):\n",
    "                print(f\"\\nüìã Detailed {field}:\")\n",
    "                print(\n",
    "                    f\"\\n  Version {previous_version['versionNumber']} (Previous):\")\n",
    "                for item in changes.get(\"previous\", []):\n",
    "                    print(f\"    - {item}\")\n",
    "                print(f\"\\n  Version {current_version} (Current):\")\n",
    "                for item in changes.get(\"current\", []):\n",
    "                    print(f\"    - {item}\")\n",
    "                print(\"-\" * 80)\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No differences detected\")\n",
    "else:\n",
    "    print(\"No version history available yet, cannot compare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978f5d22",
   "metadata": {},
   "source": [
    "## Rollback to Previous Version\n",
    "\n",
    "If the evaluation results are unsatisfactory, we can rollback to a previous version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843256dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Rollback to previous version\n",
    "# UNCOMMENT TO EXECUTE ROLLBACK\n",
    "\n",
    "# current_version = version_manager.get_current_version_number(agent_data[\"id\"])\n",
    "# \n",
    "# if current_version > 1:\n",
    "#     target_version = current_version - 1\n",
    "#     \n",
    "#     print(f\"‚èÆÔ∏è  Rolling back from version {current_version} to version {target_version}...\")\n",
    "#     \n",
    "#     success = version_manager.rollback_to_version(\n",
    "#         agent_id=agent_data[\"id\"],\n",
    "#         version_number=target_version,\n",
    "#         change_description=f\"Rollback to version {target_version} due to evaluation concerns\",\n",
    "#         changed_by=\"xle@microsoft.com\"\n",
    "#     )\n",
    "#     \n",
    "#     if success:\n",
    "#         print(\"‚úÖ Rollback successful\")\n",
    "#         \n",
    "#         # Verify rollback\n",
    "#         updated_agent = agent_manager.get_agent_metadata(azure_agent_id=AZURE_AGENT_ID)\n",
    "#         print(f\"üìã Current version after rollback: {updated_agent.get('currentVersion')}\")\n",
    "#     else:\n",
    "#         print(\"‚ùå Rollback failed\")\n",
    "# else:\n",
    "#     print(\"Cannot rollback - only one version exists\")\n",
    "\n",
    "print(\"‚ÑπÔ∏è  Rollback example code is commented out. Uncomment to execute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a53ea0b",
   "metadata": {},
   "source": [
    "## Complete CI/CD Pipeline Example\n",
    "\n",
    "Here's a complete function that encapsulates the entire CI/CD workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feb70c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_cicd_pipeline(\n",
    "    agent_id: str,\n",
    "    updates: dict,\n",
    "    change_description: str,\n",
    "    changed_by: str,\n",
    "    test_queries: list,\n",
    "    evaluation_threshold: float = 3.0,\n",
    "    auto_rollback: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete CI/CD pipeline for agent updates\n",
    "    \n",
    "    Args:\n",
    "        agent_id: Agent ID\n",
    "        updates: Dict of updates to apply\n",
    "        change_description: Description of changes\n",
    "        changed_by: Who made the change\n",
    "        test_queries: List of test queries for evaluation\n",
    "        evaluation_threshold: Minimum acceptable score\n",
    "        auto_rollback: Whether to auto-rollback on failure\n",
    "    \n",
    "    Returns:\n",
    "        Dict with pipeline results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"success\": False,\n",
    "        \"version_created\": None,\n",
    "        \"evaluation_passed\": False,\n",
    "        \"rolled_back\": False,\n",
    "        \"errors\": [],\n",
    "        \"evaluation_scores\": [],\n",
    "        \"instruction\": None,\n",
    "        \"description\": None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Update with versioning\n",
    "        print(\"üìù Step 1: Updating agent with versioning...\")\n",
    "        success = version_manager.update_agent_with_versioning(\n",
    "            agent_id=agent_id,\n",
    "            updates=updates,\n",
    "            change_description=change_description,\n",
    "            changed_by=changed_by\n",
    "        )\n",
    "        \n",
    "        if not success:\n",
    "            results[\"errors\"].append(\"Update failed\")\n",
    "            return results\n",
    "        \n",
    "        current_version = version_manager.get_current_version_number(agent_id)\n",
    "        results[\"version_created\"] = current_version\n",
    "        \n",
    "        # Store the instruction and description for comparison\n",
    "        agent_metadata = agent_manager.get_agent_metadata(agent_id=agent_id)\n",
    "        results[\"instruction\"] = agent_metadata.get(\"instruction\", \"\")[:200] + \"...\"\n",
    "        results[\"description\"] = agent_metadata.get(\"description\", \"\")\n",
    "        \n",
    "        print(f\"‚úÖ Version {current_version} created\")\n",
    "        \n",
    "        # Step 2: Run evaluations\n",
    "        print(\"\\nüîç Step 2: Running evaluations...\")\n",
    "        evaluation_scores = []\n",
    "        \n",
    "        # Get Azure agent ID for API calls\n",
    "        agent_metadata = agent_manager.get_agent_metadata(agent_id=agent_id)\n",
    "        azure_agent_id = agent_metadata.get(\"azure_agent_id\")\n",
    "        \n",
    "        if not azure_agent_id:\n",
    "            results[\"errors\"].append(\"Azure agent ID not found\")\n",
    "            return results\n",
    "        \n",
    "        for test_query in test_queries:\n",
    "            # Create thread and run agent\n",
    "            thread = agent_manager.create_thread()\n",
    "            message = project_client.agents.messages.create(\n",
    "                thread_id=thread.id,\n",
    "                role=\"user\",\n",
    "                content=test_query\n",
    "            )\n",
    "            run = project_client.agents.runs.create_and_process(\n",
    "                thread_id=thread.id,\n",
    "                agent_id=azure_agent_id  # Use Azure agent ID, not local DB ID\n",
    "            )\n",
    "            \n",
    "            # Convert and evaluate\n",
    "            converter = AIAgentConverter(project_client)\n",
    "            eval_data = converter.convert(thread_id=thread.id, run_id=run.id)\n",
    "            \n",
    "            # Run evaluators\n",
    "            intent_result = intent_resolution(\n",
    "                query=eval_data.get(\"query\"),\n",
    "                response=eval_data.get(\"response\")\n",
    "            )\n",
    "            \n",
    "            scores = [\n",
    "                intent_result.get('intent_resolution', 0),\n",
    "            ]\n",
    "            evaluation_scores.extend(scores)\n",
    "            \n",
    "            # Cleanup\n",
    "            agent_manager.delete_thread(thread.id, silent=True)\n",
    "        \n",
    "        # Step 3: Analyze results\n",
    "        avg_score = sum(evaluation_scores) / len(evaluation_scores) if evaluation_scores else 0\n",
    "        results[\"evaluation_scores\"] = evaluation_scores\n",
    "        results[\"average_score\"] = avg_score\n",
    "        results[\"evaluation_passed\"] = avg_score >= evaluation_threshold\n",
    "        \n",
    "        print(f\"\\nüìä Average evaluation score: {avg_score:.2f}\")\n",
    "        print(f\"   Threshold: {evaluation_threshold}\")\n",
    "        print(f\"   Status: {'‚úÖ PASSED' if results['evaluation_passed'] else '‚ùå FAILED'}\")\n",
    "        \n",
    "        # Step 4: Rollback if needed\n",
    "        if not results[\"evaluation_passed\"] and auto_rollback and current_version > 1:\n",
    "            print(f\"\\n‚èÆÔ∏è  Step 4: Rolling back to version {current_version - 1}...\")\n",
    "            rollback_success = version_manager.rollback_to_version(\n",
    "                agent_id=agent_id,\n",
    "                version_number=current_version - 1,\n",
    "                change_description=f\"Auto-rollback: evaluation score {avg_score:.2f} below threshold {evaluation_threshold}\",\n",
    "                changed_by=\"system\"\n",
    "            )\n",
    "            results[\"rolled_back\"] = rollback_success\n",
    "            if rollback_success:\n",
    "                print(\"‚úÖ Rollback successful\")\n",
    "            else:\n",
    "                print(\"‚ùå Rollback failed\")\n",
    "        \n",
    "        results[\"success\"] = results[\"evaluation_passed\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        results[\"errors\"].append(str(e))\n",
    "        print(f\"\\n‚ùå Pipeline error: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ CI/CD pipeline function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e8e27a",
   "metadata": {},
   "source": [
    "### Test the Complete Pipeline\n",
    "\n",
    "We'll run two experiments:\n",
    "1. **Good Update**: Enhanced instruction with clear guidelines\n",
    "2. **Bad Update**: Vague, unhelpful instruction\n",
    "\n",
    "Then compare the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4353e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries for evaluation\n",
    "import time\n",
    "test_queries = [\n",
    "    \"What are the latest AI trends?\",\n",
    "    \"Find recent news about cloud computing\"\n",
    "]\n",
    "\n",
    "# Experiment 1: Good Update - Enhanced instruction\n",
    "print(\"=\" * 100)\n",
    "print(\"EXPERIMENT 1: GOOD UPDATE\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "good_updates = {\n",
    "    \"instruction\": \"\"\"You are an Advanced Web Research Assistant with real-time web search capabilities.\n",
    "\n",
    "## Role\n",
    "You help users find current information from the web, including:\n",
    "- Latest news and current events with source verification\n",
    "- Recent industry updates and emerging trends\n",
    "- Real-time data, statistics, and market insights\n",
    "- Current product information, reviews, and comparisons\n",
    "- Academic research and technical documentation\n",
    "\n",
    "## Enhanced Capabilities\n",
    "1. **Multi-source Verification**: Cross-reference information from multiple sources\n",
    "2. **Trend Analysis**: Identify patterns and trends in search results\n",
    "3. **Source Quality Assessment**: Evaluate credibility of information sources\n",
    "4. **Contextual Summarization**: Provide concise summaries with key insights\n",
    "\n",
    "## Constraints\n",
    "1. When asked about current events or recent information, use the Bing search tool to find up-to-date information\n",
    "2. Always cite your sources with links to the websites you reference\n",
    "3. Present information clearly and concisely, using structured formats when appropriate\n",
    "4. If information is time-sensitive, mention when the data was retrieved\n",
    "5. For controversial topics, present multiple perspectives with proper attribution\n",
    "6. Reply in English unless specifically asked for another language\n",
    "7. If search results are insufficient, acknowledge limitations and suggest alternative approaches\n",
    "\n",
    "## Quality Standards\n",
    "- Verify facts across multiple sources when possible\n",
    "- Distinguish between facts, opinions, and speculation\n",
    "- Provide context for statistics and data points\n",
    "- Update users if information may have changed since retrieval\"\"\",\n",
    "    \"description\": \"Advanced Web Research Assistant with enhanced verification and analysis capabilities\"\n",
    "}\n",
    "\n",
    "good_results = agent_cicd_pipeline(\n",
    "    agent_id=agent_data[\"id\"],\n",
    "    updates=good_updates,\n",
    "    change_description=\"Enhanced instruction with clear guidelines and quality standards\",\n",
    "    changed_by=\"xle@microsoft.com\",\n",
    "    test_queries=test_queries,\n",
    "    evaluation_threshold=3.0,\n",
    "    auto_rollback=False  # Don't rollback, we want to test both versions\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Good update completed\")\n",
    "print(f\"Version created: {good_results['version_created']}\")\n",
    "print(f\"Evaluation passed: {good_results['evaluation_passed']}\")\n",
    "\n",
    "# Wait a moment before next update\n",
    "time.sleep(2)\n",
    "\n",
    "# Experiment 2: Bad Update - Vague, unhelpful instruction\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"EXPERIMENT 2: BAD UPDATE\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "bad_updates = {\n",
    "    \"instruction\": \"\"\"You are a search assistant. Help users find things on the web. Use the search tool when needed. Provide answers based on what you find.\"\"\",\n",
    "    \"description\": \"Basic search assistant\"\n",
    "}\n",
    "\n",
    "bad_results = agent_cicd_pipeline(\n",
    "    agent_id=agent_data[\"id\"],\n",
    "    updates=bad_updates,\n",
    "    change_description=\"Simplified instruction (deliberately reduced quality for testing)\",\n",
    "    changed_by=\"xle@microsoft.com\",\n",
    "    test_queries=test_queries,\n",
    "    evaluation_threshold=3.0,\n",
    "    auto_rollback=False  # Don't rollback, we want to compare results\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Bad update completed\")\n",
    "print(f\"Version created: {bad_results['version_created']}\")\n",
    "print(f\"Evaluation passed: {bad_results['evaluation_passed']}\")\n",
    "\n",
    "# Store results for comparison\n",
    "experiment_results = {\n",
    "    \"good\": good_results,\n",
    "    \"bad\": bad_results\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc8203",
   "metadata": {},
   "source": [
    "### Compare Evaluation Results\n",
    "\n",
    "Let's compare the two experiments side by side to see the impact of instruction quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7152f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "\n",
    "# Extract evaluation metrics for comparison\n",
    "comparison_data = {\n",
    "    \"Metric\": [\n",
    "        \"Version Number\",\n",
    "        \"Evaluation Passed\",\n",
    "        \"Average Score\",\n",
    "        \"Individual Scores\",\n",
    "        \"Rolled Back\",\n",
    "        \"Errors\"\n",
    "    ],\n",
    "    \"Good Update (Enhanced)\": [\n",
    "        good_results.get(\"version_created\", \"N/A\"),\n",
    "        \"‚úÖ Yes\" if good_results.get(\"evaluation_passed\") else \"‚ùå No\",\n",
    "        f\"{good_results.get('average_score', 0):.2f}\" if good_results.get('average_score') else \"N/A\",\n",
    "        \", \".join([f\"{s:.1f}\" for s in good_results.get('evaluation_scores', [])]) if good_results.get('evaluation_scores') else \"N/A\",\n",
    "        \"‚úÖ Yes\" if good_results.get(\"rolled_back\") else \"‚ùå No\",\n",
    "        \", \".join(good_results.get(\"errors\", [])) if good_results.get(\"errors\") else \"None\"\n",
    "    ],\n",
    "    \"Bad Update (Simplified)\": [\n",
    "        bad_results.get(\"version_created\", \"N/A\"),\n",
    "        \"‚úÖ Yes\" if bad_results.get(\"evaluation_passed\") else \"‚ùå No\",\n",
    "        f\"{bad_results.get('average_score', 0):.2f}\" if bad_results.get('average_score') else \"N/A\",\n",
    "        \", \".join([f\"{s:.1f}\" for s in bad_results.get('evaluation_scores', [])]) if bad_results.get('evaluation_scores') else \"N/A\",\n",
    "        \"‚úÖ Yes\" if bad_results.get(\"rolled_back\") else \"‚ùå No\",\n",
    "        \", \".join(bad_results.get(\"errors\", [])) if bad_results.get(\"errors\") else \"None\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Display as markdown table\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"EVALUATION COMPARISON\")\n",
    "print(\"=\" * 100 + \"\\n\")\n",
    "\n",
    "# Convert to markdown table\n",
    "markdown_table = \"| \" + \" | \".join(df.columns) + \" |\\n\"\n",
    "markdown_table += \"|\" + \"|\".join([\"---\" for _ in df.columns]) + \"|\\n\"\n",
    "for _, row in df.iterrows():\n",
    "    markdown_table += \"| \" + \" | \".join(str(val) for val in row) + \" |\\n\"\n",
    "\n",
    "display(Markdown(markdown_table))\n",
    "\n",
    "# Show instruction differences\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"INSTRUCTION COMPARISON\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\nüìù Good Update (Enhanced) - Instruction Preview:\")\n",
    "print(\"-\" * 100)\n",
    "print(good_results.get(\"instruction\", \"N/A\"))\n",
    "print(f\"\\nüìã Description: {good_results.get('description', 'N/A')}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 100)\n",
    "print(\"\\nüìù Bad Update (Simplified) - Instruction Preview:\")\n",
    "print(\"-\" * 100)\n",
    "print(bad_results.get(\"instruction\", \"N/A\"))\n",
    "print(f\"\\nüìã Description: {bad_results.get('description', 'N/A')}\")\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "\n",
    "# Display detailed results\n",
    "print(\"\\nüìä Detailed Results:\\n\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nüü¢ GOOD UPDATE (Enhanced Instruction)\")\n",
    "print(\"-\" * 100)\n",
    "print(json.dumps(good_results, indent=2, default=str))\n",
    "\n",
    "print(\"\\nüî¥ BAD UPDATE (Simplified Instruction)\")\n",
    "print(\"-\" * 100)\n",
    "print(json.dumps(bad_results, indent=2, default=str))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"\\nüìà Analysis:\")\n",
    "print(\"-\" * 100)\n",
    "if good_results.get(\"evaluation_passed\") and not bad_results.get(\"evaluation_passed\"):\n",
    "    print(\"‚úÖ The enhanced instruction significantly outperformed the simplified version\")\n",
    "    print(\"   This demonstrates the importance of clear, detailed instructions for agent quality\")\n",
    "elif bad_results.get(\"evaluation_passed\") and not good_results.get(\"evaluation_passed\"):\n",
    "    print(\"‚ö†Ô∏è  Surprisingly, the simplified instruction performed better\")\n",
    "    print(\"   This may indicate the test queries don't fully capture instruction quality\")\n",
    "elif good_results.get(\"evaluation_passed\") and bad_results.get(\"evaluation_passed\"):\n",
    "    print(\"‚úÖ Both versions passed evaluation\")\n",
    "    print(\"   Review individual scores to see which performed better overall\")\n",
    "else:\n",
    "    print(\"‚ùå Both versions failed evaluation\")\n",
    "    print(\"   Consider adjusting the evaluation threshold or improving both instructions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c36b24",
   "metadata": {},
   "source": [
    "### View Version History with Changes\n",
    "\n",
    "Let's examine the version history to see exactly what changed between versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce55fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get version history to see all changes\n",
    "version_history = version_manager.get_version_history(agent_id=agent_data[\"id\"])\n",
    "\n",
    "print(f\"\\nüìú Complete Version History ({len(version_history)} versions)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Sort by version number descending to show most recent first\n",
    "sorted_history = sorted(version_history, key=lambda x: x['versionNumber'], reverse=True)\n",
    "\n",
    "for i, version in enumerate(sorted_history):\n",
    "    print(f\"\\n{'üîµ' if i == 0 else '‚ö™'} Version {version['versionNumber']}\")\n",
    "    print(f\"   ‚è∞ Timestamp: {version['timestamp']}\")\n",
    "    print(f\"   üë§ Changed by: {version['changedBy']}\")\n",
    "    print(f\"   üìù Change: {version['changeDescription']}\")\n",
    "    \n",
    "    snapshot = version['snapshot']\n",
    "    \n",
    "    # Show instruction preview\n",
    "    instruction_preview = snapshot.get('instruction', '')[:150].replace('\\n', ' ')\n",
    "    print(f\"   üìÑ Instruction: {instruction_preview}...\")\n",
    "    \n",
    "    # Show description\n",
    "    print(f\"   üìã Description: {snapshot.get('description', 'N/A')}\")\n",
    "    \n",
    "    # Compare with previous version if available\n",
    "    if i < len(sorted_history) - 1:\n",
    "        prev_version = sorted_history[i + 1]\n",
    "        prev_snapshot = prev_version['snapshot']\n",
    "        \n",
    "        # Check what changed\n",
    "        changes = []\n",
    "        if snapshot.get('instruction') != prev_snapshot.get('instruction'):\n",
    "            changes.append(\"instruction\")\n",
    "        if snapshot.get('description') != prev_snapshot.get('description'):\n",
    "            changes.append(\"description\")\n",
    "        if snapshot.get('samplePrompts') != prev_snapshot.get('samplePrompts'):\n",
    "            changes.append(\"samplePrompts\")\n",
    "        \n",
    "        if changes:\n",
    "            print(f\"   üîÑ Changes from v{prev_version['versionNumber']}: {', '.join(changes)}\")\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "\n",
    "# Show detailed comparison between two most recent versions\n",
    "if len(sorted_history) >= 2:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"DETAILED COMPARISON: Latest Two Versions\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    v1 = sorted_history[0]  # Most recent\n",
    "    v2 = sorted_history[1]  # Second most recent\n",
    "    \n",
    "    print(f\"\\nüìä Comparing Version {v2['versionNumber']} ‚Üí Version {v1['versionNumber']}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Compare instructions\n",
    "    print(\"\\nüìù INSTRUCTION CHANGES:\")\n",
    "    print(f\"\\n  Version {v2['versionNumber']}:\")\n",
    "    print(f\"  {v2['snapshot'].get('instruction', '')[:300]}...\")\n",
    "    print(f\"\\n  Version {v1['versionNumber']}:\")\n",
    "    print(f\"  {v1['snapshot'].get('instruction', '')[:300]}...\")\n",
    "    \n",
    "    # Compare descriptions\n",
    "    print(\"\\nüìã DESCRIPTION CHANGES:\")\n",
    "    print(f\"  Version {v2['versionNumber']}: {v2['snapshot'].get('description', 'N/A')}\")\n",
    "    print(f\"  Version {v1['versionNumber']}: {v1['snapshot'].get('description', 'N/A')}\")\n",
    "    \n",
    "    # Compare sample prompts if they exist\n",
    "    v1_prompts = v1['snapshot'].get('samplePrompts', [])\n",
    "    v2_prompts = v2['snapshot'].get('samplePrompts', [])\n",
    "    \n",
    "    if v1_prompts or v2_prompts:\n",
    "        print(\"\\nüí¨ SAMPLE PROMPTS CHANGES:\")\n",
    "        \n",
    "        # Find added prompts\n",
    "        added = [p for p in v1_prompts if p not in v2_prompts]\n",
    "        if added:\n",
    "            print(f\"\\n  ‚ûï Added in v{v1['versionNumber']}:\")\n",
    "            for p in added:\n",
    "                print(f\"     - {p}\")\n",
    "        \n",
    "        # Find removed prompts\n",
    "        removed = [p for p in v2_prompts if p not in v1_prompts]\n",
    "        if removed:\n",
    "            print(f\"\\n  ‚ûñ Removed from v{v2['versionNumber']}:\")\n",
    "            for p in removed:\n",
    "                print(f\"     - {p}\")\n",
    "        \n",
    "        if not added and not removed:\n",
    "            print(\"  ‚úÖ No changes\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3634b17",
   "metadata": {},
   "source": [
    "### Visual Score Comparison\n",
    "\n",
    "Let's visualize the performance difference between the two versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e73106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual comparison of scores\n",
    "def create_score_bar(score, max_score=5, width=50):\n",
    "    \"\"\"Create a text-based progress bar for scores\"\"\"\n",
    "    filled = int((score / max_score) * width)\n",
    "    bar = \"‚ñà\" * filled + \"‚ñë\" * (width - filled)\n",
    "    return f\"{bar} {score:.2f}/{max_score}\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"EVALUATION SCORES VISUALIZATION\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "good_avg = good_results.get('average_score', 0)\n",
    "bad_avg = bad_results.get('average_score', 0)\n",
    "\n",
    "print(f\"\\nüìä Version {good_results.get('version_created')} (Enhanced Instruction):\")\n",
    "print(f\"    {create_score_bar(good_avg)}\")\n",
    "print(f\"    Status: {'‚úÖ PASSED' if good_results.get('evaluation_passed') else '‚ùå FAILED'}\")\n",
    "\n",
    "print(f\"\\nüìä Version {bad_results.get('version_created')} (Simplified Instruction):\")\n",
    "print(f\"    {create_score_bar(bad_avg)}\")\n",
    "print(f\"    Status: {'‚úÖ PASSED' if bad_results.get('evaluation_passed') else '‚ùå FAILED'}\")\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = ((good_avg - bad_avg) / bad_avg * 100) if bad_avg > 0 else 0\n",
    "\n",
    "print(f\"\\nüìà Performance Difference:\")\n",
    "if improvement > 0:\n",
    "    print(f\"    ‚úÖ Enhanced version is {improvement:.1f}% better\")\n",
    "elif improvement < 0:\n",
    "    print(f\"    ‚ö†Ô∏è  Enhanced version is {abs(improvement):.1f}% worse\")\n",
    "else:\n",
    "    print(f\"    ‚ûñ No difference in performance\")\n",
    "\n",
    "# Show individual test scores\n",
    "print(\"\\n\" + \"-\" * 100)\n",
    "print(\"\\nüìã Individual Test Scores:\")\n",
    "\n",
    "good_scores = good_results.get('evaluation_scores', [])\n",
    "bad_scores = bad_results.get('evaluation_scores', [])\n",
    "\n",
    "max_tests = max(len(good_scores), len(bad_scores))\n",
    "test_pairs = list(zip(range(1, max_tests + 1), \n",
    "                      good_scores + [None] * (max_tests - len(good_scores)),\n",
    "                      bad_scores + [None] * (max_tests - len(bad_scores))))\n",
    "\n",
    "print(f\"\\n{'Test':<8} {'Enhanced (v' + str(good_results.get('version_created')) + ')':<25} {'Simplified (v' + str(bad_results.get('version_created')) + ')':<25} {'Difference':<15}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for test_num, good_score, bad_score in test_pairs:\n",
    "    good_str = f\"{good_score:.2f}\" if good_score is not None else \"N/A\"\n",
    "    bad_str = f\"{bad_score:.2f}\" if bad_score is not None else \"N/A\"\n",
    "    \n",
    "    if good_score is not None and bad_score is not None:\n",
    "        diff = good_score - bad_score\n",
    "        diff_str = f\"{'+' if diff > 0 else ''}{diff:.2f}\"\n",
    "        indicator = \"üü¢\" if diff > 0 else \"üî¥\" if diff < 0 else \"‚ö™\"\n",
    "    else:\n",
    "        diff_str = \"N/A\"\n",
    "        indicator = \"‚ö™\"\n",
    "    \n",
    "    print(f\"{indicator} #{test_num:<5} {good_str:<25} {bad_str:<25} {diff_str:<15}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "\n",
    "# Summary recommendation\n",
    "print(\"\\nüí° Recommendation:\")\n",
    "if good_results.get('evaluation_passed') and not bad_results.get('evaluation_passed'):\n",
    "    print(\"   ‚úÖ Deploy Version {} (Enhanced) - significantly better performance\".format(good_results.get('version_created')))\n",
    "    print(f\"   ‚èÆÔ∏è  Consider rolling back Version {bad_results.get('version_created')} (Simplified)\")\n",
    "elif bad_results.get('evaluation_passed') and not good_results.get('evaluation_passed'):\n",
    "    print(\"   ‚ö†Ô∏è  Version {} (Enhanced) underperformed - investigate before deployment\".format(good_results.get('version_created')))\n",
    "    print(f\"   ‚úÖ Keep Version {bad_results.get('version_created')} (Simplified) as baseline\")\n",
    "elif good_results.get('evaluation_passed') and bad_results.get('evaluation_passed'):\n",
    "    if good_avg > bad_avg:\n",
    "        print(\"   ‚úÖ Both passed, but Version {} (Enhanced) has better scores\".format(good_results.get('version_created')))\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Both passed, but simpler instruction may be sufficient\")\n",
    "else:\n",
    "    print(\"   ‚ùå Both versions failed evaluation - review instructions and test queries\")\n",
    "    print(\"   üîß Consider adjusting evaluation threshold or improving agent configuration\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcb2021",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### Version Control\n",
    "1. **Descriptive Change Logs**: Always provide meaningful descriptions for changes\n",
    "2. **Incremental Updates**: Make small, focused changes rather than large overhauls\n",
    "3. **Version Tagging**: Use semantic versioning concepts (major, minor, patch)\n",
    "4. **Regular Backups**: Export agent metadata periodically\n",
    "\n",
    "### Evaluation Strategy\n",
    "1. **Multiple Test Cases**: Use diverse test queries covering different scenarios\n",
    "2. **Baseline Comparison**: Compare new version scores against baseline\n",
    "3. **Threshold Tuning**: Adjust evaluation thresholds based on your quality requirements\n",
    "4. **Progressive Rollout**: Test with limited users before full deployment\n",
    "\n",
    "### CI/CD Automation\n",
    "1. **Automated Testing**: Run evaluations automatically on every update\n",
    "2. **Quality Gates**: Block deployment if evaluation scores drop\n",
    "3. **Monitoring**: Track evaluation scores over time\n",
    "4. **Alert System**: Notify team when scores fall below threshold\n",
    "\n",
    "### Rollback Procedures\n",
    "1. **Quick Rollback**: Have automated rollback for critical failures\n",
    "2. **Investigation**: Analyze why the new version failed before retry\n",
    "3. **Graceful Degradation**: Consider keeping previous version running during testing\n",
    "4. **User Communication**: Inform users of any service changes\n",
    "\n",
    "### Database Schema Evolution\n",
    "The version control system stores:\n",
    "```json\n",
    "{\n",
    "  \"id\": \"agent-local-id\",\n",
    "  \"azure_agent_id\": \"asst_xxx\",\n",
    "  \"currentVersion\": 5,\n",
    "  \"versions\": [\n",
    "    {\n",
    "      \"versionNumber\": 1,\n",
    "      \"timestamp\": \"2025-11-10T10:00:00Z\",\n",
    "      \"changeDescription\": \"Initial version\",\n",
    "      \"changedBy\": \"user@example.com\",\n",
    "      \"snapshot\": {\n",
    "        \"name\": \"...\",\n",
    "        \"instruction\": \"...\",\n",
    "        // ... complete configuration\n",
    "      }\n",
    "    },\n",
    "    // ... more versions\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Integration with External Systems\n",
    "- **GitHub Actions**: Trigger CI/CD on code changes\n",
    "- **Azure DevOps**: Integrate with deployment pipelines\n",
    "- **Monitoring Tools**: Send evaluation results to monitoring dashboards\n",
    "- **Notification Systems**: Alert on version changes and evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac631db",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up test thread if it exists\n",
    "if 'thread' in locals():\n",
    "    agent_manager.delete_thread(thread.id, silent=True)\n",
    "    print(\"‚úÖ Test thread cleaned up\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No cleanup needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264a6fd2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated a complete CI/CD workflow for Azure AI agents:\n",
    "\n",
    "### Key Accomplishments\n",
    "1. ‚úÖ **Automatic Versioning**: Every update creates a version snapshot\n",
    "2. ‚úÖ **Version History**: Complete audit trail with timestamps and descriptions\n",
    "3. ‚úÖ **Automated Evaluation**: Run evaluators after updates\n",
    "4. ‚úÖ **Version Comparison**: See differences between versions\n",
    "5. ‚úÖ **Rollback Support**: Restore previous versions safely\n",
    "6. ‚úÖ **Complete Pipeline**: End-to-end CI/CD with quality gates\n",
    "\n",
    "### Database Updates\n",
    "- `versions` array stores complete version history\n",
    "- `currentVersion` tracks the active version number\n",
    "- Each version includes timestamp, change description, and who made the change\n",
    "- Full configuration snapshot preserved for each version\n",
    "\n",
    "### Next Steps\n",
    "1. **Integrate with Git**: Version control your agent configurations\n",
    "2. **Set Up Monitoring**: Track evaluation metrics over time\n",
    "3. **Automate Deployment**: Trigger CI/CD from source control\n",
    "4. **Expand Test Coverage**: Add more comprehensive test scenarios\n",
    "5. **A/B Testing**: Compare multiple versions in production\n",
    "\n",
    "### Resources\n",
    "- [Azure AI Evaluation Documentation](https://learn.microsoft.com/azure/ai-studio/how-to/evaluate-sdk)\n",
    "- [Azure Cosmos DB Best Practices](https://learn.microsoft.com/azure/cosmos-db/best-practices)\n",
    "- [CI/CD for ML Models](https://learn.microsoft.com/azure/machine-learning/concept-model-management-and-deployment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
