{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7768891c",
   "metadata": {},
   "source": [
    "# Agent Version Performance Comparison - REAL DEMO\n",
    "\n",
    "## What This Demo Shows\n",
    "\n",
    "This notebook demonstrates **ACTUAL PERFORMANCE DIFFERENCES** between agent versions:\n",
    "\n",
    "1. **Create a Math Tutor Agent** - Simple, focused use case\n",
    "2. **Version 1**: Poor instruction ‚Üí Low evaluation scores\n",
    "3. **Version 2**: Excellent instruction ‚Üí High evaluation scores\n",
    "4. **Compare Results**: See real performance metrics side-by-side\n",
    "\n",
    "## Why Math Tutor?\n",
    "- Clear success criteria (correct answers + explanation quality)\n",
    "- Simple word problems show instruction quality impact\n",
    "- Shows stark contrast between poor and excellent instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5e216b",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup](#setup)\n",
    "2. [Create Math Tutor Agent - Version 1 (Poor)](#create-math-tutor-agent---version-1-poor)\n",
    "3. [Test Version 1 - Evaluate Performance](#test-version-1---evaluate-performance)\n",
    "4. [Update to Version 2 - Excellent Instruction](#update-to-version-2---excellent-instruction)\n",
    "5. [Test Version 2 - Evaluate Performance](#test-version-2---evaluate-performance)\n",
    "6. [Compare Both Versions - Side by Side](#compare-both-versions---side-by-side)\n",
    "7. [Cleanup](#cleanup)\n",
    "8. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d13674",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0760ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "new_path_entry = \"/opt/homebrew/bin\"  # Replace with the directory you want to add\n",
    "current_path = os.environ.get('PATH', '')\n",
    "\n",
    "if new_path_entry not in current_path.split(os.pathsep):\n",
    "    os.environ['PATH'] = new_path_entry + os.pathsep + current_path\n",
    "    print(f\"Updated PATH for this session: {os.environ['PATH']}\")\n",
    "else:\n",
    "    print(f\"PATH already contains {new_path_entry}: {current_path}\")\n",
    "\n",
    "# You can then verify with shutil.which again\n",
    "print(f\"Location of 'az' found by kernel now: {shutil.which('az')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26605f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.evaluation import (\n",
    "    IntentResolutionEvaluator,\n",
    "    AzureOpenAIModelConfiguration\n",
    ")\n",
    "\n",
    "# Add parent directory to path\n",
    "parent_dir = Path.cwd().parent / \"utils\"\n",
    "sys.path.insert(0, str(parent_dir))\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Load environment\n",
    "env_path = Path.cwd().parent / \".env\"\n",
    "load_dotenv(env_path)\n",
    "\n",
    "from agent_db import AgentDB\n",
    "from agent_utils import AgentManager\n",
    "from agent_version_manager import AgentVersionManager\n",
    "\n",
    "# Initialize clients\n",
    "endpoint = os.getenv(\"AZURE_AI_PROJECT_ENDPOINT\")\n",
    "project_client = AIProjectClient(endpoint=endpoint, credential=DefaultAzureCredential())\n",
    "\n",
    "# Initialize managers\n",
    "agent_manager = AgentManager(project_client=project_client)\n",
    "version_manager = AgentVersionManager(agent_manager=agent_manager)\n",
    "\n",
    "# Configure evaluator\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT_GPT_4o\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY_GPT_4o\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION_GPT_4o\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_MODEl_GPT_4o\"],\n",
    ")\n",
    "\n",
    "intent_evaluator = IntentResolutionEvaluator(model_config=model_config, threshold=3)\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dc7f92",
   "metadata": {},
   "source": [
    "## Create Math Tutor Agent - Version 1 (Poor)\n",
    "\n",
    "**Intentionally poor instruction** to show low performance baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c4ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent with POOR instruction - intentionally minimal and problematic\n",
    "v1_instruction = \"\"\"Answer briefly. Just give the final answer. Skip the work.\"\"\"\n",
    "\n",
    "# Create the agent using agent_manager to save metadata to DB\n",
    "agent = agent_manager.create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    name=\"Math Tutor Performance Demo2\",\n",
    "    instructions=v1_instruction,\n",
    "    description=\"Math tutor for version performance comparison\",\n",
    "    category=\"demo\",\n",
    "    status=\"active\"\n",
    ")\n",
    "\n",
    "# Get metadata from database to get the local ID\n",
    "agent_data = agent_manager.get_agent_metadata(azure_agent_id=agent.id)\n",
    "\n",
    "print(f\"‚úÖ Agent created and saved to database\")\n",
    "print(f\"üìù Azure Agent ID: {agent.id}\")\n",
    "print(f\"üìù Database ID: {agent_data['id']}\")\n",
    "print(f\"üìù Version 1 instruction: {v1_instruction}\")\n",
    "\n",
    "# Store IDs for later\n",
    "MATH_AGENT_ID = agent.id  # For Azure API calls\n",
    "MATH_AGENT_DB_ID = agent_data['id']  # For database operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82c22e1",
   "metadata": {},
   "source": [
    "## Test Version 1 - Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c58b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries - simpler prompts let instruction quality differentiate performance\n",
    "test_queries = [\n",
    "    \"A student scores 85, 92, and 78 on three tests. What score is needed on the fourth test to achieve an overall average of 90?\",\n",
    "    \"If a rectangle has a perimeter of 50 cm and its length is 5 cm more than twice its width, find the dimensions.\",\n",
    "    \"A water tank is being filled by two pipes. Pipe A can fill the tank alone in 4 hours, and Pipe B can fill it alone in 6 hours. If both pipes work together for 1.5 hours, then Pipe A is closed, how much longer will it take for Pipe B alone to finish filling the tank?\"\n",
    "]\n",
    "\n",
    "\n",
    "def evaluate_agent_version(agent_id, queries, version_name):\n",
    "    \"\"\"Evaluate agent performance on test queries\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATING: {version_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"\\nüìù Test {i}: {query}\")\n",
    "\n",
    "        # Create thread and run\n",
    "        thread = agent_manager.create_thread()\n",
    "        project_client.agents.messages.create(\n",
    "            thread_id=thread.id,\n",
    "            role=\"user\",\n",
    "            content=query\n",
    "        )\n",
    "\n",
    "        # Process the run and wait for completion\n",
    "        _ = project_client.agents.runs.create_and_process(\n",
    "            thread_id=thread.id,\n",
    "            agent_id=agent_id\n",
    "        )\n",
    "\n",
    "        # Get response\n",
    "        messages = list(project_client.agents.messages.list(\n",
    "            thread.id, order=\"asc\"))\n",
    "        response = messages[-1].content[0].text.value if messages else \"\"\n",
    "        print(f\"ü§ñ Response: {response[:200]}...\")\n",
    "\n",
    "        # Evaluate using the original query without enhancement\n",
    "        # This lets instruction quality drive the response detail level\n",
    "        intent_result = intent_evaluator(\n",
    "            query=query,\n",
    "            response=response\n",
    "        )\n",
    "\n",
    "        intent_score = intent_result.get('intent_resolution', 0)\n",
    "\n",
    "        scores.append(intent_score)\n",
    "\n",
    "        print(f\"üìä Intent Score: {intent_score:.1f}/5\")\n",
    "\n",
    "        # Cleanup\n",
    "        agent_manager.delete_thread(thread.id, silent=True)\n",
    "\n",
    "    overall_avg = sum(scores) / len(scores)\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìà OVERALL AVERAGE: {overall_avg:.2f}/5\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"version\": version_name,\n",
    "        \"scores\": scores,\n",
    "        \"average\": overall_avg,\n",
    "        \"passed\": overall_avg >= 3.0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2ed413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Version 1\n",
    "v1_results = evaluate_agent_version(\n",
    "    MATH_AGENT_ID, test_queries, \"Version 1 (Poor)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c987c3",
   "metadata": {},
   "source": [
    "## Update to Version 2 - Excellent Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1975eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update with EXCELLENT instruction\n",
    "v2_instruction = \"\"\"You are an Expert Math Tutor specializing in clear, accurate mathematical instruction.\n",
    "\n",
    "## Your Approach\n",
    "1. **Understand**: Carefully read and identify what the problem is asking\n",
    "2. **Plan**: Determine which mathematical concepts/formulas to apply\n",
    "3. **Solve**: Show each step with clear explanations\n",
    "4. **Verify**: Check your answer makes sense\n",
    "5. **Present**: Provide the final answer clearly\n",
    "\n",
    "## Standards\n",
    "- Use proper mathematical notation\n",
    "- Show all intermediate steps\n",
    "- Explain reasoning at each step\n",
    "- Include units where applicable\n",
    "- Round to 2 decimal places unless specified otherwise\n",
    "\n",
    "## Example Format\n",
    "**Problem**: [Restate the problem]\n",
    "**Solution**:\n",
    "Step 1: [First step with explanation]\n",
    "Step 2: [Second step with explanation]\n",
    "...\n",
    "**Final Answer**: [Clear, concise answer with units]\n",
    "\"\"\"\n",
    "\n",
    "# Update agent WITH VERSIONING - saves to database\n",
    "version_manager.update_agent_with_versioning(\n",
    "    agent_id=MATH_AGENT_DB_ID,\n",
    "    updates={\"instruction\": v2_instruction},  # Note: singular 'instruction' - gets converted to plural for Azure API\n",
    "    change_description=\"Expert-level instruction with comprehensive format and standards\",\n",
    "    changed_by=\"demo@example.com\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Agent updated to Version 2 (saved to database)\")\n",
    "print(f\"üìù New instruction:\\n{v2_instruction[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3f80c5",
   "metadata": {},
   "source": [
    "## Test Version 2 - Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59dbd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Version 2\n",
    "v2_results = evaluate_agent_version(MATH_AGENT_ID, test_queries, \"Version 2 (Excellent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9406de15",
   "metadata": {},
   "source": [
    "## View Version History from Database\n",
    "\n",
    "Let's see what was saved to the database with our version manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6ea18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get version history from database\n",
    "version_history = version_manager.get_version_history(agent_id=MATH_AGENT_DB_ID)\n",
    "current_version = version_manager.get_current_version_number(agent_id=MATH_AGENT_DB_ID)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"VERSION HISTORY FROM DATABASE (Current Version: {current_version})\")\n",
    "print(\"=\" * 100 + \"\\n\")\n",
    "\n",
    "for version in sorted(version_history, key=lambda x: x['versionNumber'], reverse=True):\n",
    "    print(f\"\\nüîñ Version {version['versionNumber']}\")\n",
    "    print(f\"   ‚è∞ Timestamp: {version['timestamp']}\")\n",
    "    print(f\"   üë§ Changed by: {version['changedBy']}\")\n",
    "    print(f\"   üìù Description: {version['changeDescription']}\")\n",
    "    \n",
    "    snapshot = version['snapshot']\n",
    "    instruction = snapshot.get('instruction', '')[:100].replace('\\n', ' ')\n",
    "    print(f\"   üìÑ Instruction: {instruction}...\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "print(\"\\n‚úÖ Version history retrieved from Cosmos DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f98a9f3",
   "metadata": {},
   "source": [
    "## Compare Both Versions - Side by Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f734850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"PERFORMANCE COMPARISON - BOTH VERSIONS\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Version\": \"V1 (Poor)\",\n",
    "        \"Average Score\": f\"{v1_results['average']:.2f}\",\n",
    "        \"Test 1\": f\"{v1_results['scores'][0]:.2f}\",\n",
    "        \"Test 2\": f\"{v1_results['scores'][1]:.2f}\",\n",
    "        \"Test 3\": f\"{v1_results['scores'][2]:.2f}\",\n",
    "        \"Status\": \"‚úÖ PASS\" if v1_results['passed'] else \"‚ùå FAIL\"\n",
    "    },\n",
    "    {\n",
    "        \"Version\": \"V2 (Excellent)\",\n",
    "        \"Average Score\": f\"{v2_results['average']:.2f}\",\n",
    "        \"Test 1\": f\"{v2_results['scores'][0]:.2f}\",\n",
    "        \"Test 2\": f\"{v2_results['scores'][1]:.2f}\",\n",
    "        \"Test 3\": f\"{v2_results['scores'][2]:.2f}\",\n",
    "        \"Status\": \"‚úÖ PASS\" if v2_results['passed'] else \"‚ùå FAIL\"\n",
    "    }\n",
    "])\n",
    "\n",
    "display(comparison_df)\n",
    "\n",
    "# Visual comparison\n",
    "def create_bar(score, max_score=5, width=30):\n",
    "    filled = int((score / max_score) * width)\n",
    "    return \"‚ñà\" * filled + \"‚ñë\" * (width - filled)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"VISUAL SCORE COMPARISON\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "print(f\"V1 (Poor):      {create_bar(v1_results['average'])} {v1_results['average']:.2f}/5.0\")\n",
    "print(f\"V2 (Excellent): {create_bar(v2_results['average'])} {v2_results['average']:.2f}/5.0\")\n",
    "\n",
    "# Calculate improvement\n",
    "v1_to_v2 = ((v2_results['average'] - v1_results['average']) / v1_results['average'] * 100) if v1_results['average'] > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"PERFORMANCE IMPROVEMENT\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "print(f\"üìà V1 (Poor) ‚Üí V2 (Excellent): {v1_to_v2:+.1f}% improvement\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"KEY TAKEAWAYS\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "if v2_results['average'] > v1_results['average']:\n",
    "    print(\"‚úÖ Clear contrast: Better instructions = Better performance\")\n",
    "    print(\"‚úÖ Version 2 (Excellent) shows significantly better results\")\n",
    "    print(f\"‚úÖ {v1_to_v2:+.1f}% improvement from V1 to V2\")\n",
    "    print(\"\\nüí° Recommendation: Deploy Version 2 to production\")\n",
    "    print(\"üí° Value: Version control enables rollback if needed\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Results may vary - consider running more tests\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "# Show database info\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"DATABASE VERSION INFO\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "print(f\"üì¶ Database ID: {MATH_AGENT_DB_ID}\")\n",
    "print(f\"üîó Azure Agent ID: {MATH_AGENT_ID}\")\n",
    "print(f\"üìä Current Version in DB: {version_manager.get_current_version_number(MATH_AGENT_DB_ID)}\")\n",
    "print(f\"üìú Total Versions Saved: {len(version_manager.get_version_history(MATH_AGENT_DB_ID))}\")\n",
    "print(\"\\n‚úÖ All versions tracked in Cosmos DB with full audit trail\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556f4469",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0114fe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Delete the test agent\n",
    "# Uncomment to delete:\n",
    "# agent_manager.delete_agent(agent_id=MATH_AGENT_DB_ID)\n",
    "# print(f\"‚úÖ Agent deleted from Azure and database\")\n",
    "\n",
    "print(\"‚ÑπÔ∏è  Agent cleanup commented out. Uncomment to delete test agent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e5275",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated **REAL performance differences** between agent versions:\n",
    "\n",
    "### What We Tested\n",
    "- **Math Tutor Agent** with 2 different instruction qualities (binary comparison)\n",
    "- **3 Math Word Problems**: Percentage calculation, algebra, speed-distance-time\n",
    "- **Intent Resolution Evaluator**: Objective scoring (0-5 scale)\n",
    "\n",
    "### Results Show\n",
    "1. **Poor instructions** (V1) ‚Üí Low scores (~1-2/5) - deliberately unhelpful\n",
    "2. **Excellent instructions** (V2) ‚Üí High scores (~5/5) - comprehensive expert guidance\n",
    "\n",
    "### Why This Matters\n",
    "- **Version control** lets you track what changed with full audit trail\n",
    "- **Automated evaluation** provides objective performance metrics\n",
    "- **Rollback capability** protects against regressions (revert to previous version if needed)\n",
    "- **Clear evidence** for deployment decisions\n",
    "\n",
    "### Key Insight\n",
    "Binary comparison (Poor vs Excellent) demonstrates the value better than gradual progression. With highly capable models like GPT-4o, even minimal instructions produce good results - but version control remains critical for **audit trails, rollback capability, and change tracking** in production environments.\n",
    "\n",
    "This is exactly what you need for **production agent management**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
