{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82085eee",
   "metadata": {},
   "source": [
    "# Decorator-Based Tracing for Azure OpenAI\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates **non-intrusive tracing** using Python decorators. Instead of wrapping every API call manually, decorators allow you to add comprehensive tracing to existing functions without modifying their core logic.\n",
    "\n",
    "### What are Decorators?\n",
    "\n",
    "Python decorators are a powerful way to modify function behavior without changing the function's code. They wrap a function with additional functionality (like tracing) while keeping the original logic intact.\n",
    "\n",
    "**Traditional Approach (Intrusive):**\n",
    "```python\n",
    "def my_function():\n",
    "    with tracer.start_span(\"my_function\"):  # Manual span management\n",
    "        # Original logic here\n",
    "        result = do_work()\n",
    "        return result\n",
    "```\n",
    "\n",
    "**Decorator Approach (Non-Intrusive):**\n",
    "```python\n",
    "@trace_function(span_name=\"my_function\")\n",
    "def my_function():\n",
    "    # Original logic here - no tracing code!\n",
    "    result = do_work()\n",
    "    return result\n",
    "```\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "**Clean Code:**\n",
    "- ‚úÖ Business logic separated from observability concerns\n",
    "- ‚úÖ Functions remain readable and maintainable\n",
    "- ‚úÖ Easy to add/remove tracing without code changes\n",
    "\n",
    "**Automatic Instrumentation:**\n",
    "- ‚úÖ Captures function arguments automatically\n",
    "- ‚úÖ Records return values and exceptions\n",
    "- ‚úÖ Measures execution time (latency)\n",
    "- ‚úÖ Handles both sync and async functions\n",
    "\n",
    "**Composability:**\n",
    "- ‚úÖ Decorated functions can call other decorated functions\n",
    "- ‚úÖ Automatically creates parent-child span hierarchies\n",
    "- ‚úÖ Trace complete workflows end-to-end\n",
    "\n",
    "**Flexibility:**\n",
    "- ‚úÖ Control what gets captured (args, results, messages)\n",
    "- ‚úÖ Custom attributes for business context\n",
    "- ‚úÖ Privacy controls for sensitive data\n",
    "\n",
    "### Tracing Workflow\n",
    "\n",
    "```\n",
    "1. Setup Tracing (SQLite + OpenTelemetry)\n",
    "   ‚Üì\n",
    "2. Decorate Functions (@trace_function, @trace_openai_call)\n",
    "   ‚Üì\n",
    "3. Call Functions Normally (no code changes!)\n",
    "   ‚Üì\n",
    "4. Traces Captured Automatically\n",
    "   ‚Üì\n",
    "5. Query SQLite Database\n",
    "   ‚Üì\n",
    "6. Analyze Performance & Debug Issues\n",
    "```\n",
    "\n",
    "### Available Decorators\n",
    "\n",
    "**`@trace_openai_call`**: Specialized for Azure OpenAI API calls\n",
    "- Captures model, temperature, max_tokens\n",
    "- Records full prompts and completions (optional)\n",
    "- Tracks token usage (prompt, completion, total)\n",
    "- Measures API latency\n",
    "- Handles streaming responses\n",
    "- Traces function calling (tools, arguments, results)\n",
    "\n",
    "**`@trace_function`**: Generic decorator for any function\n",
    "- Captures function arguments\n",
    "- Records return values\n",
    "- Measures execution time\n",
    "- Handles exceptions\n",
    "- Works with sync and async functions\n",
    "- Custom attributes support\n",
    "\n",
    "### What Gets Traced\n",
    "\n",
    "**Function Metadata:**\n",
    "- Function name and module\n",
    "- Execution duration (milliseconds)\n",
    "- Success/failure status\n",
    "- Exception details (if errors)\n",
    "\n",
    "**Function Arguments:**\n",
    "- All positional arguments\n",
    "- All keyword arguments\n",
    "- Serialized as strings (JSON for complex types)\n",
    "\n",
    "**Return Values:**\n",
    "- Complete return data\n",
    "- Serialized for storage\n",
    "\n",
    "**OpenAI-Specific (with `@trace_openai_call`):**\n",
    "- Model/deployment name\n",
    "- Request parameters (temperature, max_tokens, etc.)\n",
    "- Full message history (system, user, assistant)\n",
    "- Token usage breakdown\n",
    "- Tool/function definitions\n",
    "- Function call arguments and results\n",
    "\n",
    "**Hierarchical Context:**\n",
    "- Parent-child relationships\n",
    "- Trace IDs (group related spans)\n",
    "- Span IDs (unique per operation)\n",
    "- Timestamps (start, end)\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "**Development & Testing:**\n",
    "- Debug function call sequences\n",
    "- Validate function arguments\n",
    "- Test error handling paths\n",
    "- Profile performance locally\n",
    "\n",
    "**Performance Analysis:**\n",
    "- Identify slow functions\n",
    "- Find bottlenecks in pipelines\n",
    "- Compare different implementations\n",
    "- Optimize critical paths\n",
    "\n",
    "**Agent Workflows:**\n",
    "- Trace complete agent orchestration\n",
    "- Debug function calling behavior\n",
    "- Validate tool argument passing\n",
    "- Analyze multi-step reasoning\n",
    "\n",
    "**Cost Tracking:**\n",
    "- Monitor token usage per function\n",
    "- Calculate API costs\n",
    "- Identify expensive operations\n",
    "- Optimize prompt efficiency\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Python 3.8+\n",
    "- Azure OpenAI endpoint and API key\n",
    "- OpenTelemetry packages (installed automatically)\n",
    "- SQLite (built into Python)\n",
    "- Decorator utilities (`tracing_decorators.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6efebae",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Part 1: Environment Setup](#part-1-environment-setup)\n",
    "   - 1.1: Install Dependencies\n",
    "   - 1.2: Configure Environment & Imports\n",
    "   - 1.3: Initialize Tracing\n",
    "   - 1.4: Initialize Azure OpenAI Clients\n",
    "2. [Part 2: Basic Function Decoration](#part-2-basic-function-decoration)\n",
    "3. [Part 3: Async Function Tracing](#part-3-async-function-tracing)\n",
    "4. [Part 4: Generic Function Tracing](#part-4-generic-function-tracing)\n",
    "5. [Part 5: Composing Traced Functions](#part-5-composing-traced-functions)\n",
    "6. [Part 6: Privacy Controls](#part-6-privacy-controls)\n",
    "7. [Part 7: Function Calling & Agent Orchestration](#part-7-function-calling--agent-orchestration)\n",
    "8. [Part 8: Query and Analyze Traces](#part-8-query-and-analyze-traces)\n",
    "9. [Summary and Best Practices](#summary-and-best-practices)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5fa105",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "### 1.1: Install Dependencies\n",
    "\n",
    "Install required packages for decorator-based tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb0a833",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU openai python-dotenv  opentelemetry-sdk opentelemetry-api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfb1dea",
   "metadata": {},
   "source": [
    "### 1.2: Configure Environment & Imports\n",
    "\n",
    "Load environment variables and import decorator utilities.\n",
    "\n",
    "**Required Environment Variables:**\n",
    "- `AZURE_OPENAI_ENDPOINT_GPT_4o`: Your Azure OpenAI endpoint\n",
    "- `AZURE_OPENAI_API_KEY_GPT_4o`: Your Azure OpenAI API key\n",
    "\n",
    "**Decorator Utilities:**\n",
    "- `setup_tracing()`: Initialize OpenTelemetry with SQLite\n",
    "- `trace_function()`: Generic function decorator\n",
    "- `trace_openai_call()`: OpenAI-specific decorator\n",
    "- `flush_traces()`: Force export pending spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4537b837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI, AsyncAzureOpenAI\n",
    "\n",
    "# Suppress verbose logging\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"openai\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"opentelemetry\").setLevel(logging.WARNING)\n",
    "\n",
    "# Import tracing decorators\n",
    "from tracing_decorators import (\n",
    "    setup_tracing,\n",
    "    trace_function,\n",
    "    trace_openai_call,\n",
    "    flush_traces\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Verify environment variables\n",
    "required_vars = [\"AZURE_OPENAI_ENDPOINT_GPT_4o\", \"AZURE_OPENAI_API_KEY_GPT_4o\"]\n",
    "missing = [var for var in required_vars if not os.getenv(var)]\n",
    "if missing:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Missing environment variables: {missing}\")\n",
    "else:\n",
    "    print(\"‚úÖ Environment variables loaded successfully\")\n",
    "    print(\"‚úÖ Decorator utilities imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda7475f",
   "metadata": {},
   "source": [
    "### 1.3: Initialize Tracing\n",
    "\n",
    "Configure tracing with SQLite storage and service identification.\n",
    "\n",
    "**Service Name Benefits:**\n",
    "- Multiple applications can share the same database\n",
    "- Filter traces by service for analysis\n",
    "- Essential for microservices architectures\n",
    "- Stored in both database column and span attributes\n",
    "\n",
    "**Configuration:**\n",
    "- `service_name`: Identifies this application\n",
    "- `enable_console`: Print traces to console (default: False)\n",
    "- `enable_sqlite`: Store in SQLite database (default: True)\n",
    "- `sqlite_db_path`: Database file path\n",
    "- `use_batch_processor`: Batch spans for performance (default: True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2e0ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tracing once\n",
    "tracer = setup_tracing(\n",
    "    service_name=\"decorator-tracing-demo\",\n",
    "    enable_console=False,\n",
    "    enable_sqlite=True,\n",
    "    sqlite_db_path=\"decorator_traces.db\",\n",
    "    use_batch_processor=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Tracing initialized successfully\")\n",
    "print(\"üìÅ Traces will be stored in: decorator_traces.db\")\n",
    "print(f\"üè¢ Service name: decorator-tracing-demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb1834a",
   "metadata": {},
   "source": [
    "### 1.4: Initialize Azure OpenAI Clients\n",
    "\n",
    "Create synchronous and asynchronous clients for API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03379b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT_GPT_4o\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY_GPT_4o\"),\n",
    "    api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "async_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT_GPT_4o\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY_GPT_4o\"),\n",
    "    api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "deployment_name = \"gpt-4o-lei\"\n",
    "\n",
    "print(\"‚úÖ Azure OpenAI clients initialized\")\n",
    "print(f\"   Deployment: {deployment_name}\")\n",
    "print(f\"   Endpoint: {os.getenv('AZURE_OPENAI_ENDPOINT_GPT_4o')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae094ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Basic Function Decoration\n",
    "\n",
    "Add tracing to existing Azure OpenAI functions without modifying their core logic.\n",
    "\n",
    "### The Decorator Pattern\n",
    "\n",
    "**Before (No Tracing):**\n",
    "```python\n",
    "def ask_gpt(client, messages, model):\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "```\n",
    "\n",
    "**After (With Tracing):**\n",
    "```python\n",
    "@trace_openai_call(operation=\"chat.completions\")  # Just add this line!\n",
    "def ask_gpt(client, messages, model):\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "```\n",
    "\n",
    "The function logic remains **identical** - tracing is added automatically!\n",
    "\n",
    "### What Gets Captured\n",
    "\n",
    "- ‚úÖ Function name and arguments\n",
    "- ‚úÖ Model and request parameters\n",
    "- ‚úÖ Full message history\n",
    "- ‚úÖ Token usage (prompt, completion, total)\n",
    "- ‚úÖ Response content\n",
    "- ‚úÖ Execution latency\n",
    "- ‚úÖ Errors and exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd8ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your original function - just add the decorator!\n",
    "@trace_openai_call(operation=\"chat.completions\")\n",
    "def ask_gpt(client, messages, model, temperature=0.7, max_tokens=200):\n",
    "    \"\"\"Simple chat completion function\"\"\"\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "\n",
    "# Use it normally - tracing happens automatically!\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is Azure OpenAI?\"}\n",
    "]\n",
    "\n",
    "response = ask_gpt(\n",
    "    client=client,\n",
    "    messages=messages,\n",
    "    model=deployment_name,\n",
    "    temperature=0.7,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(f\"üìä Performance Metrics:\")\n",
    "print(f\"   Tokens: {response.usage.total_tokens} (prompt: {response.usage.prompt_tokens}, completion: {response.usage.completion_tokens})\")\n",
    "print(f\"   Finish reason: {response.choices[0].finish_reason}\")\n",
    "\n",
    "print(f\"\\nüí¨ Response:\\n{response.choices[0].message.content}\")\n",
    "\n",
    "print(\"\\n‚úÖ Trace automatically captured and stored in database\")\n",
    "print(\"   No manual span management required!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ed8e98",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Async Function Tracing\n",
    "\n",
    "The `@trace_openai_call` decorator automatically detects and handles async functions.\n",
    "\n",
    "### Automatic Async Detection\n",
    "\n",
    "The decorator uses `inspect.iscoroutinefunction()` to determine if a function is async, then:\n",
    "- Returns an async wrapper for async functions\n",
    "- Returns a sync wrapper for sync functions\n",
    "\n",
    "**No configuration needed** - it just works!\n",
    "\n",
    "### Benefits of Async Tracing\n",
    "\n",
    "- ‚úÖ Non-blocking I/O for better concurrency\n",
    "- ‚úÖ Higher throughput for multiple API calls\n",
    "- ‚úÖ Same tracing features as sync version\n",
    "- ‚úÖ Identical decorator syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b47725",
   "metadata": {},
   "outputs": [],
   "source": [
    "@trace_openai_call(operation=\"chat.completions\")\n",
    "async def ask_gpt_async(client, messages, model, temperature=0.7, max_tokens=200):\n",
    "    \"\"\"Async chat completion function\"\"\"\n",
    "    return await client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "\n",
    "# Use it normally with await\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain distributed tracing briefly.\"}\n",
    "]\n",
    "\n",
    "response = await ask_gpt_async(\n",
    "    client=async_client,\n",
    "    messages=messages,\n",
    "    model=deployment_name,\n",
    "    temperature=0.7,\n",
    "    max_tokens=250\n",
    ")\n",
    "\n",
    "print(f\"üìä Performance Metrics:\")\n",
    "print(f\"   Tokens: {response.usage.total_tokens}\")\n",
    "print(f\"   Model: {deployment_name}\")\n",
    "\n",
    "print(f\"\\nüí¨ Response:\\n{response.choices[0].message.content}\")\n",
    "\n",
    "print(\"\\n‚úÖ Async trace automatically captured\")\n",
    "print(\"   Decorator detected async function and handled it correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d5f0d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Generic Function Tracing\n",
    "\n",
    "Use `@trace_function` for any Python function, not just OpenAI API calls.\n",
    "\n",
    "### Universal Function Decorator\n",
    "\n",
    "The `@trace_function` decorator works with:\n",
    "- Data processing functions\n",
    "- Business logic\n",
    "- Database operations\n",
    "- File I/O operations\n",
    "- Mathematical computations\n",
    "- Any Python function!\n",
    "\n",
    "### Custom Attributes\n",
    "\n",
    "Add business context to spans with custom attributes:\n",
    "```python\n",
    "@trace_function(span_name=\"process_order\", attributes={\"version\": \"2.0\", \"environment\": \"prod\"})\n",
    "def process_order(order_id):\n",
    "    # Your logic here\n",
    "    pass\n",
    "```\n",
    "\n",
    "### What Gets Captured\n",
    "\n",
    "- ‚úÖ Function arguments (all types)\n",
    "- ‚úÖ Return values\n",
    "- ‚úÖ Execution duration\n",
    "- ‚úÖ Exceptions with stack traces\n",
    "- ‚úÖ Custom attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e5697",
   "metadata": {},
   "outputs": [],
   "source": [
    "@trace_function(span_name=\"data_processing\", attributes={\"version\": \"1.0\"})\n",
    "def process_data(text: str, convert_upper: bool = True):\n",
    "    \"\"\"Example data processing function\"\"\"\n",
    "    import time\n",
    "    time.sleep(0.1)  # Simulate work\n",
    "    \n",
    "    if convert_upper:\n",
    "        return text.upper()\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "@trace_function(span_name=\"text_analysis\")\n",
    "def analyze_text(text: str):\n",
    "    \"\"\"Analyze text properties\"\"\"\n",
    "    return {\n",
    "        \"length\": len(text),\n",
    "        \"words\": len(text.split()),\n",
    "        \"has_numbers\": any(c.isdigit() for c in text)\n",
    "    }\n",
    "\n",
    "\n",
    "# Use them normally - tracing is automatic\n",
    "result1 = process_data(\"hello world\", convert_upper=True)\n",
    "print(f\"üìù Processed: {result1}\")\n",
    "\n",
    "result2 = analyze_text(\"Azure OpenAI has 3 main features\")\n",
    "print(f\"üìä Analysis: {result2}\")\n",
    "\n",
    "print(\"\\n‚úÖ Both function calls traced automatically\")\n",
    "print(\"   Check the database to see:\")\n",
    "print(\"   - Function arguments (text, convert_upper)\")\n",
    "print(\"   - Return values (results)\")\n",
    "print(\"   - Execution duration\")\n",
    "print(\"   - Custom attributes (version: 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b9412f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Composing Traced Functions\n",
    "\n",
    "Create pipelines of traced functions that automatically form parent-child span hierarchies.\n",
    "\n",
    "### Span Hierarchy\n",
    "\n",
    "When decorated functions call other decorated functions, OpenTelemetry automatically:\n",
    "- Creates parent-child relationships\n",
    "- Links spans with the same trace_id\n",
    "- Tracks the complete call stack\n",
    "- Preserves context across async boundaries\n",
    "\n",
    "**Example Hierarchy:**\n",
    "```\n",
    "complete_pipeline (root span)\n",
    "  ‚îú‚îÄ‚îÄ validate_input (child span)\n",
    "  ‚îú‚îÄ‚îÄ prepare_messages (child span)\n",
    "  ‚îî‚îÄ‚îÄ call_openai (child span)\n",
    "```\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- ‚úÖ **Complete visibility**: See entire workflow in one trace\n",
    "- ‚úÖ **Debugging**: Identify which step failed\n",
    "- ‚úÖ **Performance**: Find bottlenecks in the pipeline\n",
    "- ‚úÖ **Context**: Understand the call sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74094767",
   "metadata": {},
   "outputs": [],
   "source": [
    "@trace_function(span_name=\"validate_input\")\n",
    "def validate_input(text: str) -> bool:\n",
    "    \"\"\"Validate input text\"\"\"\n",
    "    return len(text) > 0 and len(text) < 1000\n",
    "\n",
    "\n",
    "@trace_function(span_name=\"prepare_messages\")\n",
    "def prepare_messages(user_input: str, system_prompt: str = None):\n",
    "    \"\"\"Prepare messages for OpenAI\"\"\"\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    return messages\n",
    "\n",
    "\n",
    "@trace_openai_call(operation=\"chat.completions\")\n",
    "def call_openai(client, messages, model, temperature=0.7):\n",
    "    \"\"\"Call OpenAI API\"\"\"\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=150\n",
    "    )\n",
    "\n",
    "\n",
    "@trace_function(span_name=\"complete_pipeline\")\n",
    "def process_user_query(user_input: str, client, model: str):\n",
    "    \"\"\"Complete pipeline with validation, preparation, and API call\"\"\"\n",
    "\n",
    "    # Each of these calls creates its own span\n",
    "    if not validate_input(user_input):\n",
    "        raise ValueError(\"Invalid input\")\n",
    "\n",
    "    messages = prepare_messages(\n",
    "        user_input=user_input,\n",
    "        system_prompt=\"You are a concise AI assistant.\"\n",
    "    )\n",
    "\n",
    "    response = call_openai(\n",
    "        client=client,\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Run the pipeline - you'll see a hierarchy of spans!\n",
    "result = process_user_query(\n",
    "    user_input=\"What are the benefits of observability?\",\n",
    "    client=client,\n",
    "    model=deployment_name\n",
    ")\n",
    "\n",
    "print(f\"üí¨ Final Result:\\n{result}\")\n",
    "\n",
    "print(\"\\n‚úÖ Complete pipeline traced with span hierarchy:\")\n",
    "print(\"   üìä complete_pipeline (root)\")\n",
    "print(\"      ‚îú‚îÄ validate_input\")\n",
    "print(\"      ‚îú‚îÄ prepare_messages\")\n",
    "print(\"      ‚îî‚îÄ call_openai\")\n",
    "print(\"\\n   Query the database to see parent-child relationships!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b077b4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Privacy Controls\n",
    "\n",
    "Control what data gets captured in traces for compliance and privacy.\n",
    "\n",
    "### Capture Control Options\n",
    "\n",
    "**`@trace_openai_call` Parameters:**\n",
    "- `capture_messages`: Capture full prompt messages (default: True)\n",
    "- `capture_response`: Capture completion content (default: True)\n",
    "- `operation`: Operation name for span\n",
    "\n",
    "**Use Cases:**\n",
    "- **Development**: Enable all capture for debugging\n",
    "- **Production**: Disable sensitive data capture\n",
    "- **Compliance**: Meet GDPR/HIPAA requirements\n",
    "- **Cost Optimization**: Reduce storage with selective capture\n",
    "\n",
    "### Example Configurations\n",
    "\n",
    "```python\n",
    "# Full capture (development)\n",
    "@trace_openai_call(operation=\"chat\", capture_messages=True, capture_response=True)\n",
    "\n",
    "# No message capture (protect PII)\n",
    "@trace_openai_call(operation=\"chat\", capture_messages=False, capture_response=True)\n",
    "\n",
    "# Metadata only (production)\n",
    "@trace_openai_call(operation=\"chat\", capture_messages=False, capture_response=False)\n",
    "```\n",
    "\n",
    "**Still captured when disabled:**\n",
    "- Token counts\n",
    "- Latency metrics\n",
    "- Model parameters\n",
    "- Success/failure status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29240c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't capture messages (for privacy)\n",
    "@trace_openai_call(operation=\"chat.completions\", capture_messages=False)\n",
    "def private_chat(client, messages, model):\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=100\n",
    "    )\n",
    "\n",
    "\n",
    "# Don't capture response content\n",
    "@trace_openai_call(operation=\"chat.completions\", capture_response=False)\n",
    "def chat_no_response_capture(client, messages, model):\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=100\n",
    "    )\n",
    "\n",
    "\n",
    "# Test both configurations\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is Python?\"}\n",
    "]\n",
    "\n",
    "response1 = private_chat(client, messages, deployment_name)\n",
    "print(f\"üîí Response 1 (messages NOT captured for privacy):\")\n",
    "print(f\"   {response1.choices[0].message.content[:50]}...\")\n",
    "\n",
    "response2 = chat_no_response_capture(client, messages, deployment_name)\n",
    "print(f\"\\nüîí Response 2 (response NOT captured for privacy):\")\n",
    "print(f\"   {response2.choices[0].message.content[:50]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ Privacy controls applied:\")\n",
    "print(\"   - Trace 1: No message content stored (protect user input)\")\n",
    "print(\"   - Trace 2: No response content stored (protect AI output)\")\n",
    "print(\"   - Both still capture: tokens, latency, model, status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b7cdcf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Function Calling & Agent Orchestration\n",
    "\n",
    "Trace complete agent workflows including function calling with tools.\n",
    "\n",
    "### Function Calling Flow\n",
    "\n",
    "```\n",
    "1. User Query ‚Üí Agent\n",
    "   ‚Üì\n",
    "2. Agent Requests Tool Call(s)\n",
    "   ‚Üì\n",
    "3. Execute Local Functions\n",
    "   ‚Üì\n",
    "4. Return Results to Agent\n",
    "   ‚Üì\n",
    "5. Agent Generates Final Response\n",
    "```\n",
    "\n",
    "Each step is automatically traced with the decorator pattern!\n",
    "\n",
    "### What Gets Traced\n",
    "\n",
    "**Tool Definitions:**\n",
    "- Function names and descriptions\n",
    "- Parameter schemas\n",
    "- Required vs optional parameters\n",
    "\n",
    "**Function Execution:**\n",
    "- Function name and arguments\n",
    "- Return values\n",
    "- Execution duration\n",
    "- Success/failure status\n",
    "\n",
    "**Agent Interactions:**\n",
    "- Initial request with tools\n",
    "- Tool call requests from agent\n",
    "- Function results submission\n",
    "- Final response generation\n",
    "\n",
    "### Hierarchical Trace\n",
    "\n",
    "The complete workflow creates a trace hierarchy:\n",
    "```\n",
    "gpt_function_calling_orchestration (root)\n",
    "  ‚îú‚îÄ chat.completions.with_tools\n",
    "  ‚îú‚îÄ execute_function_call (get_weather)\n",
    "  ‚îú‚îÄ execute_function_call (get_stock_price)\n",
    "  ‚îî‚îÄ chat.completions.with_function_result\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd236dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define tools/functions\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get the current weather for a location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"The temperature unit\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_stock_price\",\n",
    "            \"description\": \"Get the current stock price for a company\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"symbol\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The stock ticker symbol, e.g. MSFT for Microsoft\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"symbol\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Simulated function implementations\n",
    "\n",
    "\n",
    "def get_weather(location: str, unit: str = \"fahrenheit\") -> str:\n",
    "    \"\"\"Simulate getting weather data\"\"\"\n",
    "    return json.dumps({\n",
    "        \"location\": location,\n",
    "        \"temperature\": 72 if unit == \"fahrenheit\" else 22,\n",
    "        \"unit\": unit,\n",
    "        \"condition\": \"sunny\"\n",
    "    })\n",
    "\n",
    "\n",
    "def get_stock_price(symbol: str) -> str:\n",
    "    \"\"\"Simulate getting stock price\"\"\"\n",
    "    prices = {\"MSFT\": 380.50, \"AAPL\": 195.30, \"GOOGL\": 142.80}\n",
    "    return json.dumps({\n",
    "        \"symbol\": symbol,\n",
    "        \"price\": prices.get(symbol.upper(), 100.00),\n",
    "        \"currency\": \"USD\"\n",
    "    })\n",
    "\n",
    "\n",
    "# Map function names to implementations\n",
    "available_functions = {\n",
    "    \"get_weather\": get_weather,\n",
    "    \"get_stock_price\": get_stock_price\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d67330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorated function for initial GPT call with tools\n",
    "@trace_openai_call(operation=\"chat.completions.with_tools\")\n",
    "def call_gpt_with_tools(client, messages, model, tools):\n",
    "    \"\"\"Call GPT with function calling enabled\"\"\"\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Decorated function for executing local functions\n",
    "@trace_function(span_name=\"execute_function_call\")\n",
    "def execute_function_call(function_name: str, function_args: dict):\n",
    "    \"\"\"Execute a function call locally\"\"\"\n",
    "    if function_name in available_functions:\n",
    "        function_to_call = available_functions[function_name]\n",
    "        return function_to_call(**function_args)\n",
    "    else:\n",
    "        return json.dumps({\"error\": f\"Function {function_name} not found\"})\n",
    "\n",
    "\n",
    "# Decorated function for sending function result back to GPT\n",
    "@trace_openai_call(operation=\"chat.completions.with_function_result\")\n",
    "def call_gpt_with_function_result(client, messages, model):\n",
    "    \"\"\"Send function result back to GPT\"\"\"\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "\n",
    "# Complete orchestration with tracing\n",
    "@trace_function(span_name=\"gpt_function_calling_orchestration\")\n",
    "def run_agent_with_function_calling(user_query: str):\n",
    "    \"\"\"\n",
    "    Complete agent orchestration with function calling\n",
    "    This creates a hierarchy of spans showing the entire flow\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant with access to weather and stock price data.\"},\n",
    "        {\"role\": \"user\", \"content\": user_query}\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nü§ñ User Query: {user_query}\\n\")\n",
    "\n",
    "    # Step 1: Call GPT with tools\n",
    "    response = call_gpt_with_tools(\n",
    "        client=client,\n",
    "        messages=messages,\n",
    "        model=deployment_name,\n",
    "        tools=tools\n",
    "    )\n",
    "\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "\n",
    "    # If no tool calls, return the response directly\n",
    "    if not tool_calls:\n",
    "        print(f\"‚úÖ Direct Response: {response_message.content}\")\n",
    "        return response_message.content\n",
    "\n",
    "    # Add assistant's response to messages\n",
    "    messages.append(response_message)\n",
    "\n",
    "    # Step 2: Execute each function call\n",
    "    print(f\"üîß Function Calls Detected: {len(tool_calls)}\\n\")\n",
    "\n",
    "    for tool_call in tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "        print(f\"üìû Calling: {function_name}({function_args})\")\n",
    "\n",
    "        # Execute the function (traced!)\n",
    "        function_response = execute_function_call(function_name, function_args)\n",
    "\n",
    "        print(f\"üìä Result: {function_response}\\n\")\n",
    "\n",
    "        # Add function result to messages\n",
    "        messages.append({\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "            \"role\": \"tool\",\n",
    "            \"name\": function_name,\n",
    "            \"content\": function_response\n",
    "        })\n",
    "\n",
    "    # Step 3: Send function results back to GPT\n",
    "    print(\"üîÑ Sending results back to GPT...\\n\")\n",
    "\n",
    "    final_response = call_gpt_with_function_result(\n",
    "        client=client,\n",
    "        messages=messages,\n",
    "        model=deployment_name\n",
    "    )\n",
    "\n",
    "    final_content = final_response.choices[0].message.content\n",
    "    print(f\"‚úÖ Final Response: {final_content}\\n\")\n",
    "\n",
    "    return final_content\n",
    "\n",
    "\n",
    "# Test the agent!\n",
    "result = run_agent_with_function_calling(\n",
    "    \"What's the weather in San Francisco and what's the current price of Microsoft stock?\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üí¨ Final Result:\\n{result}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75a9df3",
   "metadata": {},
   "source": [
    "### What Gets Traced?\n",
    "\n",
    "The function calling example creates a **hierarchical trace** showing:\n",
    "\n",
    "1. **`gpt_function_calling_orchestration`** - Root span for entire flow\n",
    "   - **`chat.completions.with_tools`** - Initial GPT call with tool definitions\n",
    "   - **`execute_function_call`** - Each function execution (get_weather, get_stock_price)\n",
    "   - **`chat.completions.with_function_result`** - Final GPT call with function results\n",
    "\n",
    "Each span captures:\n",
    "- ‚è±Ô∏è **Latency** - How long each step took\n",
    "- üîó **Parent-child relationships** - See the complete call hierarchy\n",
    "- üìä **Tokens** - Token usage for GPT calls\n",
    "- üõ†Ô∏è **Function details** - Which functions were called with what arguments\n",
    "- ‚úÖ **Success/failure** - Status of each operation\n",
    "\n",
    "This gives you complete visibility into your agent's behavior!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00899983",
   "metadata": {},
   "source": [
    "### üîç Viewing Function Arguments in Database\n",
    "\n",
    "After running the function calling example, you can query the database to see the actual function arguments:\n",
    "\n",
    "```sql\n",
    "SELECT key, value \n",
    "FROM span_attributes \n",
    "WHERE span_id IN (\n",
    "    SELECT id FROM spans WHERE name = 'execute_function_call'\n",
    ")\n",
    "AND key LIKE 'arg.%';\n",
    "```\n",
    "\n",
    "You'll see entries like:\n",
    "- `arg.function_name`: `\"get_weather\"`\n",
    "- `arg.function_args`: `{\"location\": \"San Francisco\", \"unit\": \"fahrenheit\"}` (as JSON string)\n",
    "\n",
    "The decorator now serializes dictionaries and lists as JSON strings, so you can see the complete argument values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6426fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def view_function_call_details(db_path=\"decorator_traces.db\"):\n",
    "    \"\"\"View detailed function call arguments from traces\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    print(\"üîç Function Call Details\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Query function call spans with their arguments\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            s.id,\n",
    "            s.name,\n",
    "            s.created_at,\n",
    "            sa.key,\n",
    "            sa.value\n",
    "        FROM spans s\n",
    "        LEFT JOIN span_attributes sa ON s.id = sa.span_id\n",
    "        WHERE s.name = 'execute_function_call'\n",
    "        AND sa.key LIKE 'arg.%'\n",
    "        ORDER BY s.created_at DESC, sa.key\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "\n",
    "    if len(df) > 0:\n",
    "        # Group by span to show each function call\n",
    "        for span_id in df['id'].unique():\n",
    "            span_data = df[df['id'] == span_id]\n",
    "            print(f\"\\nüìû Function Call (Span ID: {span_id})\")\n",
    "            print(f\"   Timestamp: {span_data.iloc[0]['created_at']}\")\n",
    "            print(f\"   Arguments:\")\n",
    "\n",
    "            for _, row in span_data.iterrows():\n",
    "                key = row['key'].replace('arg.', '')\n",
    "                value = row['value']\n",
    "\n",
    "                # Try to pretty print JSON\n",
    "                if value and (value.startswith('{') or value.startswith('[')):\n",
    "                    try:\n",
    "                        parsed = json.loads(value)\n",
    "                        value = json.dumps(parsed, indent=6)\n",
    "                        print(f\"     {key}:\")\n",
    "                        for line in value.split('\\n'):\n",
    "                            print(f\"       {line}\")\n",
    "                    except:\n",
    "                        print(f\"     {key}: {value}\")\n",
    "                else:\n",
    "                    print(f\"     {key}: {value}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "    else:\n",
    "        print(\"No function call traces found.\")\n",
    "        print(\"Make sure to run Example 6 first and flush traces!\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# View the function call details\n",
    "view_function_call_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d242c3c",
   "metadata": {},
   "source": [
    "### üîó Grouping Spans by Trace\n",
    "\n",
    "Each execution of `run_agent_with_function_calling` creates a **trace** - a collection of related spans with the same `trace_id`. This lets you see all the spans that belong to a single function call execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424e1453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_traces_grouped(db_path=\"decorator_traces.db\", limit=3):\n",
    "    \"\"\"View traces grouped by trace_id showing complete execution flows\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    print(\"üîó Traces Grouped by Execution\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Get unique traces for the orchestration function\n",
    "    traces_query = \"\"\"\n",
    "        SELECT DISTINCT trace_id, MIN(created_at) as first_span_time\n",
    "        FROM spans \n",
    "        WHERE name = 'gpt_function_calling_orchestration'\n",
    "        ORDER BY first_span_time DESC\n",
    "        LIMIT ?\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(traces_query, (limit,))\n",
    "    traces = cursor.fetchall()\n",
    "    \n",
    "    if not traces:\n",
    "        print(\"No function calling traces found.\")\n",
    "        print(\"Make sure to run Example 6 first!\")\n",
    "        print(\"=\" * 80)\n",
    "        conn.close()\n",
    "        return\n",
    "    \n",
    "    for idx, (trace_id, first_time) in enumerate(traces, 1):\n",
    "        print(f\"\\nüìä Trace #{idx}: {trace_id}\")\n",
    "        print(f\"   Started: {first_time}\")\n",
    "        print(f\"   {'‚îÄ' * 76}\")\n",
    "        \n",
    "        # Get all spans for this trace\n",
    "        spans_query = \"\"\"\n",
    "            SELECT \n",
    "                id,\n",
    "                name,\n",
    "                ROUND(duration_ms, 2) as duration_ms,\n",
    "                parent_span_id,\n",
    "                status_code,\n",
    "                datetime(created_at) as created_at\n",
    "            FROM spans \n",
    "            WHERE trace_id = ?\n",
    "            ORDER BY created_at ASC\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql_query(spans_query, conn, params=(trace_id,))\n",
    "        \n",
    "        print(f\"\\n   Total spans in this trace: {len(df)}\")\n",
    "        print(f\"\\n   Span Hierarchy:\")\n",
    "        \n",
    "        # Build hierarchy visualization\n",
    "        for _, row in df.iterrows():\n",
    "            indent = \"     \"\n",
    "            if row['parent_span_id'] is None:\n",
    "                # Root span\n",
    "                print(f\"   {indent}‚îå‚îÄ {row['name']}\")\n",
    "            else:\n",
    "                # Child span\n",
    "                print(f\"   {indent}‚îú‚îÄ‚îÄ‚îÄ {row['name']}\")\n",
    "            \n",
    "            print(f\"   {indent}‚îÇ    Duration: {row['duration_ms']}ms | Status: {row['status_code']}\")\n",
    "        \n",
    "        # Get total duration\n",
    "        total_duration = df['duration_ms'].iloc[0] if len(df) > 0 else 0\n",
    "        print(f\"\\n   ‚è±Ô∏è  Total Duration: {total_duration}ms\")\n",
    "        \n",
    "        # Count function calls\n",
    "        function_calls = len(df[df['name'] == 'execute_function_call'])\n",
    "        openai_calls = len(df[df['name'].str.contains('chat.completions', na=False)])\n",
    "        \n",
    "        print(f\"   üìû Function Calls: {function_calls}\")\n",
    "        print(f\"   ü§ñ OpenAI API Calls: {openai_calls}\")\n",
    "        print(f\"   {'‚îÄ' * 76}\")\n",
    "    \n",
    "    conn.close()\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# View the most recent traces\n",
    "view_traces_grouped(limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264e77af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_trace_details(trace_id: str = None, db_path=\"decorator_traces.db\"):\n",
    "    \"\"\"View detailed hierarchy for a specific trace\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # If no trace_id provided, get the most recent one\n",
    "    if trace_id is None:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT trace_id \n",
    "            FROM spans \n",
    "            WHERE name = 'gpt_function_calling_orchestration'\n",
    "            ORDER BY created_at DESC \n",
    "            LIMIT 1\n",
    "        \"\"\")\n",
    "        result = cursor.fetchone()\n",
    "        if not result:\n",
    "            print(\"No traces found!\")\n",
    "            conn.close()\n",
    "            return\n",
    "        trace_id = result[0]\n",
    "    \n",
    "    print(\"üîç Detailed Trace View\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Trace ID: {trace_id}\\n\")\n",
    "    \n",
    "    # Get all spans for this trace\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            id,\n",
    "            span_id,\n",
    "            parent_span_id,\n",
    "            name,\n",
    "            ROUND(duration_ms, 2) as duration_ms,\n",
    "            status_code,\n",
    "            datetime(created_at) as created_at\n",
    "        FROM spans \n",
    "        WHERE trace_id = ?\n",
    "        ORDER BY created_at ASC\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_sql_query(query, conn, params=(trace_id,))\n",
    "    \n",
    "    # Build parent-child map\n",
    "    span_map = {}\n",
    "    for _, row in df.iterrows():\n",
    "        span_map[row['span_id']] = {\n",
    "            'id': row['id'],\n",
    "            'name': row['name'],\n",
    "            'duration': row['duration_ms'],\n",
    "            'status': row['status_code'],\n",
    "            'parent': row['parent_span_id'],\n",
    "            'children': []\n",
    "        }\n",
    "    \n",
    "    # Find root and build tree\n",
    "    root = None\n",
    "    for span_id, span_data in span_map.items():\n",
    "        parent_id = span_data['parent']\n",
    "        if parent_id is None:\n",
    "            root = span_id\n",
    "        elif parent_id in span_map:\n",
    "            span_map[parent_id]['children'].append(span_id)\n",
    "    \n",
    "    # Recursive function to print tree\n",
    "    def print_span_tree(span_id, indent=0):\n",
    "        span = span_map[span_id]\n",
    "        prefix = \"  \" * indent\n",
    "        \n",
    "        if indent == 0:\n",
    "            symbol = \"üìä\"\n",
    "        elif len(span['children']) > 0:\n",
    "            symbol = \"‚îú‚îÄ\"\n",
    "        else:\n",
    "            symbol = \"‚îî‚îÄ\"\n",
    "        \n",
    "        print(f\"{prefix}{symbol} {span['name']}\")\n",
    "        print(f\"{prefix}   ‚è±Ô∏è  {span['duration']}ms | ‚úÖ {span['status']} | ID: {span['id']}\")\n",
    "        \n",
    "        for child_id in span['children']:\n",
    "            print_span_tree(child_id, indent + 1)\n",
    "    \n",
    "    if root:\n",
    "        print_span_tree(root)\n",
    "    else:\n",
    "        print(\"Could not build hierarchy - no root span found\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Summary: {len(df)} spans in this trace\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "# View the most recent trace in detail\n",
    "view_trace_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14d374",
   "metadata": {},
   "source": [
    "### üí° Understanding Traces vs Spans\n",
    "\n",
    "**Key Concept:**\n",
    "- **Trace** = One complete execution flow (identified by `trace_id`)\n",
    "- **Span** = One operation within that flow (function call, API call, etc.)\n",
    "\n",
    "**Example:** When you run `run_agent_with_function_calling()` once:\n",
    "- ‚úÖ Creates **1 trace** with a unique `trace_id`\n",
    "- ‚úÖ That trace contains **5 spans**:\n",
    "  1. `gpt_function_calling_orchestration` (root span)\n",
    "  2. `chat.completions.with_tools` (child)\n",
    "  3. `execute_function_call` for get_weather (child)\n",
    "  4. `execute_function_call` for get_stock_price (child)\n",
    "  5. `chat.completions.with_function_result` (child)\n",
    "\n",
    "All 5 spans share the **same `trace_id`**, so you can query them together!\n",
    "\n",
    "**In your database:**\n",
    "```sql\n",
    "-- Find all spans for a specific trace\n",
    "SELECT * FROM spans WHERE trace_id = 'abc123...'\n",
    "\n",
    "-- Count spans per trace\n",
    "SELECT trace_id, COUNT(*) as span_count \n",
    "FROM spans \n",
    "GROUP BY trace_id\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982a64ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Query and Analyze Traces\n",
    "\n",
    "Flush pending traces and query the SQLite database for analysis.\n",
    "\n",
    "### Flush Traces\n",
    "\n",
    "Force export of all pending spans (important with BatchSpanProcessor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b42b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force flush all pending traces\n",
    "flush_traces(timeout=10)\n",
    "print(\"‚úÖ All pending traces flushed to database\")\n",
    "print(\"üìä Ready for querying and analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42624979",
   "metadata": {},
   "source": [
    "### Query Trace Summary\n",
    "\n",
    "View all captured traces with service breakdown, hierarchy, and performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86837ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "def query_traces(db_path=\"decorator_traces.db\"):\n",
    "    \"\"\"Query trace database\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"üìä TRACE SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Get total count\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM spans\")\n",
    "    total_spans = cursor.fetchone()[0]\n",
    "    print(f\"\\nüìà Total spans: {total_spans}\")\n",
    "    \n",
    "    # Check if service_name column exists (for backward compatibility)\n",
    "    cursor.execute(\"PRAGMA table_info(spans)\")\n",
    "    columns = [col[1] for col in cursor.fetchall()]\n",
    "    has_service_name = 'service_name' in columns\n",
    "    \n",
    "    # Show service breakdown if service_name exists\n",
    "    if has_service_name:\n",
    "        print(\"\\nüè¢ Traces by Service:\")\n",
    "        df_services = pd.read_sql_query(\"\"\"\n",
    "            SELECT \n",
    "                service_name,\n",
    "                COUNT(*) as count,\n",
    "                ROUND(AVG(duration_ms), 2) as avg_duration_ms\n",
    "            FROM spans \n",
    "            GROUP BY service_name\n",
    "        \"\"\", conn)\n",
    "        print(df_services.to_string(index=False))\n",
    "    \n",
    "    # Get latest spans with hierarchy\n",
    "    print(\"\\nüïê Latest Traces (showing parent-child relationships):\")\n",
    "    service_col = \"service_name,\" if has_service_name else \"\"\n",
    "    df = pd.read_sql_query(f\"\"\"\n",
    "        SELECT \n",
    "            id,\n",
    "            {service_col}\n",
    "            name,\n",
    "            ROUND(duration_ms, 2) as duration_ms,\n",
    "            parent_span_id,\n",
    "            datetime(created_at) as created_at\n",
    "        FROM spans \n",
    "        ORDER BY created_at DESC \n",
    "        LIMIT 15\n",
    "    \"\"\", conn)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Performance by operation\n",
    "    print(\"\\n‚ö° Performance by Operation:\")\n",
    "    df_perf = pd.read_sql_query(\"\"\"\n",
    "        SELECT \n",
    "            name,\n",
    "            COUNT(*) as count,\n",
    "            ROUND(AVG(duration_ms), 2) as avg_ms,\n",
    "            ROUND(MIN(duration_ms), 2) as min_ms,\n",
    "            ROUND(MAX(duration_ms), 2) as max_ms\n",
    "        FROM spans \n",
    "        GROUP BY name\n",
    "        ORDER BY avg_ms DESC\n",
    "    \"\"\", conn)\n",
    "    print(df_perf.to_string(index=False))\n",
    "    \n",
    "    conn.close()\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "query_traces()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6313170",
   "metadata": {},
   "source": [
    "## Filter Traces by Service Name\n",
    "\n",
    "Now you can see the `service_name` column in the database! This is useful when you have multiple services writing to the same database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f84f5",
   "metadata": {},
   "source": [
    "### üìö Best Practice: Hybrid Storage\n",
    "\n",
    "The implementation follows **industry best practices** by storing `service_name` in two places:\n",
    "\n",
    "1. **Database Column** (`spans.service_name`)\n",
    "   - ‚úÖ Fast filtering and querying\n",
    "   - ‚úÖ Indexed for performance\n",
    "   - ‚úÖ Used by Jaeger, Zipkin, Tempo, SignalFx\n",
    "   \n",
    "2. **Span Attribute** (`service.name`)\n",
    "   - ‚úÖ OpenTelemetry standard compliance\n",
    "   - ‚úÖ Compatible with OTel collectors\n",
    "   - ‚úÖ Follows semantic conventions\n",
    "\n",
    "This hybrid approach gives you the best of both worlds: **fast queries** + **OTel compatibility**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7806668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_by_service(service_name: str, db_path=\"decorator_traces.db\"):\n",
    "    \"\"\"Query traces for a specific service\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    print(f\"üîç Traces for service: {service_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df = pd.read_sql_query(\"\"\"\n",
    "        SELECT \n",
    "            id,\n",
    "            name,\n",
    "            ROUND(duration_ms, 2) as duration_ms,\n",
    "            status_code,\n",
    "            datetime(created_at) as created_at\n",
    "        FROM spans \n",
    "        WHERE service_name = ?\n",
    "        ORDER BY created_at DESC\n",
    "        LIMIT 10\n",
    "    \"\"\", conn, params=(service_name,))\n",
    "    \n",
    "    if len(df) > 0:\n",
    "        print(df.to_string(index=False))\n",
    "    else:\n",
    "        print(f\"No traces found for service: {service_name}\")\n",
    "    \n",
    "    conn.close()\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Query for our specific service\n",
    "query_by_service(\"decorator-tracing-demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e7db24",
   "metadata": {},
   "source": [
    "### Query Comparison: Column vs Attribute\n",
    "\n",
    "Let's compare query performance between the two approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db77b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def compare_query_methods(service_name: str, db_path=\"decorator_traces.db\"):\n",
    "    \"\"\"Compare query performance: column vs attribute\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    print(\"üèÅ Query Performance Comparison\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Method 1: Query by column (FAST)\n",
    "    start = time.time()\n",
    "    df1 = pd.read_sql_query(\"\"\"\n",
    "        SELECT COUNT(*) as count\n",
    "        FROM spans \n",
    "        WHERE service_name = ?\n",
    "    \"\"\", conn, params=(service_name,))\n",
    "    time1 = (time.time() - start) * 1000\n",
    "    \n",
    "    print(f\"\\n‚úÖ Method 1: Column-based query\")\n",
    "    print(f\"   SELECT * FROM spans WHERE service_name = '{service_name}'\")\n",
    "    print(f\"   Time: {time1:.3f}ms\")\n",
    "    print(f\"   Result: {df1['count'].values[0]} spans\")\n",
    "    \n",
    "    # Method 2: Query by attribute (SLOWER - requires JOIN)\n",
    "    start = time.time()\n",
    "    df2 = pd.read_sql_query(\"\"\"\n",
    "        SELECT COUNT(*) as count\n",
    "        FROM spans s\n",
    "        INNER JOIN span_attributes sa ON s.id = sa.span_id\n",
    "        WHERE sa.key = 'service.name' AND sa.value = ?\n",
    "    \"\"\", conn, params=(service_name,))\n",
    "    time2 = (time.time() - start) * 1000\n",
    "    \n",
    "    print(f\"\\nüìä Method 2: Attribute-based query (OTel compatible)\")\n",
    "    print(f\"   SELECT * FROM spans JOIN span_attributes WHERE key='service.name'\")\n",
    "    print(f\"   Time: {time2:.3f}ms\")\n",
    "    print(f\"   Result: {df2['count'].values[0]} spans\")\n",
    "    \n",
    "    # Show speedup\n",
    "    speedup = time2 / time1 if time1 > 0 else 0\n",
    "    print(f\"\\n‚ö° Speedup: {speedup:.1f}x faster with column-based query\")\n",
    "    print(f\"   üí° This is why we store service_name as a column!\")\n",
    "    \n",
    "    conn.close()\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Run comparison\n",
    "compare_query_methods(\"decorator-tracing-demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af5478d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Non-Intrusive Tracing**: Add observability without modifying function logic\n",
    "2. **Clean Code**: Separation of concerns (business logic vs. tracing)\n",
    "3. **Automatic Detection**: Handles sync and async functions automatically\n",
    "4. **Composability**: Creates span hierarchies for complex workflows\n",
    "5. **Privacy Controls**: Configurable capture for compliance\n",
    "6. **Universal Application**: Works with any Python function\n",
    "\n",
    "### Decorator Comparison\n",
    "\n",
    "| Feature | @trace_openai_call | @trace_function |\n",
    "|---------|-------------------|-----------------|\n",
    "| **Purpose** | Azure OpenAI API calls | Any Python function |\n",
    "| **Captures Model** | ‚úÖ Yes | ‚ùå No |\n",
    "| **Captures Tokens** | ‚úÖ Yes | ‚ùå No |\n",
    "| **Captures Messages** | ‚úÖ Optional | ‚ùå No |\n",
    "| **Captures Response** | ‚úÖ Optional | ‚ùå No |\n",
    "| **Captures Arguments** | ‚úÖ Yes | ‚úÖ Yes |\n",
    "| **Captures Return Value** | ‚úÖ Yes | ‚úÖ Yes |\n",
    "| **Measures Latency** | ‚úÖ Yes | ‚úÖ Yes |\n",
    "| **Handles Exceptions** | ‚úÖ Yes | ‚úÖ Yes |\n",
    "| **Async Support** | ‚úÖ Yes | ‚úÖ Yes |\n",
    "| **Custom Attributes** | ‚úÖ Via operation | ‚úÖ Via attributes param |\n",
    "\n",
    "### When to Use Each Decorator\n",
    "\n",
    "**Use `@trace_openai_call` for:**\n",
    "- Azure OpenAI API calls\n",
    "- Chat completions (with or without streaming)\n",
    "- Function calling / tool usage\n",
    "- When you need token usage tracking\n",
    "- When you want message/response capture\n",
    "\n",
    "**Use `@trace_function` for:**\n",
    "- Data processing functions\n",
    "- Business logic\n",
    "- Validation functions\n",
    "- Helper utilities\n",
    "- Database operations\n",
    "- File I/O\n",
    "- Any non-OpenAI function\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "#### 1. Decorator Placement\n",
    "- ‚úÖ Apply decorators to public-facing functions\n",
    "- ‚úÖ Trace complete workflows, not every helper\n",
    "- ‚úÖ Use custom span names for clarity\n",
    "- ‚úÖ Add business context with custom attributes\n",
    "- ‚ùå Avoid over-instrumentation (too many spans)\n",
    "\n",
    "#### 2. Privacy & Compliance\n",
    "- ‚úÖ Disable message capture in production (`capture_messages=False`)\n",
    "- ‚úÖ Disable response capture for sensitive data\n",
    "- ‚úÖ Review captured data for PII\n",
    "- ‚úÖ Implement data retention policies\n",
    "- ‚úÖ Use separate databases for dev/prod\n",
    "\n",
    "#### 3. Performance\n",
    "- ‚úÖ Use `use_batch_processor=True` for better performance\n",
    "- ‚úÖ Call `flush_traces()` before script/notebook ends\n",
    "- ‚úÖ Monitor database size for long-running applications\n",
    "- ‚úÖ Consider sampling for very high-volume functions\n",
    "- ‚ùå Don't flush after every single call\n",
    "\n",
    "#### 4. Span Hierarchy\n",
    "- ‚úÖ Use descriptive span names\n",
    "- ‚úÖ Compose decorated functions for workflows\n",
    "- ‚úÖ Query by `trace_id` to see complete flows\n",
    "- ‚úÖ Use `parent_span_id` to understand call stacks\n",
    "- ‚úÖ Group related operations in a root span\n",
    "\n",
    "#### 5. Service Identification\n",
    "- ‚úÖ Use meaningful service names\n",
    "- ‚úÖ Filter traces by service in shared databases\n",
    "- ‚úÖ Follow naming conventions (lowercase, hyphens)\n",
    "- ‚úÖ Include environment in service name if needed\n",
    "- ‚úÖ Use consistent naming across your organization\n",
    "\n",
    "#### 6. Function Calling\n",
    "- ‚úÖ Decorate tool definition functions\n",
    "- ‚úÖ Trace function execution (arguments, results)\n",
    "- ‚úÖ Create orchestration root span\n",
    "- ‚úÖ Capture tool call metadata\n",
    "- ‚úÖ Link function results to tool calls\n",
    "\n",
    "### Code Patterns\n",
    "\n",
    "**Simple Function:**\n",
    "```python\n",
    "@trace_function(span_name=\"process_data\")\n",
    "def process_data(input: str) -> dict:\n",
    "    # Your logic here\n",
    "    return result\n",
    "```\n",
    "\n",
    "**OpenAI Call:**\n",
    "```python\n",
    "@trace_openai_call(operation=\"chat.completions\")\n",
    "def ask_gpt(client, messages, model):\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "```\n",
    "\n",
    "**Privacy-Aware:**\n",
    "```python\n",
    "@trace_openai_call(\n",
    "    operation=\"chat.completions\",\n",
    "    capture_messages=False,  # Don't store prompts\n",
    "    capture_response=False   # Don't store completions\n",
    ")\n",
    "def private_chat(client, messages, model):\n",
    "    return client.chat.completions.create(...)\n",
    "```\n",
    "\n",
    "**With Custom Attributes:**\n",
    "```python\n",
    "@trace_function(\n",
    "    span_name=\"process_order\",\n",
    "    attributes={\n",
    "        \"version\": \"2.0\",\n",
    "        \"environment\": \"production\",\n",
    "        \"region\": \"us-west-2\"\n",
    "    }\n",
    ")\n",
    "def process_order(order_id: str):\n",
    "    # Your logic here\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Agent Orchestration:**\n",
    "```python\n",
    "@trace_function(span_name=\"agent_workflow\")\n",
    "def run_agent(user_query: str):\n",
    "    # Each decorated call creates a child span\n",
    "    validated = validate_input(user_query)  # @trace_function\n",
    "    messages = prepare_messages(validated)  # @trace_function\n",
    "    response = call_openai(messages)        # @trace_openai_call\n",
    "    result = post_process(response)         # @trace_function\n",
    "    return result\n",
    "```\n",
    "\n",
    "### Database Queries\n",
    "\n",
    "**Find slow functions:**\n",
    "```sql\n",
    "SELECT name, duration_ms \n",
    "FROM spans \n",
    "WHERE duration_ms > 1000\n",
    "ORDER BY duration_ms DESC;\n",
    "```\n",
    "\n",
    "**Trace hierarchy:**\n",
    "```sql\n",
    "SELECT id, name, parent_span_id, duration_ms\n",
    "FROM spans\n",
    "WHERE trace_id = 'your_trace_id'\n",
    "ORDER BY id;\n",
    "```\n",
    "\n",
    "**Service breakdown:**\n",
    "```sql\n",
    "SELECT service_name, COUNT(*) as count, AVG(duration_ms) as avg_ms\n",
    "FROM spans\n",
    "GROUP BY service_name;\n",
    "```\n",
    "\n",
    "**Function call arguments:**\n",
    "```sql\n",
    "SELECT s.name, sa.key, sa.value\n",
    "FROM spans s\n",
    "JOIN span_attributes sa ON s.id = sa.span_id\n",
    "WHERE sa.key LIKE 'arg.%'\n",
    "ORDER BY s.created_at DESC;\n",
    "```\n",
    "\n",
    "### Advantages Over Manual Instrumentation\n",
    "\n",
    "| Aspect | Manual Instrumentation | Decorator-Based |\n",
    "|--------|----------------------|-----------------|\n",
    "| **Code Changes** | Many (wrap every call) | Minimal (one line) |\n",
    "| **Readability** | Lower (mixed concerns) | Higher (separated) |\n",
    "| **Maintainability** | Harder (scattered code) | Easier (centralized) |\n",
    "| **Consistency** | Error-prone | Guaranteed |\n",
    "| **Async Handling** | Manual detection | Automatic |\n",
    "| **Error Handling** | Must implement | Built-in |\n",
    "| **Testing** | Complex (mock spans) | Simple (mock function) |\n",
    "\n",
    "### Migration Strategy\n",
    "\n",
    "**From No Tracing:**\n",
    "1. Start with critical functions (`@trace_function`)\n",
    "2. Add OpenAI decorators (`@trace_openai_call`)\n",
    "3. Test with sample workloads\n",
    "4. Review captured data\n",
    "5. Roll out to production\n",
    "\n",
    "**From Manual Tracing:**\n",
    "1. Identify manually instrumented functions\n",
    "2. Remove manual span creation code\n",
    "3. Add appropriate decorator\n",
    "4. Test that spans are still created\n",
    "5. Clean up imports and helper code\n",
    "\n",
    "**To Cloud Tracing:**\n",
    "1. Keep decorators (they work with any exporter)\n",
    "2. Replace `setup_tracing()` with Azure Monitor config\n",
    "3. Update connection string\n",
    "4. Test export to Application Insights\n",
    "5. Maintain local tracing for development\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| Decorator not tracing | Check `setup_tracing()` was called first |\n",
    "| Missing spans | Call `flush_traces()` before querying |\n",
    "| No async traces | Ensure async function uses `await` |\n",
    "| Function args truncated | Check decorator serialization logic |\n",
    "| Performance impact | Use `use_batch_processor=True` |\n",
    "| Database too large | Implement data retention policy |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Experiment**: Add decorators to your own functions\n",
    "2. **Query**: Explore the database with SQL/Pandas\n",
    "3. **Analyze**: Find slow functions and bottlenecks\n",
    "4. **Optimize**: Use insights to improve performance\n",
    "5. **Integrate**: Connect to your CI/CD pipeline\n",
    "6. **Scale**: Migrate to cloud tracing for production\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [OpenTelemetry Python SDK](https://opentelemetry.io/docs/instrumentation/python/)\n",
    "- [Python Decorators Guide](https://realpython.com/primer-on-python-decorators/)\n",
    "- [Azure Monitor OpenTelemetry](https://learn.microsoft.com/azure/azure-monitor/app/opentelemetry-enable)\n",
    "- [Semantic Conventions](https://opentelemetry.io/docs/specs/semconv/)\n",
    "\n",
    "### Related Notebooks\n",
    "\n",
    "- `02_local_tracing.ipynb`: Manual tracing with wrapper functions\n",
    "- `01_azure_foundry_tracing.ipynb`: Cloud-based tracing with Application Insights\n",
    "- `../01_agent/`: Azure AI agent examples\n",
    "- `../05_evaluation/`: Agent evaluation and metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
